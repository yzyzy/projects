{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']\n",
    "import gzip\n",
    "\n",
    "import lxml.etree\n",
    "from lxml import etree\n",
    "from StringIO import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests as re\n",
    "import pandas as pd \n",
    "pd.set_option('display.max_columns', 30)\n",
    "import sklearn as sk\n",
    "import json as json\n",
    "import urllib2\n",
    "from urllib import urlopen\n",
    "\n",
    "import gzip\n",
    "import ujson\n",
    "import wget\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "\n",
    "from numpy import random\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']\n",
    "\n",
    "from IPython import display\n",
    "from sklearn.externals.six import StringIO\n",
    "import pydot\n",
    "import numpy as np\n",
    "from matplotlib import pylab as plt\n",
    "from sklearn import tree\n",
    "from sklearn import datasets, linear_model, utils, preprocessing\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random, re, time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local[*]\", \"temp\")\n",
    "print sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vagrant/miniprojects/spark\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print str(os.path.abspath(os.path.curdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def localpath(path):\n",
    "    return 'file://' + str(os.path.abspath(os.path.curdir)) + '/' + path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = '/allPosts/part-00000.xml.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-3e97665f1bda>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mujson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m#file_content = f.read()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "with gzip.open(test, 'rb') as f:\n",
    "    data.extend([ujson.loads(line) for line in f.readlines()])\n",
    "    #file_content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def parse(line):\n",
    "#    lxml.etree.XML(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 2.0 failed 1 times, most recent failure: Lost task 3.0 in stage 2.0 (TID 15, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 267, in dump_stream\n    bytes = self.serializer.dumps(vs)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 415, in dumps\n    return pickle.dumps(obj, protocol)\nPicklingError: Can't pickle <class '__main__.Post'>: attribute lookup __main__.Post failed\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:129)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:125)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:905)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:905)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:905)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:904)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 267, in dump_stream\n    bytes = self.serializer.dumps(vs)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 415, in dumps\n    return pickle.dumps(obj, protocol)\nPicklingError: Can't pickle <class '__main__.Post'>: attribute lookup __main__.Post failed\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:129)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:125)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:905)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:905)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-e77dc742c551>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mposts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocalpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/small_data/allPosts\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparse_post\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/opt/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    771\u001b[0m         \"\"\"\n\u001b[0;32m    772\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 773\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    774\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 2.0 failed 1 times, most recent failure: Lost task 3.0 in stage 2.0 (TID 15, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 267, in dump_stream\n    bytes = self.serializer.dumps(vs)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 415, in dumps\n    return pickle.dumps(obj, protocol)\nPicklingError: Can't pickle <class '__main__.Post'>: attribute lookup __main__.Post failed\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:129)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:125)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:905)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:905)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:905)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:904)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 267, in dump_stream\n    bytes = self.serializer.dumps(vs)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 415, in dumps\n    return pickle.dumps(obj, protocol)\nPicklingError: Can't pickle <class '__main__.Post'>: attribute lookup __main__.Post failed\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:129)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:125)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:905)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:905)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "class Post(object):\n",
    "    def __init__(self, postId, fav_count):\n",
    "        self.postId = postId\n",
    "        self.fav_count = int(fav_count)\n",
    "        \n",
    "def parse_post(line):\n",
    "    try:\n",
    "        tree = etree.fromstring(line.encode('utf-8'))\n",
    "        postId = tree.get(\"Id\")\n",
    "        fav_count = tree.get(\"FavoriteCount\")\n",
    "        if fav_count is None: fav_count=-1\n",
    "        return Post(postId, fav_count)\n",
    "    except etree.XMLSyntaxError:\n",
    "        pass\n",
    "    \n",
    "posts = sc.textFile(localpath(\"/small_data/allPosts\")) \\\n",
    "    .map(parse_post).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = sc.textFile(test).map(parse)\n",
    "#totalLines = data.count()\n",
    "#print \"total lines: %d\" % totalLines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-10-94779349cffa>\", line 8, in get_votes_by_id\nUnboundLocalError: local variable 'postid' referenced before assignment\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:905)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:904)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-10-94779349cffa>\", line 8, in get_votes_by_id\nUnboundLocalError: local variable 'postid' referenced before assignment\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-94779349cffa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0msoData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocalpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mvotedata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_votes_by_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;31m#.reduce(lambda x, y: x + y)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    771\u001b[0m         \"\"\"\n\u001b[0;32m    772\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 773\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    774\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-10-94779349cffa>\", line 8, in get_votes_by_id\nUnboundLocalError: local variable 'postid' referenced before assignment\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:905)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:904)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-10-94779349cffa>\", line 8, in get_votes_by_id\nUnboundLocalError: local variable 'postid' referenced before assignment\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "def get_votes_by_id(line):\n",
    "    try:\n",
    "        row = lxml.etree.XML(line)\n",
    "        postid = row.get(\"PostId\")\n",
    "        voteid = row.get(\"VoteTypeId\")\n",
    "    except:\n",
    "        pass\n",
    "    return (postid,voteid)\n",
    "\n",
    "soData = sc.textFile(localpath('small_data/allVotes/part-00001.xml.gz'))\n",
    "votedata = soData.map(get_votes_by_id).collect()\n",
    "#.reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    }
   ],
   "source": [
    "print '123'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-52-2ae137f23c16>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-52-2ae137f23c16>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    print data.map(lambda x: x.reading)\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print data.map(lambda x: x.reading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############# Q1 ## upvote_percentage_by_favorites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import lxml.etree as ET\n",
    "import pandas\n",
    "\n",
    "def isHeader(line):\n",
    "    return \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\"\"\" in line\n",
    "def isNone(line):\n",
    "    return \"None\" in line\n",
    "\n",
    "def localpath(path):\n",
    "    return 'file://' + str(os.path.abspath(os.path.curdir)) + '/' + path\n",
    "\n",
    "class votesRecord(object):\n",
    "    def __init__(self, CreationDate, Id, PostId, UserId, VoteTypeId, BountyAmount):\n",
    "        self.CreationDate = CreationDate\n",
    "        self.Id           = Id\n",
    "        self.PostId       = PostId\n",
    "        self.UserId       = UserId\n",
    "        self.VoteTypeId   = VoteTypeId\n",
    "        self.BountyAmount = BountyAmount\n",
    "\n",
    "def votesParse(xmlString):\n",
    "    try:  \n",
    "        rows = ET.fromstring(xmlString.encode('utf-8'))\n",
    "        CreationDate = rows.get('CreationDate')\n",
    "        Id           = rows.get('Id')\n",
    "        PostId       = rows.get('PostId')\n",
    "        UserId       = rows.get('UserId')\n",
    "        VoteTypeId   = int(rows.get('VoteTypeId'))\n",
    "        BountyAmount = rows.get('BountyAmount')\n",
    "#        print CreationDate, Id, PostId, UserId, VoteTypeId, BountyAmount\n",
    "        return votesRecord(CreationDate, Id, PostId, UserId, VoteTypeId, BountyAmount)    \n",
    "    except:\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row_rdd = sc.textFile(localpath(\"small_data/allVotes\")).map(votesParse)\n",
    "\n",
    "upVote = row_rdd.filter(lambda x: x is not None and x.VoteTypeId == 2)\\\n",
    "               .map(lambda x: (x.PostId, 1))\\\n",
    "               .reduceByKey(lambda a, b: a + b).sortByKey()\n",
    "\n",
    "dnVote = row_rdd.filter(lambda x: x is not None and x.VoteTypeId == 3)\\\n",
    "               .map(lambda x: (x.PostId, 1))\\\n",
    "               .reduceByKey(lambda a, b: a + b).sortByKey()\n",
    "\n",
    "fvVote = row_rdd.filter(lambda x: x is not None and x.VoteTypeId == 5)\\\n",
    "               .map(lambda x: (x.PostId, 1))\\\n",
    "               .reduceByKey(lambda a, b: a + b).sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test1 = upVote.reduce(lambda x, y: x+y)\n",
    "test1 = dnVote.map(lambda x:x[1]).reduce(lambda x,y:x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('100031', 1), ('100037', 2), ('100041', 1), ('100042', 1), ('100043', 1)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnVote.take(5)\n",
    "#print test1 #154450+146907+12462"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313819"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "154450+146907+12462"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "favDn= upVote.fullOuterJoin(dnVote)\\\n",
    "        .map(lambda (id, (x,y)): (id, (0 if x is None else x, 0 if y is None else y)))\\\n",
    "        .leftOuterJoin(fvVote)\\\n",
    "        .map(lambda (id, (x,y)): (id,(x, 0 if y is None else y)))\\\n",
    "        .map(lambda (id, ((upVote, downVote), favorite)): (favorite, (upVote, downVote)))\\\n",
    "        .map(lambda (id, (x,y)): (id, y))\\\n",
    "        .reduceByKey(lambda x, y: x+y).sortByKey()\n",
    "        \n",
    "favUp= upVote.fullOuterJoin(dnVote)\\\n",
    "        .map(lambda (id, (x,y)): (id, (0 if x is None else x, 0 if y is None else y)))\\\n",
    "        .leftOuterJoin(fvVote)\\\n",
    "        .map(lambda (id, (x,y)): (id,(x, 0 if y is None else y)))\\\n",
    "        .map(lambda (id, ((upVote, downVote), favorite)): (favorite, (upVote, downVote)))\\\n",
    "        .map(lambda (id, (x,y)): (id, x))\\\n",
    "        .reduceByKey(lambda x, y: x+y).sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 11828), (1, 702), (2, 187), (3, 79), (4, 52), (5, 28), (6, 15), (7, 22), (8, 10), (9, 6), (10, 12), (11, 9), (12, 5), (13, 2), (14, 5), (15, 6), (16, 0), (17, 0), (18, 1), (19, 1), (20, 1), (21, 3), (22, 1), (23, 2), (24, 0), (25, 0), (26, 5), (27, 4), (28, 1), (29, 1), (30, 1), (31, 2), (32, 0), (33, 0), (34, 0), (35, 0), (36, 0), (37, 1), (38, 2), (39, 1), (40, 0), (41, 0), (42, 0), (44, 0), (45, 0), (47, 0), (48, 0), (49, 0), (50, 0), (52, 0), (54, 0), (55, 0), (58, 0), (59, 0), (60, 0), (61, 0), (64, 4), (65, 0), (66, 1), (67, 0), (69, 2), (70, 0), (72, 0), (73, 2), (76, 0), (79, 1), (82, 1), (83, 1), (88, 0), (91, 0), (95, 0), (96, 4), (100, 2), (102, 0), (103, 0), (113, 0), (118, 0), (129, 0), (138, 1), (141, 0), (155, 0), (158, 5), (238, 0), (275, 0)]\n",
      "[(0, 232141), (1, 23800), (2, 13064), (3, 7811), (4, 5321), (5, 3753), (6, 2900), (7, 2370), (8, 1785), (9, 1865), (10, 1423), (11, 1052), (12, 1189), (13, 733), (14, 822), (15, 842), (16, 688), (17, 474), (18, 698), (19, 468), (20, 327), (21, 533), (22, 453), (23, 415), (24, 204), (25, 347), (26, 311), (27, 360), (28, 206), (29, 367), (30, 218), (31, 329), (32, 66), (33, 221), (34, 164), (35, 82), (36, 213), (37, 110), (38, 320), (39, 121), (40, 119), (41, 51), (42, 52), (44, 76), (45, 64), (47, 181), (48, 103), (49, 148), (50, 53), (52, 114), (54, 147), (55, 68), (58, 91), (59, 69), (60, 63), (61, 114), (64, 85), (65, 68), (66, 133), (67, 138), (69, 281), (70, 72), (72, 170), (73, 141), (76, 54), (79, 206), (82, 142), (83, 203), (88, 150), (91, 97), (95, 90), (96, 58), (100, 120), (102, 110), (103, 75), (113, 121), (118, 70), (129, 132), (138, 135), (141, 112), (155, 166), (158, 175), (238, 214), (275, 222)]\n"
     ]
    }
   ],
   "source": [
    "print favDn.collect()\n",
    "\n",
    "print favUp.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "comb = favDn.leftOuterJoin(favUp)\\\n",
    "        .map(lambda (id, (x,y)): (id, (0 if x is None else x, 0 if y is None else y)))\\\n",
    "        .map(lambda (fav, (downvotes, upvotes)): (fav, (upvotes / (upvotes + downvotes)))).sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.9515184306202837), (1, 0.971349277609991), (2, 0.9858878575201871), (3, 0.9899873257287706), (4, 0.990321980271729), (5, 0.9925945517058979), (6, 0.9948542024013722), (7, 0.9908026755852842), (8, 0.9944289693593314), (9, 0.9967931587386424), (10, 0.9916376306620209), (11, 0.9915174363807728), (12, 0.9958123953098827), (13, 0.9972789115646259), (14, 0.9939540507859734), (15, 0.9929245283018868), (16, 1.0), (17, 1.0), (18, 0.9985693848354793), (19, 0.997867803837953), (20, 0.9969512195121951), (21, 0.9944029850746269), (22, 0.9977973568281938), (23, 0.9952038369304557), (24, 1.0), (25, 1.0), (26, 0.9841772151898734), (27, 0.989010989010989), (28, 0.9951690821256038), (29, 0.9972826086956522), (30, 0.9954337899543378), (31, 0.9939577039274925), (32, 1.0), (33, 1.0), (34, 1.0), (35, 1.0), (36, 1.0), (37, 0.990990990990991), (38, 0.9937888198757764), (39, 0.9918032786885246), (40, 1.0), (41, 1.0), (42, 1.0), (44, 1.0), (45, 1.0), (47, 1.0), (48, 1.0), (49, 1.0), (50, 1.0), (52, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "print comb.take(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[('35540', ((4, 0), 1)),\n",
    " ('35236', ((2, 1), 0)),\n",
    " ('110000', ((0, 1), 0)),\n",
    " ('73393', ((1, 0), 0)),\n",
    " ('53001', ((2, 0), 2)),\n",
    " ('89370', ((2, 0), 0)),\n",
    " ('16424', ((5, 0), 0)),\n",
    " ('11546', ((2, 0), 0)),\n",
    " ('88197', ((2, 0), 0)),\n",
    " ('14545', ((8, 0), 0))]\n",
    "########\n",
    "[(1, (4, 0)),\n",
    " (0, (2, 1)),\n",
    " (0, (0, 1)),\n",
    " (0, (1, 0)),\n",
    " (2, (2, 0)),\n",
    " (0, (2, 0)),\n",
    " (0, (5, 0)),\n",
    " (0, (2, 0)),\n",
    " (0, (2, 0)),\n",
    " (0, (8, 0))]\n",
    "#######\n",
    "[(1, 0),\n",
    " (0, 1),\n",
    " (0, 1),\n",
    " (0, 0),\n",
    " (2, 0),\n",
    " (0, 0),\n",
    " (0, 0),\n",
    " (0, 0),\n",
    " (0, 0),\n",
    " (0, 0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 11828),\n",
       " (1, 702),\n",
       " (2, 187),\n",
       " (3, 79),\n",
       " (4, 52),\n",
       " (5, 28),\n",
       " (6, 15),\n",
       " (7, 22),\n",
       " (8, 10),\n",
       " (9, 6)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "favUpDn.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 6410), (1, 316), (2, 78), (3, 39), (4, 12), (5, 7), (6, 3), (7, 2), (8, 2), (9, 4), (10, 10), (11, 4), (12, 1), (13, 0), (14, 0), (15, 0), (16, 1), (17, 0), (18, 0), (19, 0), (20, 0), (21, 2), (22, 1), (23, 0), (24, 0), (25, 1), (26, 2), (27, 0), (28, 0), (29, 0), (30, 2), (31, 0), (35, 0), (36, 0), (37, 1), (38, 2), (39, 0), (41, 0), (43, 0), (46, 1), (50, 0), (51, 0), (52, 0), (53, 0), (58, 0), (59, 0), (65, 3), (67, 1), (69, 2), (72, 0), (75, 0), (97, 0), (108, 0), (129, 0)]\n"
     ]
    }
   ],
   "source": [
    "q1 = favUpDn.collect()\n",
    "#print q1\n",
    "favUpDn.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "############# Q2 ## user_answer_percentage_by_reputation\n",
    "print 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class usersRecord(object):\n",
    "    def __init__(self, AboutMe, AccountId, CreationDate, DisplayName, DownVotes, Id, LastAccessDate, Location, Reputation, UpVotes, Views, WebsiteUrl):\n",
    "        self.AboutMe        = AboutMe\n",
    "        self.AccountId      = AccountId\n",
    "        self.CreationDate   = CreationDate\n",
    "        self.DisplayName    = DisplayName\n",
    "        self.DownVotes      = DownVotes\n",
    "        self.Id             = Id\n",
    "        self.LastAccessDate = LastAccessDate\n",
    "        self.Location       = Location\n",
    "        self.Reputation     = Reputation\n",
    "        self.UpVotes        = UpVotes\n",
    "        self.Views          = Views\n",
    "        self.WebsiteUrl     = WebsiteUrl\n",
    "\n",
    "def usersParse(xmlString):\n",
    "    try:  \n",
    "        rows = ET.fromstring(xmlString.encode('utf-8'))\n",
    "        AboutMe        = rows.get('AboutMe')\n",
    "        AccountId      = rows.get('AccountId')\n",
    "        CreationDate   = rows.get('CreationDate')\n",
    "        DisplayName    = rows.get('DisplayName')\n",
    "        DownVotes      = rows.get('DownVotes')\n",
    "        Id             = rows.get('Id')\n",
    "        LastAccessDate = rows.get('LastAccessDate')\n",
    "        Location       = rows.get('Location')\n",
    "        Reputation     = int(rows.get('Reputation'))\n",
    "        UpVotes        = rows.get('UpVotes')\n",
    "        Views          = rows.get('Views')\n",
    "        WebsiteUrl     = rows.get('WebsiteUrl')\n",
    "\n",
    "        return usersRecord(AboutMe, AccountId, CreationDate, DisplayName, DownVotes, Id, LastAccessDate, Location, Reputation, UpVotes, Views, WebsiteUrl) \n",
    "    except:\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class postsRecord(object):\n",
    "    def __init__(self, AcceptedAnswerId, AnswerCount, Body, CommentCount, CreationDate, FavoriteCount, Id, LastActivityDate, \\\n",
    "                 LastEditDate, LastEditorUserId, OwnerUserId, PostTypeId, Score, Tags, Title, ViewCount):\n",
    "        self.AcceptedAnswerId   = AcceptedAnswerId\n",
    "        self.AnswerCount        = AnswerCount\n",
    "        self.Body               = Body\n",
    "        self.CommentCount       = CommentCount\n",
    "        self.CreationDate       = CreationDate\n",
    "        self.FavoriteCount      = FavoriteCount\n",
    "        self.Id                 = Id\n",
    "        self.LastActivityDate   = LastActivityDate\n",
    "        self.LastEditDate       = LastEditDate\n",
    "        self.LastEditorUserId   = LastEditorUserId\n",
    "        self.OwnerUserId        = OwnerUserId\n",
    "        self.PostTypeId         = PostTypeId\n",
    "        self.Score              = Score\n",
    "        self.Tags               = Tags\n",
    "        self.Title              = Title\n",
    "        self.ViewCount          = ViewCount\n",
    "# import lxml.etree as ET\n",
    "#tree = etree.parse('somefile.xml')\n",
    "#notags = etree.tostring(tree, encoding='utf8', method='text')\n",
    "#print(notags)\n",
    "\n",
    "def postsParse(xmlString):\n",
    "    try:  \n",
    "        rows = ET.fromstring(xmlString.encode('utf-8'))\n",
    "        AcceptedAnswerId    = rows.get('AcceptedAnswerId')\n",
    "        AnswerCount         = rows.get('AnswerCount')\n",
    "        Body0                = rows.get('Body')\n",
    "        Body1 = ET.fromstring(Body0.encode('utf-8'))\n",
    "        Body = ET.tostring(Body1, encoding='utf8', method='text')\n",
    "        CommentCount        = rows.get('CommentCount')\n",
    "        CreationDate        = rows.get('CreationDate')\n",
    "        FavoriteCount       = rows.get('FavoriteCount')\n",
    "        Id                  = rows.get('Id')\n",
    "        LastActivityDate    = rows.get('LastActivityDate')\n",
    "        LastEditDate        = rows.get('LastEditDate')\n",
    "        LastEditorUserId    = rows.get('LastEditorUserId')\n",
    "        OwnerUserId         = rows.get('OwnerUserId')\n",
    "        PostTypeId          = int(rows.get('PostTypeId'))\n",
    "        Score               = rows.get('Score')\n",
    "        #Tags                = rows.get('Tags')\n",
    "        Tags = re.sub( ur\">\", \" \", re.sub(ur\"<\", \"\",rows.get('Tags')))\n",
    "        Title               = rows.get('Title')\n",
    "        ViewCount           = rows.get('ViewCount')\n",
    "\n",
    "\n",
    "        return postsRecord(AcceptedAnswerId, AnswerCount, Body, CommentCount, CreationDate, FavoriteCount, Id, LastActivityDate, \\\n",
    "                 LastEditDate, LastEditorUserId, OwnerUserId, PostTypeId, Score, Tags, Title, ViewCount)\n",
    "    except:\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        Body0                = rows.get('Body')\n",
    "        Body1 = ET.parse(Body0)\n",
    "        Body = ET.tostring(Body1, encoding='utf8', method='text')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row_rdd = sc.textFile(localpath(\"small_data/allPosts\")).map(postsParse)\n",
    "\n",
    "question = row_rdd.filter(lambda x: x is not None and x.PostTypeId == 1)\n",
    "answer = row_rdd.filter(lambda x: x is not None and x.PostTypeId == 2)\n",
    "#                .map(lambda x: (x.Reputation))\n",
    "#               .reduceByKey(lambda a, b: a + b).sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rep = reputation.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = sorted(rep, reverse = True)[:99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11893.4646465\n"
     ]
    }
   ],
   "source": [
    "print reduce(lambda x, y: x + y, l) / len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52060"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55304"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row_rdd = sc.textFile(localpath(\"small_data/allUsers\")).map(usersParse)\n",
    "\n",
    "userRep = row_rdd.filter(lambda x: x is not None and x.Id is not None and x.Reputation is not None)\\\n",
    "               .map(lambda x: (x.Id, x.Reputation))\n",
    "            #.takeOrdered(99, key = lambda x: -x[1])\n",
    "        \n",
    "row_rdd = sc.textFile(localpath(\"small_data/allPosts\")).map(postsParse)\n",
    "\n",
    "question = row_rdd.filter(lambda x: x is not None and x.OwnerUserId is not None and x.PostTypeId == 1)\\\n",
    "               .map(lambda x: (x.OwnerUserId, 1))\\\n",
    "               .reduceByKey(lambda a, b: a + b).sortByKey()\n",
    "        \n",
    "answer = row_rdd.filter(lambda x: x is not None and x.OwnerUserId is not None and x.PostTypeId == 2)\\\n",
    "               .map(lambda x: (x.OwnerUserId, 1))\\\n",
    "               .reduceByKey(lambda a, b: a + b).sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('60581', 1)]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question.filter(lambda x: x[0] == '60581' ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rep99 = userRep.takeOrdered(99, key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('919', 100976), ('805', 92624), ('686', 47334), ('7290', 46907), ('930', 32283), ('4505', 27599), ('4253', 25406), ('183', 23610), ('11032', 23102), ('28746', 22706), ('887', 20315), ('159', 20133), ('2116', 19312), ('4856', 18866), ('22047', 17719), ('5739', 16854), ('3277', 16131), ('88', 14768), ('2970', 14500), ('601', 14100), ('17230', 13557), ('449', 13078), ('2392', 12491), ('1390', 12098), ('5836', 11989), ('7555', 11865), ('603', 11830), ('7972', 11795), ('6633', 11662), ('2958', 11083), ('9394', 10750), ('7828', 10728), ('2817', 10552), ('7224', 10394), ('4598', 10383), ('7071', 10045), ('1739', 9619), ('1036', 9530), ('3382', 9294), ('8013', 9047), ('3019', 8794), ('4376', 8629), ('251', 8221), ('28666', 8013), ('1764', 7971), ('23853', 7765), ('32036', 7729), ('10849', 7725), ('26338', 7608), ('1352', 7552), ('401', 7116), ('5', 6962), ('8', 6948), ('7250', 6888), ('1909', 6814), ('21054', 6716), ('4257', 6694), ('196', 6682), ('442', 6588), ('279', 6367), ('2669', 6352), ('8402', 6208), ('36041', 6149), ('2126', 6145), ('44269', 6127), ('6029', 6040), ('11981', 5970), ('1934', 5967), ('795', 5849), ('25433', 5775), ('253', 5762), ('364', 5739), ('25', 5661), ('22311', 5500), ('334', 5444), ('13047', 5398), ('8507', 5315), ('264', 5085), ('14188', 5042), ('307', 4934), ('8076', 4795), ('5862', 4656), ('8413', 4438), ('1307', 4238), ('2860', 4204), ('223', 4192), ('11887', 4149), ('52554', 4147), ('2074', 4127), ('35989', 4092), ('1005', 4080), ('22228', 4065), ('4862', 3971), ('3601', 3958), ('17908', 3957), ('13138', 3821), ('1108', 3805), ('1679', 3747), ('11852', 3732)]\n"
     ]
    }
   ],
   "source": [
    "print rep99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, 0, 1), (106, 0, 1), (108, 7, 8), (121, 1, 1), (101, 0, 1)]"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QA= sc.parallelize(rep99).leftOuterJoin(answer)\\\n",
    "        .map(lambda (id, (x,y)): (id, (0 if x is None else x, 0 if y is None else y))).sortByKey(ascending=False) \\\n",
    "        .leftOuterJoin(question)\\\n",
    "        .map(lambda (id, (x,y)): (id, (0 if x is None else x, 0 if y is None else y))).sortByKey(ascending=False) \\\n",
    "        .map(lambda (id, ((rep,answers),questions)):(int(id), answers /(answers + questions)  )) \\\n",
    "        #.filter(lambda x: x[2] != 0) \\\n",
    "        #.map(lambda (rep,answers,tot):(int(rep), answers/tot )).sortByKey(ascending=False)\n",
    "        #.map(lambda (id, (questions, answers)): (id, (answers / (answers + questions))))\\\n",
    "\n",
    "[('-1', (1, 0)),\n",
    " ('1', (101, 0)),\n",
    " ('1000135', (103, 0)),\n",
    " ('1000180', (41, 0)),\n",
    " ('1000186', (101, 0))]\n",
    "\n",
    "[('-1', ((1, 0), 0)),\n",
    " ('1', ((101, 0), 0)),\n",
    " ('1000135', ((103, 0), 0)),\n",
    " ('1000180', ((41, 0), 0)),\n",
    " ('1000186', ((101, 0), 0))]\n",
    "\n",
    "[(1, (0, 0)), (101, (0, 0)), (103, (0, 0)), (41, (0, 0)), (101, (0, 0))]\n",
    "[(101, 0, 1), (106, 0, 1), (108, 7, 8), (121, 1, 1), (101, 0, 1)]\n",
    "\n",
    "#QA= question.join(answer)\\\n",
    "        #.map(lambda (id, (x,y)): (id, (0 if x is None else x, 0 if y is None else y)))\\\n",
    "        #.map(lambda (id, (questions, answers)): (id, (answers / (answers + questions))))\\\n",
    "        #.leftOuterJoin(userRep)\\\n",
    "        #.map(lambda (id, (x,y)): (id,(x, 0 if y is None else y)))\\\n",
    "        #.map(lambda (id, (x,y)): (y,x)).takeOrdered(99, key = lambda x: -x[0])\n",
    "#        .map(lambda (id, ((upVote, downVote), favorite)): (favorite, (upVote, downVote)))\\\n",
    "#        .map(lambda (id, (x,y)): (id, y))\\\n",
    "#        .reduceByKey(lambda x, y: x+y).sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9394, 0.9700854700854701), (930, 0.9817351598173516), (919, 0.996694214876033), (887, 0.9794871794871794), (88, 0.9660493827160493), (8507, 0.9428571428571428), (8413, 0.9836065573770492), (8402, 0.6521739130434783), (8076, 0.9333333333333333), (805, 0.9959749552772809), (8013, 0.9040697674418605), (8, 0.8991596638655462), (7972, 0.9823008849557522), (795, 0.676056338028169), (7828, 0.9850427350427351), (7555, 1.0), (7290, 0.9918887601390498), (7250, 0.9877300613496932), (7224, 0.9757575757575757), (7071, 0.9107142857142857), (686, 0.9803049555273189), (6633, 0.9912280701754386), (603, 0.8158844765342961), (6029, 1.0), (601, 0.9772151898734177), (5862, 1.0), (5836, 0.846441947565543), (5739, 0.9872773536895675), (52554, 0.9652777777777778), (5, 0.8547008547008547), (4862, 0.8974358974358975), (4856, 0.9543147208121827), (4598, 0.9857142857142858), (4505, 1.0), (449, 1.0), (44269, 0.9033613445378151), (442, 0.8712121212121212), (4376, 0.963302752293578), (4257, 0.9757575757575757), (4253, 0.9909747292418772), (401, 0.9119496855345912), (364, 0.6736111111111112), (36041, 0.9889807162534435), (3601, 0.9852941176470589), (35989, 0.9487179487179487), (3382, 1.0), (334, 1.0), (3277, 0.956081081081081), (32036, 0.9959839357429718), (307, 1.0), (3019, 0.8571428571428571), (2970, 1.0), (2958, 0.9930313588850174), (28746, 0.968421052631579), (28666, 0.9), (2860, 0.8903225806451613), (2817, 0.8206896551724138), (279, 1.0), (2669, 0.946843853820598), (264, 0.90625), (26338, 0.9691358024691358), (25433, 0.9867256637168141), (253, 0.3695652173913043), (251, 0.9924242424242424), (25, 0.9166666666666666), (2392, 0.9724137931034482), (23853, 1.0), (22311, 0.9401709401709402), (223, 0.8588235294117647), (22228, 0.8513513513513513), (22047, 1.0), (2126, 1.0), (2116, 0.9833333333333333), (21054, 0.9345794392523364), (2074, 1.0), (196, 0.7357512953367875), (1934, 0.9680851063829787), (1909, 0.9518072289156626), (183, 0.847870182555781), (17908, 1.0), (1764, 0.9325842696629213), (1739, 0.9948717948717949), (17230, 0.9970059880239521), (1679, 0.941747572815534), (159, 0.9728813559322034), (14188, 0.8983050847457628), (1390, 0.9411764705882353), (1352, 0.9902912621359223), (13138, 0.8639455782312925), (1307, 0.8333333333333334), (13047, 0.9733333333333334), (11981, 0.9649122807017544), (11887, 0.976878612716763), (11852, 1.0), (1108, 0.9722222222222222), (11032, 0.9875647668393782), (10849, 0.9518072289156626), (1036, 0.9545454545454546), (1005, 0.0034482758620689655)]\n"
     ]
    }
   ],
   "source": [
    "print QA.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = [i[1] for i in userRep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11893.4646465\n"
     ]
    }
   ],
   "source": [
    "print reduce(lambda x, y: x + y, t) / len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[531] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upVote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(None, 838),\n",
       " ('10002', 1),\n",
       " ('10003', 1),\n",
       " ('10004', 1),\n",
       " ('10006', 1),\n",
       " ('1001', 1),\n",
       " ('10011', 1),\n",
       " ('10013', 1),\n",
       " ('10017', 23),\n",
       " ('10020', 1)]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100976, 92624, 47334, 46907, 32283, 27599, 25406, 23610, 23102, 22706, 20315, 20133, 19312, 18866, 17719, 16854, 16131, 14768, 14500, 14100, 13557, 13078, 12491, 12098, 11989, 11865, 11830, 11795, 11662, 11083, 10750, 10728, 10552, 10394, 10383, 10045, 9619, 9530, 9294, 9047, 8794, 8629, 8221, 8013, 7971, 7765, 7729, 7725, 7608, 7552, 7116, 6962, 6948, 6888, 6814, 6716, 6694, 6682, 6588, 6367, 6352, 6208, 6149, 6145, 6127, 6040, 5970, 5967, 5849, 5775, 5762, 5739, 5661, 5500, 5444, 5398, 5315, 5085, 5042, 4934, 4795, 4656, 4438, 4238, 4204, 4192, 4149, 4147, 4127, 4092, 4080, 4065, 3971, 3958, 3957, 3821, 3805, 3747, 3732]\n"
     ]
    }
   ],
   "source": [
    "print l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3 ## user_reputation_by_tenure\n",
    "If we use the total number of posts made on the site as a metric for tenure, we\n",
    "can look at the differences between \"younger\" and \"older\" users. You can\n",
    "imagine there might be many interesting features - for now just return the top\n",
    "100 post counts (of all types of posts) and the average reputation for every\n",
    "user who has that count.\n",
    "\n",
    "**Checkpoint:**\n",
    "Mean of top 100 post counts: 281.51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row_rdd = sc.textFile(localpath(\"small_data/allPosts\")).map(postsParse)\n",
    "\n",
    "postcount = row_rdd.filter(lambda x: x is not None and x.OwnerUserId is not None)\\\n",
    "               .map(lambda x: (x.OwnerUserId, 1))\\\n",
    "               .reduceByKey(lambda a, b: a + b).sortByKey(ascending=False)\n",
    "\n",
    "        \n",
    "Q3= postcount.leftOuterJoin(userRep)\\\n",
    "        .map(lambda (id, (x,y)): (id, (0 if x is None else x, 0 if y is None else y))).sortByKey(ascending=False) \\\n",
    "        .map(lambda (id, (x,y)): (x,y)).sortByKey(ascending=False)\n",
    "        \n",
    "# http://abshinn.github.io/python/apache-spark/2014/10/11/using-combinebykey-in-apache-spark/\n",
    "\n",
    "sumCount = Q3.combineByKey(lambda value: (value, 1),\n",
    "                             lambda x, value: (x[0] + value, x[1] + 1),\n",
    "                             lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "averageByKey = sumCount.map(lambda (label, (value_sum, count)): (label, value_sum / count)).sortByKey(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2325, (92624, 1)),\n",
       " (1663, (47334, 1)),\n",
       " (1287, (100976, 1)),\n",
       " (1018, (46907, 1)),\n",
       " (965, (23102, 1)),\n",
       " (695, (27599, 1)),\n",
       " (570, (22706, 1)),\n",
       " (558, (25406, 1)),\n",
       " (495, (9294, 1)),\n",
       " (494, (23610, 1))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa =Q3.aggregateByKey((0,0), lambda a,b: (a[0] + b,    a[1] + 1),\n",
    "                                      lambda a,b: (a[0] + b[0], a[1] + b[1])).sortByKey(ascending=False)\n",
    "aa.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2325, 92624),\n",
       " (1663, 47334),\n",
       " (1287, 100976),\n",
       " (1018, 46907),\n",
       " (965, 23102),\n",
       " (695, 27599),\n",
       " (570, 22706),\n",
       " (558, 25406),\n",
       " (495, 9294),\n",
       " (494, 23610)]"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q3.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2325, 92624.0), (1663, 47334.0), (1287, 100976.0), (1018, 46907.0), (965, 23102.0), (695, 27599.0), (570, 22706.0), (558, 25406.0), (495, 9294.0), (494, 23610.0), (469, 10728.0), (452, 32283.0), (424, 16854.0), (419, 17719.0), (395, 14100.0), (390, 20315.0), (369, 19312.0), (363, 6149.0), (350, 9047.0), (345, 14768.0), (343, 13557.0), (339, 11795.0), (338, 10045.0), (304, 16131.0), (301, 6352.0), (297, 20133.0), (292, 10552.0), (290, 8285.5), (287, 11083.0), (282, 10383.0), (277, 11830.0), (269, 7729.0), (268, 11989.0), (267, 7971.0), (265, 7765.0), (257, 13078.0), (248, 7608.0), (247, 12496.5), (239, 1.0), (234, 11307.5), (228, 11662.0), (226, 5775.0), (218, 5849.0), (211, 7552.0), (208, 6208.0), (202, 9530.0), (195, 9619.0), (193, 6682.0), (188, 12098.0), (187, 8013.0), (185, 4149.0), (184, 5762.0), (177, 5042.0), (173, 10394.0), (168, 7725.0), (167, 3957.0), (165, 6694.0), (164, 1544.0), (163, 6888.0), (161, 6367.0), (159, 7116.0), (157, 6040.0), (156, 4086.6666666666665), (155, 4204.0), (150, 5398.0), (147, 3821.0), (146, 4127.0), (145, 2189.0), (144, 4943.0), (140, 1063.0), (133, 8794.0), (132, 7404.5), (131, 1875.0), (128, 5085.0), (124, 3650.0), (122, 2401.0), (119, 6948.0), (118, 3736.5), (117, 5237.0), (114, 5970.0), (113, 1267.0), (112, 2052.5), (111, 2533.0), (110, 2992.0), (109, 8629.0), (107, 6430.5), (105, 3890.0), (103, 3747.0), (101, 2597.0), (99, 2465.3333333333335), (98, 1584.0), (96, 2336.0), (94, 4668.0), (90, 3315.5), (88, 3036.5), (87, 2110.0), (86, 1282.0), (85, 2054.3333333333335), (84, 3880.5), (83, 3237.5)]\n"
     ]
    }
   ],
   "source": [
    "print averageByKey.take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## quick_answers_by_hour\n",
    "How long do you have to wait to get your question answered? Look at the set of\n",
    "ACCEPTED answers which are posted less than three hours after question\n",
    "creation. What is the average number of these \"quick answers\" as a function of\n",
    "the hour of day the question was asked?  You should normalize by how many total\n",
    "accepted answers are garnered by questions posted in a given hour, just like\n",
    "we're counting how many quick accepted answers are garnered by questions posted\n",
    "in a given hour, eg. (quick accepted answers when question hour is 15 / total\n",
    "accepted answers when question hour is 15).\n",
    "\n",
    "Return a list, whose ith element correspond to ith hour (e.g. 0 -> midnight, 1\n",
    "-> 1:00, etc.)\n",
    "\n",
    "**Note:** When using Scala's SimpleDateFormat class, it's important to account\n",
    "for your machine's local time zone.  Our policy will be to use GMT:\n",
    "`hourFormat.setTimeZone(TimeZone.getTimeZone(\"GMT\"))`\n",
    "\n",
    "Question: What biases are present in our result, that we don't account for? How\n",
    "would we handle this?\n",
    "\n",
    "**Checkpoints:**\n",
    "Total quick accepted answers: 8,468;\n",
    "Total accepted answers: 17,096\n",
    "\n",
    "@fellow.app.task(name=\"spark.quick_answers_by_hour\")\n",
    "@typecheck.returns(\"24 * number\")\n",
    "def quick_answers_by_hour():\n",
    "    return [0.] * 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#hourFormat.setTimeZone(TimeZone.getTimeZone(\"GMT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row_rdd = sc.textFile(localpath(\"small_data/allPosts\")).map(postsParse)\n",
    "\n",
    "AccpQue = row_rdd.filter(lambda x: x is not None and x.PostTypeId == 1 and x.AcceptedAnswerId is not None)\\\n",
    "               .map(lambda x: (x.AcceptedAnswerId, x.CreationDate))\\\n",
    "              # .reduceByKey(lambda a, b: a + b).sortByKey(ascending=False)\n",
    "Ans = row_rdd.filter(lambda x: x is not None and x.PostTypeId == 2)\\\n",
    "               .map(lambda x: (x.Id, x.CreationDate))\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-685-9d0a68893b83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mAccpAnsTot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAccpQue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAns\u001b[0m\u001b[1;33m)\u001b[0m        \u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m        \u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m19\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"%Y-%m-%dT%H:%M:%S\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhour\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;33m)\u001b[0m        \u001b[1;33m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msortByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/opt/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msortByKey\u001b[1;34m(self, ascending, numPartitions, keyfunc)\u001b[0m\n\u001b[0;32m    627\u001b[0m         \u001b[1;31m# the key-space into bins such that the bins have roughly the same\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m         \u001b[1;31m# number of (key, value) pairs falling into them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 629\u001b[1;33m         \u001b[0mrddSize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    630\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mrddSize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m  \u001b[1;31m# empty RDD\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1004\u001b[0m         \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m         \"\"\"\n\u001b[1;32m-> 1006\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1008\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    995\u001b[0m         \u001b[1;36m6.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m         \"\"\"\n\u001b[1;32m--> 997\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[1;34m(self, zeroValue, op)\u001b[0m\n\u001b[0;32m    869\u001b[0m         \u001b[1;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m         \u001b[1;31m# to the final reduce call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m         \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    872\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    771\u001b[0m         \"\"\"\n\u001b[0;32m    772\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 773\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    774\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mEND_COMMAND_PART\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 536\u001b[1;33m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[0;32m    538\u001b[0m                 self.target_id, self.name)\n",
      "\u001b[1;32m/opt/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry)\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_give_back_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    471\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msendall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    474\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Answer received: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[1;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    432\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m                         \u001b[1;32mwhile\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m                             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m                                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "AccpAnsTot = AccpQue.join(Ans)\\\n",
    "        .map(lambda (id, (x,y)): (id, (0 if x is None else x, 0 if y is None else y)))\\\n",
    "        .map(lambda (id, (x,y)): (datetime.strptime(x[:19], \"%Y-%m-%dT%H:%M:%S\").hour, 1)   )\\\n",
    "        .reduceByKey(lambda a, b: a + b).sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "AccpAns = AccpQue.join(Ans)\\\n",
    "        .map(lambda (id, (x,y)): (id, (0 if x is None else x, 0 if y is None else y)))\\\n",
    "        .map(lambda (id, (x,y)): (id, (datetime.strptime(y[:19], \"%Y-%m-%dT%H:%M:%S\") -  datetime.strptime(x[:19], \"%Y-%m-%dT%H:%M:%S\")).total_seconds()//3600, datetime.strptime(x[:19], \"%Y-%m-%dT%H:%M:%S\").hour ) )\\\n",
    "        .filter(lambda x: x[1] < 3.0)\\\n",
    "        .map(lambda (id,x,y): (y,1))\\\n",
    "        .reduceByKey(lambda a, b: a + b).sortByKey()\n",
    "        \n",
    "        #.map(lambda (id, (x,y)): (x,y)).sortByKey(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "print AccpAnsTot.count()\n",
    "#total = AccpAnsTot.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('61518', ('2013-06-11T13:43:24.690', '2013-06-12T05:26:54.733')),\n",
       " ('117070', ('2014-09-28T17:36:17.667', '2014-09-28T17:36:17.667')),\n",
       " ('11542', ('2011-06-03T23:01:34.050', '2011-06-03T23:08:25.943')),\n",
       " ('126872', ('2014-12-06T04:31:36.763', '2014-12-06T06:10:05.283')),\n",
       " ('5988', ('2011-01-03T15:08:29.897', '2011-01-04T21:04:30.177'))]"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AccpAns.count()\n",
    "AccpAns.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "print AccpAns.count()\n",
    "quick = AccpAns.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:43:30\n",
      "15.0\n",
      "13\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "a = '2013-06-11T13:43:24.690'\n",
    "b = '2013-06-12T05:26:54.733'\n",
    "a[:19]\n",
    "print  (datetime.strptime(b[:19], \"%Y-%m-%dT%H:%M:%S\") -  datetime.strptime(a[:19], \"%Y-%m-%dT%H:%M:%S\"))\n",
    "print  (datetime.strptime(b[:19], \"%Y-%m-%dT%H:%M:%S\") -  datetime.strptime(a[:19], \"%Y-%m-%dT%H:%M:%S\")).total_seconds()//3600\n",
    "print datetime.strptime(a[:19], \"%Y-%m-%dT%H:%M:%S\").hour\n",
    "print  (datetime.strptime(b[:19], \"%Y-%m-%dT%H:%M:%S\") -  datetime.strptime(a[:19], \"%Y-%m-%dT%H:%M:%S\")).days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-06-11T13:43:24\n",
      "2013-06-11 13:43:24\n"
     ]
    }
   ],
   "source": [
    "nofrag, frag = a.split('.')\n",
    "print nofrag\n",
    "nofrag_dt = datetime.strptime(nofrag, \"%Y-%m-%dT%H:%M:%S\")\n",
    "print nofrag_dt\n",
    "#datetime.strptime(a, \"%Y-%m-%d %X%%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 535), (1, 540), (2, 502), (3, 458), (4, 422), (5, 400), (6, 385), (7, 475), (8, 535), (9, 636), (10, 698), (11, 718), (12, 763), (13, 917), (14, 1029), (15, 1019), (16, 1030), (17, 998), (18, 915), (19, 974), (20, 891), (21, 861), (22, 754), (23, 651)]\n"
     ]
    }
   ],
   "source": [
    "print total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8478\n",
      "17106\n"
     ]
    }
   ],
   "source": [
    "print sum(q)\n",
    "print sum(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 241), (1, 242), (2, 181), (3, 174), (4, 170), (5, 165), (6, 177), (7, 222), (8, 247), (9, 315), (10, 360), (11, 391), (12, 408), (13, 487), (14, 539), (15, 547), (16, 564), (17, 479), (18, 476), (19, 532), (20, 463), (21, 444), (22, 348), (23, 306)]\n"
     ]
    }
   ],
   "source": [
    "print quick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratio = []\n",
    "for i in range(0,24):\n",
    "    #print i\n",
    "    ratio.append(quick[i][1]/total[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4504672897196262, 0.44814814814814813, 0.3605577689243028, 0.3799126637554585, 0.4028436018957346, 0.4125, 0.4597402597402597, 0.4673684210526316, 0.4616822429906542, 0.49528301886792453, 0.5157593123209169, 0.5445682451253482, 0.5347313237221494, 0.5310796074154853, 0.5238095238095238, 0.5368007850834151, 0.5475728155339806, 0.47995991983967934, 0.5202185792349727, 0.5462012320328542, 0.5196408529741863, 0.5156794425087108, 0.46153846153846156, 0.4700460829493088]\n"
     ]
    }
   ],
   "source": [
    "print ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## quick_answers_by_hour_full\n",
    "Same as above, but on the full StackOverflow dataset.\n",
    "\n",
    "**Checkpoints:**\n",
    "Total quick accepted answers: 3,700,224;\n",
    "Total accepted answers: 5,086,888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row_rdd = sc.textFile(localpath(\"allPosts\")).map(postsParse)\n",
    "\n",
    "AccpQue = row_rdd.filter(lambda x: x is not None and x.PostTypeId == 1 and x.AcceptedAnswerId is not None)\\\n",
    "               .map(lambda x: (x.AcceptedAnswerId, x.CreationDate))\\\n",
    "              # .reduceByKey(lambda a, b: a + b).sortByKey(ascending=False)\n",
    "Ans = row_rdd.filter(lambda x: x is not None and x.PostTypeId == 2)\\\n",
    "               .map(lambda x: (x.Id, x.CreationDate))\\\n",
    "    \n",
    "AccpAnsTot = AccpQue.join(Ans)\\\n",
    "        .map(lambda (id, (x,y)): (id, (0 if x is None else x, 0 if y is None else y)))\\\n",
    "        .map(lambda (id, (x,y)): (datetime.strptime(x[:19], \"%Y-%m-%dT%H:%M:%S\").hour, 1)   )\\\n",
    "        .reduceByKey(lambda a, b: a + b).sortByKey()\n",
    "        \n",
    "AccpAns = AccpQue.join(Ans)\\\n",
    "        .map(lambda (id, (x,y)): (id, (0 if x is None else x, 0 if y is None else y)))\\\n",
    "        .map(lambda (id, (x,y)): (id, (datetime.strptime(y[:19], \"%Y-%m-%dT%H:%M:%S\") -  datetime.strptime(x[:19], \"%Y-%m-%dT%H:%M:%S\")).total_seconds()//3600, datetime.strptime(x[:19], \"%Y-%m-%dT%H:%M:%S\").hour ) )\\\n",
    "        .filter(lambda x: x[1] < 3.0)\\\n",
    "        .map(lambda (id,x,y): (y,1))\\\n",
    "        .reduceByKey(lambda a, b: a + b).sortByKey()\n",
    "        \n",
    "print AccpAnsTot.count()\n",
    "totalT = AccpAnsTot.collect()\n",
    "\n",
    "print AccpAns.count()\n",
    "quickT = AccpAns.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratio = []\n",
    "for i in range(0,24):\n",
    "    #print i\n",
    "    ratio.append(quickT[i][1]/totalT[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.690509573675678, 0.6960107966498631, 0.6996508810120343, 0.7043822789835892, 0.7101039989211401, 0.7178884888038708, 0.7242146432479951, 0.7270583958385681, 0.726148461144255, 0.722902538582882, 0.7285241068881352, 0.7372748898429328, 0.7444631254763009, 0.7452409780074815, 0.7414569607227338, 0.7344364120705962, 0.7349356935222515, 0.7395005567526688, 0.7463487204984385, 0.7475967835398525, 0.7360056557665117, 0.7158930708870861, 0.7017970445817332, 0.694720298548583]\n"
     ]
    }
   ],
   "source": [
    "print ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 136938), (1, 125965), (2, 121735), (3, 120896), (4, 126059), (5, 142192), (6, 167217), (7, 187051), (8, 206842), (9, 245964), (10, 252376), (11, 247601), (12, 254566), (13, 276685), (14, 299308), (15, 305569), (16, 284497), (17, 263133), (18, 257444), (19, 251954), (20, 244706), (21, 225570), (22, 191648), (23, 159706)]\n"
     ]
    }
   ],
   "source": [
    "print totalT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## identify_veterans_from_first_post_stats\n",
    "It can be interesting to think about what factors influence a user to remain\n",
    "active on the site over a long period of time.  In order not to bias the\n",
    "results towards older users, we'll define a time window between 100 and 150\n",
    "days after account creation. If the user has made a post in this time, we'll\n",
    "consider them active and well on their way to being veterans of the site; if\n",
    "not, they are inactive and were likely brief users.\n",
    "\n",
    "*Question*: What other parameterizations of \"activity\" could we use, and how\n",
    "would they differ in terms of splitting our user base?\n",
    "\n",
    "*Question*: What other biases are still not dealt with, after using the above\n",
    "approach?\n",
    "\n",
    "Let's see if there are differences between the first ever question posts of\n",
    "\"veterans\" vs. \"brief users\". For each group separately, average the score,\n",
    "views, number of answers, and number of favorites of users' first question.\n",
    "\n",
    "*Question*: What story could you tell from these numbers? How do the numbers\n",
    "support it?\n",
    "\n",
    "**Checkpoints:**\n",
    "Total brief users: 24,864;\n",
    "Total veteran users: 2,027\n",
    "\n",
    "\n",
    "def identify_veterans_from_first_post_stats():\n",
    "    return {\"vet_views\": 0.,\n",
    "            \"vet_score\": 0.,\n",
    "            \"vet_favorites\": 0.,\n",
    "            \"vet_answers\": 0.,\n",
    "            \"brief_views\": 0.,\n",
    "            \"brief_score\": 0.,\n",
    "            \"brief_favorites\": 0.,\n",
    "            \"brief_answers\": 0.\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return usersRecord(AboutMe, AccountId, CreationDate, DisplayName, DownVotes, Id, LastAccessDate, \n",
    "#                    Location, Reputation, UpVotes, Views, WebsiteUrl) \n",
    "#return postsRecord(AcceptedAnswerId, AnswerCount, Body, CommentCount, CreationDate, FavoriteCount, Id, LastActivityDate, \\\n",
    "#                    LastEditDate, LastEditorUserId, OwnerUserId, PostTypeId, Score, Tags, Title, ViewCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row_rdd = sc.textFile(localpath(\"small_data/allUsers\")).map(usersParse)\n",
    "\n",
    "users = row_rdd.filter(lambda x: x is not None and x.Id is not None)\\\n",
    "               .map(lambda x: (x.Id, x.CreationDate))\n",
    "\n",
    "row_rdd = sc.textFile(localpath(\"small_data/allPosts\")).map(postsParse)\n",
    "\n",
    "posts = row_rdd.filter(lambda x: x is not None and x.OwnerUserId is not None)\\\n",
    "               .map(lambda x: (x.OwnerUserId, x.CreationDate))\\\n",
    "               #.reduceByKey(lambda a, b: a + b).sortByKey()\n",
    "\n",
    "questions = row_rdd.filter(lambda x: x is not None and x.OwnerUserId is not None and x.PostTypeId == 1)\\\n",
    "               .map(lambda x: (x.OwnerUserId, x.CreationDate))\\\n",
    "        \n",
    "veterans = users.join(posts)\\\n",
    "        .map(lambda (id, (x,y)): (id, (0 if x is None else x, 0 if y is None else y)))\\\n",
    "        .map(lambda (id, (x,y)): (id, (datetime.strptime(y[:19], \"%Y-%m-%dT%H:%M:%S\") -  datetime.strptime(x[:19], \"%Y-%m-%dT%H:%M:%S\")).days)   )\\\n",
    "        .filter(lambda x: x[1] >= 100)\\\n",
    "        .filter(lambda x: x[1] <= 150)\\\n",
    "        .map(lambda (id, y): (id, 1))\\\n",
    "        .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "brief = users.join(posts)\\\n",
    "        .map(lambda (id, (x,y)): (id, (0 if x is None else x, 0 if y is None else y)))\\\n",
    "        .map(lambda (id, (x,y)): (id, (datetime.strptime(y[:19], \"%Y-%m-%dT%H:%M:%S\") -  datetime.strptime(x[:19], \"%Y-%m-%dT%H:%M:%S\")).days)   )\\\n",
    "        .filter(lambda x: x[1] < 100 or x[1] > 150)\\\n",
    "        .map(lambda (id, y): (id, 1))\\\n",
    "        .reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('-1', '2010-07-19T06:55:26.860'),\n",
       " ('2', '2010-07-19T14:01:36.697'),\n",
       " ('3', '2010-07-19T15:34:50.507'),\n",
       " ('4', '2010-07-19T19:03:27.400'),\n",
       " ('5', '2010-07-19T19:03:57.227'),\n",
       " ('6', '2010-07-19T19:04:07.647'),\n",
       " ('8', '2010-07-19T19:04:52.280'),\n",
       " ('11', '2010-07-19T19:06:02.713'),\n",
       " ('12', '2010-07-19T19:06:34.507'),\n",
       " ('13', '2010-07-19T19:06:49.527')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('7071', '2012-02-01T15:28:54.727'),\n",
       " ('401', '2012-02-01T15:44:24.457'),\n",
       " ('8882', '2012-02-01T16:02:49.273'),\n",
       " ('4872', '2012-02-01T16:14:10.550'),\n",
       " ('8844', '2012-02-01T16:14:37.607'),\n",
       " ('8884', '2012-02-01T16:21:00.013'),\n",
       " ('7411', '2012-02-01T16:22:42.353'),\n",
       " ('401', '2012-02-01T16:24:26.793'),\n",
       " ('8724', '2012-02-01T16:39:19.773'),\n",
       " ('199', '2012-02-01T16:59:02.603')]"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('7071', '2012-02-01T15:28:54.727'),\n",
       " ('401', '2012-02-01T15:44:24.457'),\n",
       " ('8882', '2012-02-01T16:02:49.273'),\n",
       " ('4872', '2012-02-01T16:14:10.550'),\n",
       " ('8844', '2012-02-01T16:14:37.607')]"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('27495', 1),\n",
       " ('16789', 2),\n",
       " ('39953', 15),\n",
       " ('11050', 1),\n",
       " ('10217', 4),\n",
       " ('8783', 9),\n",
       " ('52150', 1),\n",
       " ('54695', 1),\n",
       " ('39588', 3),\n",
       " ('2329', 1)]"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "veterans.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vepost = veterans.join(questions)\\\n",
    "        .map(lambda (id, (x,y)): (id, (0 if x is None else x, 0 if y is None else y)))\\\n",
    "        .map(lambda (id, (x,y)): (id,y)   ).sortByKey()\\\n",
    "        .reduceByKey(lambda a,b : a if datetime.strptime(a[:19], \"%Y-%m-%dT%H:%M:%S\") < datetime.strptime(b[:19], \"%Y-%m-%dT%H:%M:%S\") else b).sortByKey()\\\n",
    "        .map(lambda x: ((x[0], x[1]),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('10017', '2012-03-21T12:15:31.693'), 1),\n",
       " (('10026', '2012-05-30T17:31:50.220'), 1),\n",
       " (('1004', '2010-08-19T15:13:49.887'), 1),\n",
       " (('1005', '2010-09-30T16:19:47.023'), 1),\n",
       " (('10064', '2012-03-23T09:28:17.890'), 1)]"
      ]
     },
     "execution_count": 648,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vepost.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brpost = brief.join(questions)\\\n",
    "        .map(lambda (id, (x,y)): (id, (0 if x is None else x, 0 if y is None else y)))\\\n",
    "        .map(lambda (id, (x,y)): (id,y)   ).sortByKey()\\\n",
    "        .reduceByKey(lambda a,b : a if datetime.strptime(a[:19], \"%Y-%m-%dT%H:%M:%S\") < datetime.strptime(b[:19], \"%Y-%m-%dT%H:%M:%S\") else b).sortByKey()\\\n",
    "        .map(lambda x: ((x[0], x[1]),1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22924"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brpost.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('10002', '2012-03-20T18:45:26.960'), 1),\n",
       " (('10003', '2012-03-20T20:43:37.323'), 1),\n",
       " (('10004', '2012-03-20T21:35:53.837'), 1),\n",
       " (('10006', '2012-03-20T23:57:36.023'), 1),\n",
       " (('1001', '2010-08-19T11:33:01.067'), 1)]"
      ]
     },
     "execution_count": 666,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brpost.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For each group separately, average the    score, views, number of answers, and number of favorites of users' first question.\n",
    "#return postsRecord(AcceptedAnswerId, AnswerCount, Body, CommentCount, CreationDate, FavoriteCount, Id, LastActivityDate, \\\n",
    "#                    LastEditDate, LastEditorUserId, OwnerUserId, PostTypeId, Score, Tags, Title, ViewCount)\n",
    "class question1(object):\n",
    "    def __init__(self, OwnerUserId, CreationDate, Score, ViewCount,AnswerCount,FavoriteCount):\n",
    "        self.OwnerUserId   = OwnerUserId\n",
    "        self.CreationDate  = CreationDate\n",
    "        self.Score         = Score\n",
    "        self.ViewCount     = ViewCount\n",
    "        self.AnswerCount   = AnswerCount\n",
    "        self.FavoriteCount = FavoriteCount\n",
    "\n",
    "def question2(data):\n",
    "    return question1(data[0],data[1],data[2],data[3],data[4],data[5])\n",
    "\n",
    "row_rdd = sc.textFile(localpath(\"small_data/allPosts\")).map(postsParse)\n",
    "\n",
    "allquestions = row_rdd.filter(lambda x: x is not None and x.OwnerUserId is not None and x.PostTypeId == 1)\\\n",
    "               .map(lambda x: (x.OwnerUserId, x.CreationDate\\\n",
    "                               , 0 if x.Score is None else int(x.Score)\\\n",
    "                               , 0 if x.ViewCount is None else int(x.ViewCount)\\\n",
    "                               , 0 if x.AnswerCount is None else int(x.AnswerCount)\\\n",
    "                               , 0 if x.FavoriteCount is None else int(x.FavoriteCount)))\\\n",
    "                .map(lambda x: ((x[0], x[1]),(x[2],x[3],x[4],x[5])))\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vestat = vepost.join(allquestions)\\\n",
    "        .map(lambda (id, (x, (y1, y2, y3, y4))): (y1, y2, y3, y4)    )\n",
    "        #.map(lambda (id, (x,y)): (id, (0 if x is None else x, 0 if y is None else y)))\\\n",
    "        #.map(lambda (id, (x,y)): (id,y)   ).sortByKey()\\\n",
    "        #.reduceByKey(lambda a,b : a if datetime.strptime(a[:19], \"%Y-%m-%dT%H:%M:%S\") < datetime.strptime(b[:19], \"%Y-%m-%dT%H:%M:%S\") else b).sortByKey()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('4872', '2012-02-01T16:14:10.550'), (4, 447, 2, 0)),\n",
       " (('8884', '2012-02-01T16:21:00.013'), (5, 645, 3, 3)),\n",
       " (('7411', '2012-02-01T16:22:42.353'), (8, 2516, 4, 6)),\n",
       " (('4872', '2012-02-01T17:21:35.447'), (3, 115, 1, 0)),\n",
       " (('8896', '2012-02-01T17:53:21.863'), (2, 128, 1, 0))]"
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print allquestions.count()\n",
    "allquestions.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1843\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('10017', '2012-03-21T12:15:31.693'),\n",
       " ('10026', '2012-05-30T17:31:50.220'),\n",
       " ('1004', '2010-08-19T15:13:49.887'),\n",
       " ('1005', '2010-09-30T16:19:47.023'),\n",
       " ('10064', '2012-03-23T09:28:17.890')]"
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print vepost.count()\n",
    "vepost.sortByKey().take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(14, 4864, 2, 9), (3, 218, 1, 1), (1, 290, 0, 2), (1, 174, 1, 0), (7, 341, 1, 5)]\n"
     ]
    }
   ],
   "source": [
    "print vestat.take(5)\n",
    "vestat2 =  vestat.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scorev = [i[0] for i in vestat2]\n",
    "viewv = [i[1] for i in vestat2]\n",
    "ansv = [i[2] for i in vestat2]\n",
    "favv = [i[3] for i in vestat2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'brstatT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-0e731ac96632>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfavvT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvestatT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mscorebT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbrstatT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mviewbT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbrstatT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mansbT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbrstatT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'brstatT' is not defined"
     ]
    }
   ],
   "source": [
    "scorevT = [i[0] for i in vestatT]\n",
    "viewvT = [i[1] for i in vestatT]\n",
    "ansvT = [i[2] for i in vestatT]\n",
    "favvT = [i[3] for i in vestatT]\n",
    "\n",
    "scorebT = [i[0] for i in brstatT]\n",
    "viewbT = [i[1] for i in brstatT]\n",
    "ansbT = [i[2] for i in brstatT]\n",
    "favbT = [i[3] for i in brstatT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-104-37cb2f9b60b0>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-104-37cb2f9b60b0>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    print reduce(lambda x, y: x + y, scorevT) / len(scorevT)\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print reduce(lambda x, y: x + y, scorevT) / len(scorevT)\n",
    "print reduce(lambda x, y: x + y, viewvT) / len(viewvT)\n",
    "print reduce(lambda x, y: x + y, ansvT) / len(ansvT)\n",
    "print reduce(lambda x, y: x + y, favvT) / len(favvT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2561266751584585"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(lambda x, y: x + y, scorevT) / len(scorevT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2561266751584585"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(lambda x, y: x + y, scorevT) / len(scorevT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1841.8189951718825"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(lambda x, y: x + y, viewvT) / len(viewvT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.841804785404288"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(lambda x, y: x + y, ansvT) / len(ansvT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8661474978452715"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(lambda x, y: x + y, favvT) / len(favvT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.53228431905\n",
      "927.704286489\n",
      "1.29625610418\n",
      "1.29300054259\n"
     ]
    }
   ],
   "source": [
    "print reduce(lambda x, y: x + y, scorev) / len(scorev)\n",
    "print reduce(lambda x, y: x + y, viewv) / len(viewv)\n",
    "print reduce(lambda x, y: x + y, ansv) / len(ansv)\n",
    "print reduce(lambda x, y: x + y, favv) / len(favv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1222.25\n",
      "55.75\n",
      "73.25\n",
      "44.0\n"
     ]
    }
   ],
   "source": [
    "stat(vestat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brstat = brpost.join(allquestions)\\\n",
    "        .map(lambda (id, (x, (y1, y2, y3, y4))): (y1, y2, y3, y4)    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22925\n",
      "[(14, 4864, 2, 9), (3, 231, 1, 2), (3, 92, 2, 0), (0, 42, 0, 0), (4, 499, 3, 1)]\n"
     ]
    }
   ],
   "source": [
    "print brstat.count()\n",
    "print brstat.take(5)\n",
    "brstat2 =  brstat.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.21561613959\n",
      "585.388745911\n",
      "0.996990185387\n",
      "0.634765539804\n"
     ]
    }
   ],
   "source": [
    "scoreb = [i[0] for i in brstat2]\n",
    "viewb = [i[1] for i in brstat2]\n",
    "ansb = [i[2] for i in brstat2]\n",
    "favb = [i[3] for i in brstat2]\n",
    "\n",
    "print reduce(lambda x, y: x + y, scoreb) / len(scoreb)\n",
    "print reduce(lambda x, y: x + y, viewb) / len(viewb)\n",
    "print reduce(lambda x, y: x + y, ansb) / len(ansb)\n",
    "print reduce(lambda x, y: x + y, favb) / len(favb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## identify_veterans_from_first_post_stats_full\n",
    "Same thing on the full StackOverflow dataset.\n",
    "\n",
    "**Checkpoints:**\n",
    "Total brief users: 1,848,628;\n",
    "Total veteran users: 288,285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row_rdd = sc.textFile(localpath(\"allUsers\")).map(usersParse)\n",
    "\n",
    "users = row_rdd.filter(lambda x: x is not None and x.Id is not None)\\\n",
    "               .map(lambda x: (x.Id, x.CreationDate))\n",
    "\n",
    "row_rdd = sc.textFile(localpath(\"allPosts\")).map(postsParse)\n",
    "\n",
    "posts = row_rdd.filter(lambda x: x is not None and x.OwnerUserId is not None)\\\n",
    "               .map(lambda x: (x.OwnerUserId, x.CreationDate))\\\n",
    "               #.reduceByKey(lambda a, b: a + b).sortByKey()\n",
    "\n",
    "questions = row_rdd.filter(lambda x: x is not None and x.OwnerUserId is not None and x.PostTypeId == 1)\\\n",
    "               .map(lambda x: (x.OwnerUserId, x.CreationDate))\\\n",
    "\n",
    "allquestions = row_rdd.filter(lambda x: x is not None and x.OwnerUserId is not None and x.PostTypeId == 1)\\\n",
    "               .map(lambda x: (x.OwnerUserId, x.CreationDate\\\n",
    "                               , 0 if x.Score is None else int(x.Score)\\\n",
    "                               , 0 if x.ViewCount is None else int(x.ViewCount)\\\n",
    "                               , 0 if x.AnswerCount is None else int(x.AnswerCount)\\\n",
    "                               , 0 if x.FavoriteCount is None else int(x.FavoriteCount)))\\\n",
    "               .map(lambda x: ((x[0], x[1]),(x[2],x[3],x[4],x[5])))\n",
    "        \n",
    "#veterans = users.join(posts)\\\n",
    "#        .map(lambda (id, (x,y)): (id, (0 if x is None else x, 0 if y is None else y)))\\\n",
    "#        .map(lambda (id, (x,y)): (id, (datetime.strptime(y[:19], \"%Y-%m-%dT%H:%M:%S\") -  datetime.strptime(x[:19], \"%Y-%m-%dT%H:%M:%S\")).days)   )\\\n",
    "#        .filter(lambda x: x[1] >= 100)\\\n",
    "#        .filter(lambda x: x[1] <= 150)\\\n",
    "#        .map(lambda (id, y): (id, 1))\\\n",
    "#        .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "brief = users.join(posts)\\\n",
    "        .map(lambda (id, (x,y)): (id, (0 if x is None else x, 0 if y is None else y)))\\\n",
    "        .map(lambda (id, (x,y)): (id, (datetime.strptime(y[:19], \"%Y-%m-%dT%H:%M:%S\") -  datetime.strptime(x[:19], \"%Y-%m-%dT%H:%M:%S\")).days)   )\\\n",
    "        .filter(lambda x: x[1] < 100 or x[1] > 150)\\\n",
    "        .map(lambda (id, y): (id, 1))\\\n",
    "        .reduceByKey(lambda a, b: a + b)\n",
    "        \n",
    "### missing a step: actually need to take out veteran users from brief users!!!\n",
    "        \n",
    "#######\n",
    "\n",
    "#vepost = veterans.join(questions)\\\n",
    "#        .map(lambda (id, (x,y)): (id, (0 if x is None else x, 0 if y is None else y)))\\\n",
    "#        .map(lambda (id, (x,y)): (id,y)   ).sortByKey()\\\n",
    "#        .reduceByKey(lambda a,b : a if datetime.strptime(a[:19], \"%Y-%m-%dT%H:%M:%S\") < datetime.strptime(b[:19], \"%Y-%m-%dT%H:%M:%S\") else b).sortByKey()\\\n",
    "#        .map(lambda x: ((x[0], x[1]),1))\n",
    "        \n",
    "brpost = brief.join(questions)\\\n",
    "        .map(lambda (id, (x,y)): (id, (0 if x is None else x, 0 if y is None else y)))\\\n",
    "        .map(lambda (id, (x,y)): (id,y)   ).sortByKey()\\\n",
    "        .reduceByKey(lambda a,b : a if datetime.strptime(a[:19], \"%Y-%m-%dT%H:%M:%S\") < datetime.strptime(b[:19], \"%Y-%m-%dT%H:%M:%S\") else b).sortByKey()\\\n",
    "        .map(lambda x: ((x[0], x[1]),1))\n",
    "###\n",
    "\n",
    "        \n",
    "#vestat = vepost.join(allquestions)\\\n",
    "#        .map(lambda (id, (x, (y1, y2, y3, y4))): (y1, y2, y3, y4)    )\n",
    "    \n",
    "#brstat = brpost.join(allquestions)\\\n",
    "#        .map(lambda (id, (x, (y1, y2, y3, y4))): (y1, y2, y3, y4)    )\n",
    "\n",
    "#print vestat.count()\n",
    "#print vestat.take(5)\n",
    "#vestatT =  vestat.collect()\n",
    "\n",
    "#print brstat.count()\n",
    "#print brstat.take(5)\n",
    "#brstatT =  brstat.collect()\n",
    "\n",
    "brstat = brpost.join(allquestions)\\\n",
    "        .map(lambda (id, (x, (y1, y2, y3, y4))): (1, y1*1.0, y2*1.0, y3*1.0, y4*1.0)    )\n",
    "    \n",
    "y1 = brstat.map(lambda (k,y1,y2,y3,y4): (k,y1)).combineByKey(lambda value: (value, 1),\n",
    "                             lambda x, value: (x[0] + value, x[1] + 1),\n",
    "                             lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "b_score = y1.map(lambda (label, (value_sum, count)): (label, value_sum / count)).collect()\n",
    "\n",
    "y2 = brstat.map(lambda (k,y1,y2,y3,y4): (k,y2)).combineByKey(lambda value: (value, 1),\n",
    "                             lambda x, value: (x[0] + value, x[1] + 1),\n",
    "                             lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "b_views = y2.map(lambda (label, (value_sum, count)): (label, value_sum / count)).collect()\n",
    "\n",
    "y3 = brstat.map(lambda (k,y1,y2,y3,y4): (k,y3)).combineByKey(lambda value: (value, 1),\n",
    "                             lambda x, value: (x[0] + value, x[1] + 1),\n",
    "                             lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "b_answers = y3.map(lambda (label, (value_sum, count)): (label, value_sum / count)).collect()\n",
    "\n",
    "y4 = brstat.map(lambda (k,y1,y2,y3,y4): (k,y4)).combineByKey(lambda value: (value, 1),\n",
    "                             lambda x, value: (x[0] + value, x[1] + 1),\n",
    "                             lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "b_favorites = y4.map(lambda (label, (value_sum, count)): (label, value_sum / count)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2111691"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brief.count() #2111691"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1.3137305156649044)]\n",
      "[(1, 1219.2296000947456)]\n",
      "[(1, 1.5599007600902512)]\n",
      "[(1, 0.4635584856499941)]\n"
     ]
    }
   ],
   "source": [
    "print b_score\n",
    "print b_views\n",
    "print b_answers\n",
    "print b_favorites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://abshinn.github.io/python/apache-spark/2014/10/11/using-combinebykey-in-apache-spark/\n",
    "\n",
    "#sumCount = Q3.combineByKey(lambda value: (value, 1),\n",
    "#                             lambda x, value: (x[0] + value, x[1] + 1),\n",
    "#                             lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "#averageByKey = sumCount.map(lambda (label, (value_sum, count)): (label, value_sum / count)).sortByKey(ascending=False)\n",
    "\n",
    "brief = users.join(posts)\\\n",
    "        .map(lambda (id, (x,y)): (id, (0 if x is None else x, 0 if y is None else y)))\\\n",
    "        .map(lambda (id, (x,y)): (id, (datetime.strptime(y[:19], \"%Y-%m-%dT%H:%M:%S\") -  datetime.strptime(x[:19], \"%Y-%m-%dT%H:%M:%S\")).days)   )\\\n",
    "        .filter(lambda x: x[1] < 100)\\\n",
    "        .map(lambda (id, y): (id, 1))\\\n",
    "        .reduceByKey(lambda a, b: a + b)\n",
    "        \n",
    "        \n",
    "#######\n",
    "\n",
    "#vepost = veterans.join(questions)\\\n",
    "#        .map(lambda (id, (x,y)): (id, (0 if x is None else x, 0 if y is None else y)))\\\n",
    "#        .map(lambda (id, (x,y)): (id,y)   ).sortByKey()\\\n",
    "#        .reduceByKey(lambda a,b : a if datetime.strptime(a[:19], \"%Y-%m-%dT%H:%M:%S\") < datetime.strptime(b[:19], \"%Y-%m-%dT%H:%M:%S\") else b).sortByKey()\\\n",
    "#        .map(lambda x: ((x[0], x[1]),1))\n",
    "        \n",
    "brpost = brief.join(questions)\\\n",
    "        .map(lambda (id, (x,y)): (id, (0 if x is None else x, 0 if y is None else y)))\\\n",
    "        .map(lambda (id, (x,y)): (id,y)   ).sortByKey()\\\n",
    "        .reduceByKey(lambda a,b : a if datetime.strptime(a[:19], \"%Y-%m-%dT%H:%M:%S\") < datetime.strptime(b[:19], \"%Y-%m-%dT%H:%M:%S\") else b).sortByKey()\\\n",
    "        .map(lambda x: ((x[0], x[1]),1))\n",
    "        \n",
    "        \n",
    "\n",
    "brstat = brpost.join(allquestions)\\\n",
    "        .map(lambda (id, (x, (y1, y2, y3, y4))): (1, y1*1.0, y2*1.0, y3*1.0, y4*1.0)    )\n",
    "    \n",
    "y1 = brstat.map(lambda (k,y1,y2,y3,y4): (k,y1)).combineByKey(lambda value: (value, 1),\n",
    "                             lambda x, value: (x[0] + value, x[1] + 1),\n",
    "                             lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "b_score = y1.map(lambda (label, (value_sum, count)): (label, value_sum / count)).collect()\n",
    "\n",
    "#y2 = brstat.map(lambda (k,y1,y2,y3,y4): (k,y2)).combineByKey(lambda value: (value, 1),\n",
    "#                             lambda x, value: (x[0] + value, x[1] + 1),\n",
    "#                             lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "#b_views = y2.map(lambda (label, (value_sum, count)): (label, value_sum / count)).collect()\n",
    "\n",
    "#y3 = brstat.map(lambda (k,y1,y2,y3,y4): (k,y3)).combineByKey(lambda value: (value, 1),\n",
    "#                             lambda x, value: (x[0] + value, x[1] + 1),\n",
    "#                             lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "#b_answers = y3.map(lambda (label, (value_sum, count)): (label, value_sum / count)).collect()\n",
    "\n",
    "#y4 = brstat.map(lambda (k,y1,y2,y3,y4): (k,y4)).combineByKey(lambda value: (value, 1),\n",
    "#                             lambda x, value: (x[0] + value, x[1] + 1),\n",
    "#                             lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "#b_favorites = y4.map(lambda (label, (value_sum, count)): (label, value_sum / count)).collect()\n",
    "\n",
    "    \n",
    "#brstat2 = brpost.join(allquestions)\\\n",
    "#        .map(lambda (id, (x, (y1, y2, y3, y4))): (1, y1, y2, y3, y4)    )\n",
    "\n",
    "print b_score\n",
    "print b_views\n",
    "print b_answers\n",
    "print b_favorites\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.6347655398037078)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_favorites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1.3137305156649044)]\n",
      "[(1, 1219.2296000947456)]\n",
      "[(1, 1.5599007600902512)]\n",
      "[(1, 0.4635584856499941)]\n"
     ]
    }
   ],
   "source": [
    "print b_score\n",
    "print b_views\n",
    "print b_answers\n",
    "print b_favorites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## word2vec\n",
    "[Word2Vec](https://code.google.com/p/word2vec/) is an alternative approach for\n",
    "vectorizing text data. The vectorized representations of words in the\n",
    "vocabulary tend to be useful for predicting other words in the document,\n",
    "hence the famous example \"vector('king') - vector('man') + vector('woman')\n",
    "~= vector('queen')\".\n",
    "\n",
    "Let's see how good a Word2Vec model we can train using the tags of each\n",
    "StackOverflow post as documents (this uses the full dataset). Use Spark ML's\n",
    "implementation of Word2Vec (this will require using DataFrames) to return a\n",
    "list of the top 25 closest synonyms to \"ggplot2\" and their similarity score\n",
    "in tuple format (\"string\", number).\n",
    "\n",
    "### Parameters\n",
    "The dimensionality of the vector space should be 100.\n",
    "The random seed should be `42L`.\n",
    "\n",
    "**Checkpoint:**\n",
    "Mean of the top 25 cosine similarities: 2.5444579897502217\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# needed to convert RDDs into DataFrames\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "from pyspark.mllib.linalg import Vectors, SparseVector\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "from pyspark.mllib.linalg import Matrices\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "from pyspark.ml.feature import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row_rdd = sc.textFile(localpath(\"small_data/allPosts\")).map(postsParse)\n",
    "\n",
    "postbody = row_rdd.filter(lambda x: x is not None)\\\n",
    "               .map(lambda x: (x.Body))\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I guess you can consider $\\\\beta_{1}x_{1}+ \\\\beta_{2}x_{1}^{2}$ as a quadratic effect. Depending on the sign of $\\\\beta_1$, for increasing values of $x_1$ the quadratic effect may increase or decrease. We can consider the effects of other covariates by holding the quadratic effect constant. ',\n",
       " \"I'm looking for a good source of information about Analysis of Molecular Variance (AMOVA), a kind of statistical analysis used in population genetics. Especially how are the $\\\\phi$-statistics calculated and what is the difference compare to F-statistics (fixation index) in measuring the level of differentiation among diffrent population divisions. \",\n",
       " 'I applied SVM to perform the classification against several data sets. It turns out that the performance metric of recall is pretty bad for one data set. It has recall around 50% while other data sets have recall around 80%. For this kind of scenario, what are the possible approaches that are available to improve the recall? Besides, why some data sets can have a poor performance in terms of recall? How to analyze this kind of problem?',\n",
       " 'For questions focusing on the Internet, please use the internet tag.',\n",
       " 'You could try a radar chart. One of the dimensions could be used to animate it, or you could have all temporal dimensions as lines in the chart. As the temporal values move outwards, you might be able to see a pattern in the movement of the other dimensions. Similarly, you could try parallel coordinates.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postbody.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a c a c a c a c a c a c a c a c a c a c \n",
      "['a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a c a c a c a c a c a c a c a c a c a c ', 'a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a b a c a c a c a c a c a c a c a c a c a c ']\n",
      "[(u'b', 0.37951410827997845), (u'c', -0.62407121541737132)]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"a b \" * 100 + \"a c \" * 10\n",
    "localDoc = [sentence, sentence]\n",
    "doc = sc.parallelize(localDoc).map(lambda line: line.split(\" \"))\n",
    "model = Word2Vec().setVectorSize(100).setSeed(42).fit(doc)\n",
    "syms = model.findSynonyms(\"a\", 2)\n",
    "print sentence\n",
    "print localDoc\n",
    "doc.collect()\n",
    "print syms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3585838"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "srow_rdd = sc.textFile(localpath(\"small_data/allPosts\")).map(postsParse)\n",
    "\n",
    "spostbody = srow_rdd.filter(lambda x: x is not None)\\\n",
    "               .map(lambda x: (x.Body.lower()))\n",
    "sinp = spostbody.map(lambda row: re.split('\\W+', row))\n",
    "#sc = SparkContext(appName='Word2Vec')\n",
    "sinp2 = spostbody.flatMap(lambda row: row.split(\" \"))\n",
    "sinp3 = spostbody.flatMap(lambda row: re.split('\\W+', row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143093453"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141100836"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how', 'does', 'more', 'like', 'this', 'work', 'does', 'it', 'look', 'at']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp3.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "word2Vec = Word2Vec(vectorSize=100,seed=42,inputCol=\"text\", outputCol=\"result\")\n",
    "\n",
    "# text = sc.parallelize(reviews + test_reviews).map(lambda (line, score): (line.split(\" \"), score)).toDF(['text', 'score'])\n",
    "#gutenberg = inp.toDF(['text', 'score'])\n",
    "#w2v = Word2Vec(vectorSize=100,seed=42,inputCol=\"text\", outputCol=\"result\")\n",
    "#modelo = w2v.fit(gutenberg)\n",
    "#result = modelo.transform(gutenberg)\n",
    "\n",
    "gutenberg = spostbody.map(lambda line: (line.split(\" \"), 1)).toDF(['text', 'score'])\n",
    "w2v = Word2Vec(vectorSize=100,seed=42,inputCol=\"text\", outputCol=\"result\")\n",
    "model = w2v.fit(gutenberg)\n",
    "result = model.transform(gutenberg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from pyspark.mllib.feature import Word2Vec\n",
    "#model2 = Word2Vec().setVectorSize(100).setSeed(42).fit(inp3)\n",
    "#modelt = Word2Vec().fit(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "york: 0.706269518918\n",
      "library: 0.700642679334\n",
      "studio: 0.672101664024\n",
      "programming: 0.662908905223\n",
      "vegan: 0.662726610624\n",
      "hadoop: 0.659086228342\n",
      "tutorials: 0.657229287778\n",
      "gnuplot: 0.653202214884\n",
      "manuals: 0.650310176978\n",
      "stats: 0.650064441772\n",
      "forge: 0.648824565421\n",
      "springer: 0.64689583673\n",
      "videos: 0.646455695312\n",
      "weka: 0.640037838323\n",
      "python: 0.63911967139\n",
      "interactive: 0.634160543741\n",
      "recognition: 0.632137293002\n",
      "interface: 0.631327763719\n",
      "companion: 0.630654299657\n",
      "app: 0.629391831615\n",
      "e1071: 0.627175836959\n",
      "bugs: 0.626625687639\n",
      "matlab: 0.626041942052\n",
      "graphics: 0.623361238706\n",
      "breiman: 0.622651315086\n",
      "toolkit: 0.622407363483\n",
      "doc: 0.621743356256\n",
      "analytics: 0.621712592815\n",
      "revolution: 0.621271661314\n",
      "wayne: 0.619078115782\n",
      "download: 0.617132784099\n",
      "reshape: 0.615011773231\n",
      "nz: 0.61371047661\n",
      "scipy: 0.611591712079\n",
      "publications: 0.610867362685\n",
      "server: 0.609772475382\n",
      "nlp: 0.6090878086\n",
      "sankey: 0.608451342959\n",
      "java: 0.608370770147\n",
      "simon: 0.608106092025\n"
     ]
    }
   ],
   "source": [
    "synonyms = modelt.findSynonyms('ggplot2', 40)\n",
    "\n",
    "for word, cosine_distance in synonyms:\n",
    "    print(\"{}: {}\".format(word, cosine_distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o69.trainWord2VecModel.\n: java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1842)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1534)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.scala:137)\n\tat scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.scala:135)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)\n\tat scala.collection.mutable.HashTable$class.serializeTo(HashTable.scala:124)\n\tat scala.collection.mutable.HashMap.serializeTo(HashMap.scala:39)\n\tat scala.collection.mutable.HashMap.writeObject(HashMap.scala:135)\n\tat sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1028)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-46e1d4c8bdb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#inp = postbody.map(lambda row: re.split('\\W+', row)) #.map(lambda word: word.lower())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#word2vec = Word2Vec()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetVectorSize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetSeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0msynonyms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindSynonyms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ggplot2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/spark/python/pyspark/mllib/feature.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    577\u001b[0m                                \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearningRate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumPartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m                                \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumIterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m                                int(self.minCount))\n\u001b[0m\u001b[0;32m    580\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mWord2VecModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/spark/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallMLlibFunc\u001b[1;34m(name, *args)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[0mapi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonMLLibAPI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/spark/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[1;34m(sc, func, *args)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o69.trainWord2VecModel.\n: java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1842)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1534)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.scala:137)\n\tat scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.scala:135)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)\n\tat scala.collection.mutable.HashTable$class.serializeTo(HashTable.scala:124)\n\tat scala.collection.mutable.HashMap.serializeTo(HashMap.scala:39)\n\tat scala.collection.mutable.HashMap.writeObject(HashMap.scala:135)\n\tat sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1028)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.feature import Word2Vec\n",
    "\n",
    "#inp = postbody.map(lambda row: re.split('\\W+', row)) #.map(lambda word: word.lower())\n",
    "#word2vec = Word2Vec()\n",
    "model = Word2Vec().setVectorSize(100).setSeed(42).fit(inp)\n",
    "\n",
    "synonyms = model.findSynonyms('ggplot2', 25)\n",
    "\n",
    "for word, cosine_distance in synonyms:\n",
    "    print(\"('{}', {}),\".format(word, cosine_distance))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Word2Vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-5c73e1366153>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0minp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpostbody\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\W+'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m#inp = postbody.map(lambda row: row.split(\" \"))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mword2vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;31m#random.seed(42)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Word2Vec' is not defined"
     ]
    }
   ],
   "source": [
    "#from pyspark import SparkContext\n",
    "#from pyspark.mllib.feature import Word2Vec\n",
    "\n",
    "#from pyspark import SparkContext\n",
    "#sc = SparkContext(\"local[*]\", \"temp\")\n",
    "#print sc.version\n",
    "\n",
    "\n",
    "row_rdd = sc.textFile(localpath(\"small_data/allPosts\")).map(postsParse)\n",
    "\n",
    "postbody = row_rdd.filter(lambda x: x is not None)\\\n",
    "               .map(lambda x: (x.Body))\\\n",
    "    \n",
    "#sc = SparkContext(appName='Word2Vec')\n",
    "inp = postbody.map(lambda row: re.split('\\W+', row))\n",
    "#inp = postbody.map(lambda row: row.split(\" \"))\n",
    "word2vec = Word2Vec()\n",
    "#random.seed(42)\n",
    "model = word2vec.fit(inp)\n",
    "\n",
    "synonyms = model.findSynonyms('ggplot2', 25)\n",
    "\n",
    "for word, cosine_distance in synonyms:\n",
    "    print(\"('{}', {}),\".format(word, cosine_distance))\n",
    "    \n",
    "\n",
    "sc = SparkContext(appName='Word2Vec')\n",
    "inp = sc.textFile(\"text8_lines\").map(lambda row: row.split(\" \"))\n",
    "\n",
    "word2vec = Word2Vec()\n",
    "model = word2vec.fit(inp)\n",
    "\n",
    "synonyms = model.findSynonyms('china', 40)\n",
    "\n",
    "for word, cosine_distance in synonyms:\n",
    "    print(\"{}: {}\".format(word, cosine_distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Word2VecModel' object has no attribute 'layer_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-b4b3be32bd93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Word2VecModel' object has no attribute 'layer_size'"
     ]
    }
   ],
   "source": [
    "model.layer_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(text=[u'I', u'guess', u'you', u'can', u'consider', u'beta_', u'1', u'x_', u'1', u'beta_', u'2', u'x_', u'1', u'2', u'as', u'a', u'quadratic', u'effect', u'Depending', u'on', u'the', u'sign', u'of', u'beta_1', u'for', u'increasing', u'values', u'of', u'x_1', u'the', u'quadratic', u'effect', u'may', u'increase', u'or', u'decrease', u'We', u'can', u'consider', u'the', u'effects', u'of', u'other', u'covariates', u'by', u'holding', u'the', u'quadratic', u'effect', u'constant', u''], score=1),\n",
       " Row(text=[u'I', u'm', u'looking', u'for', u'a', u'good', u'source', u'of', u'information', u'about', u'Analysis', u'of', u'Molecular', u'Variance', u'AMOVA', u'a', u'kind', u'of', u'statistical', u'analysis', u'used', u'in', u'population', u'genetics', u'Especially', u'how', u'are', u'the', u'phi', u'statistics', u'calculated', u'and', u'what', u'is', u'the', u'difference', u'compare', u'to', u'F', u'statistics', u'fixation', u'index', u'in', u'measuring', u'the', u'level', u'of', u'differentiation', u'among', u'diffrent', u'population', u'divisions', u''], score=1)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_rdd = sc.textFile(localpath(\"small_data/allPosts\")).map(postsParse)\n",
    "\n",
    "#row_rdd.take(2)\n",
    "postbody = row_rdd.filter(lambda x: x is not None)\\\n",
    "               .map(lambda x: (x.Body))\n",
    "#postbody.take(2)\n",
    "\n",
    "postbody2 = postbody.map(lambda line: (re.split('\\W+', line), 1))\\\n",
    "            .toDF(['text', 'score'])\n",
    "postbody2.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "postbody3 = postbody.map(lambda line: re.split('\\W+', line)).reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'c', 'a', 'c', 'a', 'c', 'a', 'c', 'a', 'c', 'a', 'c', 'a', 'c', 'a', 'c', 'a', 'c', 'a', 'c', '']\n",
      "[Row(sentence=[u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'b', u'a', u'c', u'a', u'c', u'a', u'c', u'a', u'c', u'a', u'c', u'a', u'c', u'a', u'c', u'a', u'c', u'a', u'c', u'a', u'c', u''])]\n"
     ]
    }
   ],
   "source": [
    "sent = (\"a b \" * 100 + \"a c \" * 10).split(\" \")\n",
    "doc = sqlContext.createDataFrame([(sent,)], [\"sentence\"])\n",
    "print sent\n",
    "print doc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#return postsRecord(AcceptedAnswerId, AnswerCount, Body, CommentCount, CreationDate, FavoriteCount, Id, LastActivityDate, \\\n",
    "#                    LastEditDate, LastEditorUserId, OwnerUserId, PostTypeId, Score, Tags, Title, ViewCount)\n",
    "\n",
    "#word2Vec = Word2Vec(vectorSize=100,seed=42,inputCol=\"text\", outputCol=\"result\")\n",
    "#model = word2Vec.fit(documentDF)\n",
    "\n",
    "#from pyspark.sql import SQLContext\n",
    "#sqlContext = SQLContext(sc)\n",
    "\n",
    "row_rdd = sc.textFile(localpath(\"small_data/allPosts\")).map(postsParse)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "postbody = row_rdd.filter(lambda x: x is not None and x.Body is not None)\\\n",
    "               .map(lambda x: (x.Body.lower()))\\\n",
    "\n",
    "###\n",
    "\n",
    "postbody2 = postbody.map(lambda line: (re.split('\\W+', line), 1))\\\n",
    "            #.toDF(['text', 'score'])\n",
    "\n",
    "doc = sqlContext.createDataFrame(postbody2,['text','score'])\n",
    "#postbody3 = postbody.map(lambda line: (re.split('\\W+', line), 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "w2v = Word2Vec(vectorSize=100,seed=42,inputCol=\"text\", outputCol=\"result\")\n",
    "model2 = w2v.fit(doc)\n",
    "#result = model.transform(postbody2)\n",
    "\n",
    "#vectors = model.getVectors().rdd.map(lambda x: (x.word, x.vector))\n",
    "\n",
    "#ggplot2_vec = vectors.lookup('ggplot2')[0]\n",
    "\n",
    "#print ggplot2_vec\n",
    "#vdistance = vectors.map(lambda x: (x[0], x[1].squared_distance(ggplot2_vec))).takeOrdered(30, key = lambda x: x[1])\n",
    "#print vdistance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|       word|        similarity|\n",
      "+-----------+------------------+\n",
      "|       york|0.7062695189183829|\n",
      "|    library| 0.700642679334344|\n",
      "|     studio|0.6721016640239801|\n",
      "|programming|0.6629089052234611|\n",
      "|      vegan|0.6627266106244828|\n",
      "|     hadoop|0.6590862283416311|\n",
      "|  tutorials|0.6572292877782048|\n",
      "|    gnuplot|0.6532022148843897|\n",
      "|    manuals|0.6503101769775328|\n",
      "|      stats| 0.650064441771754|\n",
      "|      forge|0.6488245654211844|\n",
      "|   springer|0.6468958367302863|\n",
      "|     videos|0.6464556953121021|\n",
      "|       weka|0.6400378383232235|\n",
      "|     python|0.6391196713902423|\n",
      "|interactive|0.6341605437414961|\n",
      "|recognition|0.6321372930015228|\n",
      "|  interface|0.6313277637194156|\n",
      "|  companion|0.6306542996574925|\n",
      "|        app| 0.629391831614645|\n",
      "|      e1071|0.6271758369589053|\n",
      "|       bugs|0.6266256876390509|\n",
      "|     matlab|0.6260419420518545|\n",
      "|   graphics|0.6233612387063332|\n",
      "|    breiman|0.6226513150862493|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model2.findSynonyms(\"ggplot2\", 25).show(25)\n",
    "\n",
    "#top25.show(25)\n",
    "#result = model.transform(postbody2)\n",
    "#for feature in result.select(\"result\").take(3):\n",
    "#    print(feature)\n",
    "\n",
    "#vectors = model.getVectors().rdd.map(lambda x: (x.word, x.vector))\n",
    "\n",
    "#ggplot2_vec = vectors.lookup('ggplot2')[0]\n",
    "\n",
    "#print ggplot2_vec\n",
    "#vdistance = vectors.map(lambda x: (x[0], x[1].squared_distance(ggplot2_vec))).takeOrdered(25, key = lambda x: x[1])\n",
    "#print vdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row_rdd = sc.textFile(localpath(\"allPosts\")).map(postsParse)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tags = row_rdd.filter(lambda x: x is not None and x.Tags is not None)\\\n",
    "               .map(lambda x: (x.Tags.strip()))\n",
    "\n",
    "gutenberg = tags.map(lambda line: (line.split(\" \"), 1)).toDF(['text', 'score']) \n",
    "    \n",
    "w2v = Word2Vec(inputCol=\"text\", outputCol=\"vectors\").setMinCount(1)\n",
    "model = w2v.setVectorSize(100).setSeed(42L).fit(gutenberg)\n",
    "result = model.transform(gutenberg)\n",
    "\n",
    "synonyms = model.findSynonyms('ggplot2', 25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|      word|        similarity|\n",
      "+----------+------------------+\n",
      "|   lattice| 2.490866938763332|\n",
      "|    r-grid| 2.388864482720233|\n",
      "|data.table| 2.371288863636657|\n",
      "|      plyr| 2.355951655533203|\n",
      "|     shiny| 2.345726703908764|\n",
      "|        na|2.3455225892306957|\n",
      "|   boxplot|2.3284574917794396|\n",
      "|data.frame| 2.319748501568665|\n",
      "|        lm| 2.278327247826568|\n",
      "|  geom-bar| 2.259660307196463|\n",
      "|  r-factor|2.2575083970519465|\n",
      "| levelplot| 2.256077775664676|\n",
      "|   plotrix| 2.255963239570896|\n",
      "|   reshape|2.2533174999499295|\n",
      "|    tapply| 2.235029644078851|\n",
      "| geom-text| 2.234737233123434|\n",
      "|    subset| 2.232513243551458|\n",
      "| gridextra| 2.227716426353371|\n",
      "|subsetting|2.2223105724034973|\n",
      "|      grob| 2.222196589506796|\n",
      "|facet-wrap| 2.220145368952026|\n",
      "|       rgl| 2.220144235505828|\n",
      "|       zoo|2.2198036259615757|\n",
      "|     rbind| 2.217788123204816|\n",
      "|   do.call| 2.217059981328256|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "synonyms.show(25)\n",
    "#print gutenberg.take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import lxml.etree as ET\n",
    "import pandas\n",
    "import datetime as dt, dateutil.parser\n",
    "import re\n",
    "def isHeader(line):\n",
    "    return \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\"\"\" in line\n",
    "def isNone(line):\n",
    "    return \"None\" in line\n",
    "def localpath(path):\n",
    "    return 'file://' + str(os.path.abspath(os.path.curdir)) + '/' + path\n",
    "              \n",
    "class postsRecord(object):\n",
    "    def __init__(self, Tags):\n",
    "        self.Tags  = Tags\n",
    "    \n",
    "def postsParse(xmlString):\n",
    "    try:\n",
    "        rows = ET.fromstring(xmlString.encode('utf-8'))\n",
    "#        Tags = rows.get('Tags')\n",
    "        Tags = re.sub( ur\">\", \" \", re.sub(ur\"<\", \"\",rows.get('Tags')))       \n",
    "        return postsRecord(Tags)\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "posts = sc.textFile(localpath(\"/allPosts/*\"))\\\n",
    "        .map(postsParse)\\\n",
    "        .filter(lambda x: x is not None)\\\n",
    "        .map(lambda x: (x.Tags.strip(), 0))\n",
    "        \n",
    "gutenberg = posts.map(lambda (line, x): (line.split(\" \"), 1)).toDF(['text', 'score'])       \n",
    "w2v = Word2Vec(inputCol=\"text\", outputCol=\"vectors\").setMinCount(1)\n",
    "model = w2v.setVectorSize(100).setSeed(42L).fit(gutenberg)\n",
    "result = model.transform(gutenberg)\n",
    "vectors = model.getVectors().rdd.map(lambda x: (x.word, x.vector))\n",
    "vectors = model.getVectors().rdd.map(lambda x: (x.word, x.vector))\n",
    "synonyms = model.findSynonyms('ggplot2', 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "postbody3 = postbody.map(lambda line: (re.split('\\W+', line), 1)).reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#vdistance = vectors.map(lambda x: (x[0], x[1].similarity(ggplot2_vec))).takeOrdered(30, key = lambda x: x[1])\n",
    "#trained_model.similarity('woman', 'man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.0348, 0.0708, 0.0742, -0.0676, -0.0776, 0.0262, -0.0391, -0.0203, 0.0931, -0.0393, 0.0764, 0.0301, 0.1241, -0.0638, -0.0841, 0.1015, 0.0831, 0.1151, -0.0334, -0.0256, -0.0663, -0.0521, 0.0047, -0.0659, 0.0302, -0.0804, 0.0086, 0.1597, 0.0351, 0.0907, 0.024, 0.1271, 0.0925, -0.0601, -0.0479, 0.1641, 0.0441, 0.2279, 0.0119, 0.0604, 0.0913, -0.064, -0.0736, -0.0144, 0.004, 0.0703, 0.0116, -0.0528, 0.1349, -0.1563, -0.0148, 0.0985, -0.0311, 0.0034, -0.0537, -0.1782, 0.1178, 0.0485, 0.0013, -0.1084, 0.0152, 0.0462, 0.068, 0.1496, 0.1813, 0.1928, 0.2012, -0.0396, 0.0065, 0.0659, -0.013, -0.1244, -0.0401, 0.0884, 0.0381, -0.0003, 0.0829, 0.0548, -0.141, 0.0565, 0.0871, 0.0838, -0.1245, 0.03, -0.0971, 0.0122, 0.1067, 0.0466, 0.131, 0.0129, -0.0695, 0.0755, -0.0703, -0.0456, 0.0925, -0.1401, 0.0337, -0.0026, 0.1373, 0.045])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ggplot2_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'ggplot2', 0.0),\n",
       " (u'Weka', 0.23475422303218832),\n",
       " (u'reshape', 0.28511764052379274),\n",
       " (u'dlm', 0.28526517210183677),\n",
       " (u'homals', 0.29041871495733851),\n",
       " (u'scipy', 0.2910591519794693),\n",
       " (u'excel', 0.29574289924287023),\n",
       " (u'commands', 0.29938388690792678),\n",
       " (u'demo', 0.30389997453725881),\n",
       " (u'libsvm', 0.30412186379358674),\n",
       " (u'vegan', 0.30413122540861842),\n",
       " (u'companion', 0.3066950821458882),\n",
       " (u'vignette', 0.30748505752221394),\n",
       " (u'googling', 0.30972463765331421),\n",
       " (u'Holt', 0.31234435177917869),\n",
       " (u'python', 0.31363030810093484),\n",
       " (u'graphics', 0.31499559486423168),\n",
       " (u'nlm', 0.31956438511739937),\n",
       " (u'lmerTest', 0.32357826715332905),\n",
       " (u'Kernel', 0.32520283301509811),\n",
       " (u'e1071', 0.32617779089227211),\n",
       " (u'interactive', 0.327094388896779),\n",
       " (u'lrm', 0.34083772917008859),\n",
       " (u'lme4', 0.34449510887459905),\n",
       " (u'server', 0.34451079085738534),\n",
       " (u'app', 0.34518710286451126),\n",
       " (u'mice', 0.34593293501964362),\n",
       " (u'interface', 0.34696307939083232),\n",
       " (u'Strategies', 0.34843516880494663),\n",
       " (u'gamlss', 0.34895059869595813)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I guess you can consider $\\\\beta_{1}x_{1}+ \\\\beta_{2}x_{1}^{2}$ as a quadratic effect. Depending on the sign of $\\\\beta_1$, for increasing values of $x_1$ the quadratic effect may increase or decrease. We can consider the effects of other covariates by holding the quadratic effect constant. ',\n",
       " \"I'm looking for a good source of information about Analysis of Molecular Variance (AMOVA), a kind of statistical analysis used in population genetics. Especially how are the $\\\\phi$-statistics calculated and what is the difference compare to F-statistics (fixation index) in measuring the level of differentiation among diffrent population divisions. \",\n",
       " 'I applied SVM to perform the classification against several data sets. It turns out that the performance metric of recall is pretty bad for one data set. It has recall around 50% while other data sets have recall around 80%. For this kind of scenario, what are the possible approaches that are available to improve the recall? Besides, why some data sets can have a poor performance in terms of recall? How to analyze this kind of problem?',\n",
       " 'For questions focusing on the Internet, please use the internet tag.',\n",
       " 'You could try a radar chart. One of the dimensions could be used to animate it, or you could have all temporal dimensions as lines in the chart. As the temporal values move outwards, you might be able to see a pattern in the movement of the other dimensions. Similarly, you could try parallel coordinates.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postbody.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "word2Vec = Word2Vec(vectorSize=100,seed=42,inputCol=\"text\", outputCol=\"result\")\n",
    "\n",
    "# text = sc.parallelize(reviews + test_reviews).map(lambda (line, score): (line.split(\" \"), score)).toDF(['text', 'score'])\n",
    "#gutenberg = inp.toDF(['text', 'score'])\n",
    "#w2v = Word2Vec(vectorSize=100,seed=42,inputCol=\"text\", outputCol=\"result\")\n",
    "#modelo = w2v.fit(gutenberg)\n",
    "#result = modelo.transform(gutenberg)\n",
    "\n",
    "gutenberg = spostbody.map(lambda line: (line.split(\" \"), 1)).toDF(['text', 'score'])\n",
    "w2v = Word2Vec(vectorSize=100,seed=42,inputCol=\"text\", outputCol=\"result\")\n",
    "model = w2v.fit(gutenberg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "postbody2 = postbody.map(lambda line: (line.split(\" \"), 1))\\\n",
    "            .toDF(['text', 'score'])\n",
    "w2v = Word2Vec(inputCol=\"text\", outputCol=\"vectors\")\n",
    "model = w2v.fit(postbody2)\n",
    "result = model.transform(postbody2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectors = model.getVectors().rdd.map(lambda x: (x.word, x.vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#improve_vec = vectors.lookup('improve')[0]\n",
    "ggplot2_vec = vectors.lookup('ggplot2')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-0.0525, 0.0029, 0.0107, 0.0145, 0.0252, -0.0414, 0.0262, 0.0368, -0.0828, 0.0135, 0.0353, 0.0211, 0.0811, -0.0303, 0.0503, 0.0264, 0.0187, 0.043, 0.0143, 0.0195, 0.0113, -0.0573, 0.0181, 0.0205, 0.0068, 0.0461, 0.0166, -0.0095, 0.0196, -0.0226, -0.0002, 0.0316, -0.0181, 0.0386, -0.0511, 0.0049, 0.0455, -0.0317, -0.0052, -0.0414, -0.0636, 0.0258, 0.0096, -0.0662, -0.0552, -0.0247, 0.0488, -0.0423, -0.0055, -0.0333, 0.0035, -0.0234, 0.0295, 0.0608, -0.0162, -0.1031, 0.0127, -0.0673, -0.0182, 0.0236, 0.0127, -0.0039, 0.0111, -0.0518, 0.0025, 0.0541, -0.0743, 0.0266, -0.0033, -0.0502, -0.0292, 0.0294, 0.0086, -0.0969, -0.0268, 0.0038, -0.0113, 0.0295, 0.018, -0.0273, 0.0253, -0.0276, 0.0085, -0.0229, 0.0458, 0.0181, -0.0407, -0.0053, -0.0006, 0.0171, 0.0077, 0.0366, -0.0239, 0.0117, -0.0082, 0.0348, -0.0213, 0.0357, 0.0353, 0.0931])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ggplot2_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.9354612824161332"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "may_vec.squared_distance(improve_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vdistance = vectors.map(lambda x: (x[0], x[1].squared_distance(ggplot2_vec))).takeOrdered(30, key = lambda x: x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'ggplot2', 0.0),\n",
       " (u'bugs', 0.066396579322429816),\n",
       " (u'book:', 0.070696099038579027),\n",
       " (u'glmnet.', 0.071530631770533509),\n",
       " (u'problems?', 0.072841157234798115),\n",
       " (u'e1071', 0.075424486210312738),\n",
       " (u'2008.', 0.07713752187268591),\n",
       " (u'chains.', 0.078195478180128453),\n",
       " (u'user,', 0.079667594130792468),\n",
       " (u'eviews', 0.079846731371020233),\n",
       " (u'neighbourhood', 0.08225580636734775),\n",
       " (u'amos', 0.082727990685793285),\n",
       " (u'forward,', 0.08276491457711202),\n",
       " (u'conjoint', 0.083045812591298881),\n",
       " (u'segmented', 0.08351286440597834),\n",
       " (u'ec2', 0.083881493650187905),\n",
       " (u'glm.', 0.08441809028825642),\n",
       " (u'outputs.', 0.084738573031798853),\n",
       " (u'vegan', 0.08578319719711186),\n",
       " (u'completely.', 0.085792051990780896),\n",
       " (u\"matlab's\", 0.086514112633934451),\n",
       " (u'shop', 0.08744532005968185),\n",
       " (u'pandas', 0.087480016863185781),\n",
       " (u'adaptive', 0.087539220297992021),\n",
       " (u'think)', 0.08778591164165693),\n",
       " (u'recognition.', 0.088121651940025941),\n",
       " (u'tools.', 0.088525715842649927),\n",
       " (u'fingerprint', 0.088551822824382731),\n",
       " (u'programming.', 0.089116753986208816),\n",
       " (u'engine', 0.089117214178534326)]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vdistance)\n",
    "vdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91631"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.count() #15916 #91631"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'specifically,',\n",
       "  DenseVector([-0.1944, 0.1558, 0.1921, -0.1491, -0.0861, -0.1852, 0.1091, -0.1292, -0.1604, -0.1161, -0.0072, -0.146, 0.0499, 0.1638, -0.3187, -0.0836, 0.1058, -0.4993, 0.2421, 0.2831, 0.1068, -0.3036, 0.0755, -0.3846, 0.1152, 0.1591, 0.0119, 0.5944, -0.1889, 0.0005, -0.0798, -0.1974, 0.1481, -0.1801, 0.2797, 0.0226, 0.1276, -0.0747, 0.1797, 0.1258, -0.1645, 0.0968, 0.0845, -0.0156, -0.1364, -0.141, 0.0199, -0.0999, -0.0983, -0.2004, -0.0337, -0.045, 0.102, 0.0655, 0.0678, -0.2162, -0.0252, -0.0445, 0.3757, -0.1643, 0.0589, -0.051, -0.2009, -0.0299, -0.2284, 0.2264, 0.1303, -0.1859, -0.1966, 0.1388, -0.0503, 0.1525, -0.0375, -0.159, 0.0833, -0.0212, -0.0189, -0.0993, 0.047, 0.0892, 0.4079, -0.0427, 0.132, 0.2488, 0.0589, -0.0339, -0.0517, 0.1564, 0.0565, 0.0278, -0.2768, 0.1926, 0.0227, -0.035, 0.3689, -0.0357, -0.0251, -0.1752, -0.0101, -0.0247])),\n",
       " (u'games).',\n",
       "  DenseVector([-0.0445, -0.0046, 0.012, -0.0142, 0.0248, -0.0983, -0.0069, 0.098, 0.0394, 0.0401, -0.0019, -0.0556, -0.0138, -0.0407, -0.0368, 0.0594, -0.0576, 0.0451, 0.0198, -0.031, -0.0065, 0.0795, 0.0175, -0.0122, 0.0638, 0.0721, 0.0161, 0.0348, -0.0803, -0.0046, 0.0076, -0.0322, 0.0282, -0.1089, -0.0134, -0.0065, -0.0033, -0.0461, -0.0754, 0.0068, 0.0156, -0.0108, 0.0751, 0.0118, -0.0439, -0.0299, 0.0456, -0.0065, 0.0247, -0.0862, -0.0161, 0.0585, -0.0523, 0.0338, 0.0159, -0.0139, -0.0022, -0.0191, 0.0308, -0.03, 0.0316, -0.0571, 0.0271, -0.026, 0.0286, 0.0532, 0.0534, -0.0415, -0.0107, -0.096, -0.0504, 0.0558, -0.0138, 0.0144, 0.018, 0.0581, 0.0015, 0.0384, -0.0073, 0.0002, -0.0128, -0.0025, -0.0182, -0.0043, 0.0421, -0.0905, -0.0025, 0.014, -0.035, 0.0696, -0.0214, 0.0569, 0.0389, -0.0145, -0.0219, -0.0458, 0.0261, 0.0201, 0.0419, -0.0607])),\n",
       " (u\"Jeffreys'\",\n",
       "  DenseVector([0.0915, 0.0073, -0.0963, 0.1147, -0.0278, -0.1234, -0.2013, 0.1476, 0.0011, 0.1784, 0.0134, 0.0736, 0.0798, 0.0213, -0.0765, 0.0301, -0.0385, -0.0138, -0.0589, 0.1604, -0.0861, 0.1588, -0.0055, -0.2195, 0.0617, 0.1054, 0.0334, 0.0031, 0.0758, 0.0187, 0.0564, -0.0266, -0.0651, -0.1203, 0.0552, 0.1755, 0.0433, -0.0656, 0.1441, 0.0467, 0.0769, -0.0342, 0.0194, -0.0445, -0.1645, -0.0496, -0.0023, 0.0998, -0.2513, -0.2456, -0.0057, 0.059, -0.1711, -0.0047, -0.0286, 0.0138, -0.0319, 0.1221, 0.1651, -0.0781, 0.0123, -0.0212, 0.0161, -0.102, -0.0226, 0.0153, 0.06, 0.0252, 0.0764, 0.2373, -0.1132, 0.0644, -0.0614, -0.116, 0.0735, -0.025, -0.1289, 0.0481, 0.0877, 0.0232, 0.1182, -0.0268, 0.0158, 0.1027, -0.0241, -0.0382, -0.0207, -0.0888, -0.1069, -0.1018, -0.1374, 0.1245, 0.0007, -0.0241, 0.0504, -0.2613, -0.1411, 0.1141, 0.0014, 0.0928])),\n",
       " (u'$p$-value.',\n",
       "  DenseVector([-0.1073, -0.0531, -0.1322, -0.2179, 0.0179, -0.0391, -0.1557, 0.1697, -0.0344, 0.0498, -0.0537, -0.1186, 0.1255, 0.0555, 0.0264, 0.0494, -0.0204, 0.0311, -0.0373, 0.1307, -0.0486, 0.0377, 0.0225, -0.178, -0.1186, 0.2206, -0.0188, 0.0738, -0.0874, -0.1284, -0.1277, -0.1127, -0.1457, -0.1357, -0.1619, -0.1445, 0.0219, 0.0187, 0.0838, -0.0409, 0.0385, 0.0016, -0.03, -0.0681, -0.167, -0.057, -0.0614, -0.1127, -0.1572, -0.1891, 0.0996, -0.1269, -0.1186, -0.0681, 0.1565, -0.1401, -0.0377, 0.0812, 0.1562, -0.0964, -0.0092, 0.0657, -0.02, 0.0634, -0.1917, 0.1548, -0.0257, -0.0544, 0.0219, -0.1003, -0.2222, 0.0671, -0.2834, -0.1022, 0.0985, 0.1025, -0.0188, 0.2465, -0.017, -0.315, -0.0205, -0.1362, -0.0677, 0.1553, -0.0113, -0.0969, 0.0354, -0.1758, -0.2448, 0.0296, -0.0533, 0.0716, -0.199, -0.007, 0.0909, -0.0428, -0.0776, 0.099, -0.1578, -0.0263]))]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "te = '<p>From $$\\\\mathbb{E}[s^{X_1}]=(sp+q)^K$$\\n(where $q=1-p$), it is rather straightforward to show that\\n$$\\\\mathbb{E}[s^{X_1+\\\\ldots+X_\\\\ell}]=\\\\left\\\\{s(1-q^\\\\ell)+q^\\\\ell\\\\right\\\\}^K$$ Indeed, if we assume it holds for a given $\\\\ell$ (and it does for $\\\\ell=1$), then \\n\\\\begin{align*}\\\\mathbb{E}[s^{X_1+\\\\ldots+X_{\\\\ell+1}}]&amp;=\\\\mathbb{E}[\\\\mathbb{E}[s^{X_1+\\\\ldots+X_{\\\\ell+1}}|X_1+\\\\ldots+X_\\\\ell]]\\\\\\\\ &amp;=\\\\mathbb{E}[s^{X_1+\\\\ldots+X_{\\\\ell}}(sp+q)^{K-X_1-\\\\ldots-X_\\\\ell}]\\\\\\\\\\n&amp;=(sp+q)^K \\\\left\\\\{ \\\\frac{s}{sp+q}\\\\,(1-q^\\\\ell)+q^\\\\ell\\\\right\\\\}^K\\\\\\\\\\n&amp;=\\\\left\\\\{s(1-q^{\\\\ell-1})+q^{\\\\ell+1}\\\\right\\\\}^K\\n\\\\end{align*}\\nFrom there, it follows that, for a given $\\\\ell$, $X_1+\\\\ldots+X_\\\\ell$ is distributed as a Binomial $\\\\text{B}(K,1-q^\\\\ell)$ random variable. Hence,\\n\\\\begin{align*}\\\\mathbb{P}(L=\\\\ell)&amp;=\\\\mathbb{P}(X_1+\\\\ldots+X_{\\\\ell-1}&lt;K=X_1+\\\\ldots+X_{\\\\ell})\\\\\\\\ &amp;=\\\\mathbb{E}[\\\\mathbb{P}(K=X_1+\\\\ldots+X_{\\\\ell}|X_1+\\\\ldots+X_{\\\\ell-1})\\\\mathbb{I}_{X_1+\\\\ldots+X_{\\\\ell-1}&lt;K}]\\\\\\\\&amp;=\\\\mathbb{E}[p^{K-X_1-\\\\ldots-X_{\\\\ell-1}}\\\\,\\\\mathbb{I}_{X_1+\\\\ldots+X_{\\\\ell-1}&lt;K}]\\\\\\\\&amp;=\\\\sum_{i=1}^{K-1} {K \\\\choose i} (1-q^{\\\\ell-1})^i (q^{\\\\ell-1})^{K-i} p^{K-i}\\\\\\\\\\n&amp;=\\\\sum_{i=1}^{K-1} {K \\\\choose i} (1-q^{\\\\ell-1})^i \\\\left[q^{\\\\ell-1} p\\\\right]^{K-i}\\\\\\\\\\n&amp;=\\\\left[1-q^{\\\\ell-1}+q^{\\\\ell-1} p\\\\right]^K-(1-q^{\\\\ell-1})^K\\n\\\\end{align*}\\nThis gives you the distribution of $L$.</p>\\n\\n<p>As a checkup, you can run the following code</p>\\n\\n<pre><code>T=10^6\\nN=13\\np=.85\\nell=rep(1,T)\\nfor (t in 1:T){\\n  x=rbinom(1,N,p)\\n  while (x&lt;N){ ell[t]=ell[t]+1; x=x+rbinom(1,N-x,p)}}\\n</code></pre>\\n\\n<p>and compare the frequencies with</p>\\n\\n<pre><code>probel=function(N,p,el){\\n (1-(1-p)^(el-1)+p*(1-p)^(el-1))^N-(1-(1-p)^(el-1))^N}\\n</code></pre>\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<p>From $$\\\\mathbb{E}[s^{X_1}]=(sp+q)^K$$\\n(where $q=1-p$), it is rather straightforward to show that\\n$$\\\\mathbb{E}[s^{X_1+\\\\ldots+X_\\\\ell}]=\\\\left\\\\{s(1-q^\\\\ell)+q^\\\\ell\\\\right\\\\}^K$$ Indeed, if we assume it holds for a given $\\\\ell$ (and it does for $\\\\ell=1$), then',\n",
       " '\\\\begin{align*}\\\\mathbb{E}[s^{X_1+\\\\ldots+X_{\\\\ell+1}}]&amp;=\\\\mathbb{E}[\\\\mathbb{E}[s^{X_1+\\\\ldots+X_{\\\\ell+1}}|X_1+\\\\ldots+X_\\\\ell]]\\\\\\\\ &amp;=\\\\mathbb{E}[s^{X_1+\\\\ldots+X_{\\\\ell}}(sp+q)^{K-X_1-\\\\ldots-X_\\\\ell}]\\\\\\\\\\n&amp;=(sp+q)^K \\\\left\\\\{ \\\\frac{s}{sp+q}\\\\,(1-q^\\\\ell)+q^\\\\ell\\\\right\\\\}^K\\\\\\\\\\n&amp;=\\\\left\\\\{s(1-q^{\\\\ell-1})+q^{\\\\ell+1}\\\\right\\\\}^K\\n\\\\end{align*}\\nFrom there, it follows that, for a given $\\\\ell$, $X_1+\\\\ldots+X_\\\\ell$ is distributed as a Binomial $\\\\text{B}(K,1-q^\\\\ell)$ random variable. Hence,\\n\\\\begin{align*}\\\\mathbb{P}(L=\\\\ell)&amp;=\\\\mathbb{P}(X_1+\\\\ldots+X_{\\\\ell-1}&lt;K=X_1+\\\\ldots+X_{\\\\ell})\\\\\\\\ &amp;=\\\\mathbb{E}[\\\\mathbb{P}(K=X_1+\\\\ldots+X_{\\\\ell}|X_1+\\\\ldots+X_{\\\\ell-1})\\\\mathbb{I}_{X_1+\\\\ldots+X_{\\\\ell-1}&lt;K}]\\\\\\\\&amp;=\\\\mathbb{E}[p^{K-X_1-\\\\ldots-X_{\\\\ell-1}}\\\\,\\\\mathbb{I}_{X_1+\\\\ldots+X_{\\\\ell-1}&lt;K}]\\\\\\\\&amp;=\\\\sum_{i=1}^{K-1} {K \\\\choose i} (1-q^{\\\\ell-1})^i (q^{\\\\ell-1})^{K-i} p^{K-i}\\\\\\\\\\n&amp;=\\\\sum_{i=1}^{K-1} {K \\\\choose i} (1-q^{\\\\ell-1})^i \\\\left[q^{\\\\ell-1} p\\\\right]^{K-i}\\\\\\\\\\n&amp;=\\\\left[1-q^{\\\\ell-1}+q^{\\\\ell-1} p\\\\right]^K-(1-q^{\\\\ell-1})^K\\n\\\\end{align*}\\nThis gives you the distribution of $L$.</p>\\n\\n<p>As a checkup, you can run the following code</p>\\n\\n<pre><code>T=10^6\\nN=13\\np=.85\\nell=rep(1,T)\\nfor (t in 1:T){\\n  x=rbinom(1,N,p)\\n  while (x&lt;N){ ell[t]=ell[t]+1; x=x+rbinom(1,N-x,p)}}\\n</code></pre>\\n\\n<p>and compare the frequencies with</p>\\n\\n<pre><code>probel=function(N,p,el){\\n (1-(1-p)^(el-1)+p*(1-p)^(el-1))^N-(1-(1-p)^(el-1))^N}\\n</code></pre>\\n']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "XMLSyntaxError",
     "evalue": "Extra content at the end of the document, line 15, column 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<string>\"\u001b[1;36m, line \u001b[1;32munknown\u001b[0m\n\u001b[1;31mXMLSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Extra content at the end of the document, line 15, column 1\n"
     ]
    }
   ],
   "source": [
    " #       Body0                = rows.get('Body')\n",
    "Body1 = ET.parse(te)\n",
    "rows = ET.fromstring(te)\n",
    "#Body = ET.tostring(rows, encoding='utf8', method='text')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree = ET.fromstring('<p>I guess you can consider $\\\\beta_{1}x_{1}+ \\\\beta_{2}x_{1}^{2}$ as a quadratic effect. Depending on the sign of $\\\\beta_1$, for increasing values of $x_1$ the quadratic effect may increase or decrease. We can consider the effects of other covariates by holding the quadratic effect constant. </p>\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Type 'NoneType' cannot be serialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-d2426a26da05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mBody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mET\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mlxml.etree.pyx\u001b[0m in \u001b[0;36mlxml.etree.tostring (src/lxml/lxml.etree.c:71960)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Type 'NoneType' cannot be serialized."
     ]
    }
   ],
   "source": [
    "Body = ET.tostring(x, encoding='utf8', method='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot parse from 'lxml.etree._Element'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-c2cbebfa8744>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mBody1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mET\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mlxml.etree.pyx\u001b[0m in \u001b[0;36mlxml.etree.parse (src/lxml/lxml.etree.c:72517)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mparser.pxi\u001b[0m in \u001b[0;36mlxml.etree._parseDocument (src/lxml/lxml.etree.c:106226)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot parse from 'lxml.etree._Element'"
     ]
    }
   ],
   "source": [
    "Body1 = ET.parse(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tree.get('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 425, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "  File \"/opt/conda/lib/python2.7/socket.py\", line 228, in meth\n",
      "    return getattr(self._sock,name)(*args)\n",
      "error: [Errno 111] Connection refused\n"
     ]
    },
    {
     "ename": "Py4JNetworkError",
     "evalue": "An error occurred while trying to connect to the Java server",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-f154e069615b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/opt/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mstop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    337\u001b[0m         \"\"\"\n\u001b[0;32m    338\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_jsc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_accumulatorServer\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mEND_COMMAND_PART\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 536\u001b[1;33m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[0;32m    538\u001b[0m                 self.target_id, self.name)\n",
      "\u001b[1;32m/opt/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry)\u001b[0m\n\u001b[0;32m    360\u001b[0m          \u001b[0mthe\u001b[0m \u001b[0mPy4J\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m         \"\"\"\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m             \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m             \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_create_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    323\u001b[0m         connection = GatewayConnection(self.address, self.port,\n\u001b[0;32m    324\u001b[0m                 self.auto_close, self.gateway_property)\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[1;34m'server'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JNetworkError\u001b[0m: An error occurred while trying to connect to the Java server"
     ]
    }
   ],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print 'a'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## classification\n",
    "We'd like to see if we can predict the tags of a question from its body text.\n",
    "Technically this is a multi-label classification problem, but to simplify\n",
    "things we'll use a one-vs-all approach where we choose the top k most\n",
    "common tags and train k binary classifiers where the labels indicate the\n",
    "presence or absence of that tag.\n",
    "\n",
    "Use a logistic regression model as your classifer.\n",
    "\n",
    "Since we can't reliably save and load models, return a list of 100 tuples\n",
    "(\"string\", [number, number, number,...])\n",
    "where \"string\" is the tag and the numbers are your model's predicted\n",
    "probabilities for class 0 (eg. 0.2 means a prediction that the tag is present)\n",
    "across the test set.\n",
    "\n",
    "* Note that this will require some digging into the DataFrame to extract.\n",
    "* The length of these probability lists is equal to len(X_test): 5121.\n",
    "* You should pickle the list of tuples and load it in __init__.py (which\n",
    "means committing the pickled file as well).\n",
    "\n",
    "### Parameters\n",
    "* Use the Cross Validated subset (stats.stackexchange.com) as your source data.\n",
    "* Tokenize the body text into words\n",
    "* number of tags to consider = 100\n",
    "* Sort the data by post_id ascending prior to train/test split\n",
    "* Use `X_train, X_test = X.randomSplit([0.9, 0.1], 42)` to split the dataset\n",
    "where X is a dataFrame containing the vectorized features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return usersRecord(AboutMe, AccountId, CreationDate, DisplayName, DownVotes, Id, LastAccessDate, \n",
    "#                    Location, Reputation, UpVotes, Views, WebsiteUrl) \n",
    "#return postsRecord(AcceptedAnswerId, AnswerCount, Body, CommentCount, CreationDate, FavoriteCount, Id, LastActivityDate, \\\n",
    "#                    LastEditDate, LastEditorUserId, OwnerUserId, PostTypeId, Score, Tags, Title, ViewCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row_rdd = sc.textFile(localpath(\"small_data/allPosts\")).map(postsParse)\n",
    "\n",
    "tags1 = row_rdd.filter(lambda x: x is not None and x.Tags is not None)\\\n",
    "               .map(lambda x: (x.Tags))\\\n",
    "\n",
    "tags = row_rdd.filter(lambda x: x is not None and x.Tags is not None)\\\n",
    "                .map(lambda x: (x.Tags))\\\n",
    "                .flatMap(lambda line: (re.split('<|>', line)))\\\n",
    "                .filter(lambda x: x != '')\\\n",
    "                .map(lambda x: (x,1))\\\n",
    "                .reduceByKey(lambda a, b: a + b)\n",
    "            \n",
    "tags2 = row_rdd.filter(lambda x: x is not None and x.Tags is not None)\\\n",
    "               .map(lambda x: (x.Tags))\\\n",
    "               .map(lambda line: (line.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('regression', 1006), ('r', 727), ('machine-learning', 551), ('time-series', 420), ('hypothesis-testing', 334), ('distributions', 329), ('probability', 311), ('correlation', 299), ('logistic', 282), ('classification', 250), ('statistical-significance', 225), ('self-study', 225), ('anova', 206), ('mathematical-statistics', 201), ('normal-distribution', 194), ('clustering', 193), ('bayesian', 192), ('spss', 175), ('multiple-regression', 164), ('svm', 153), ('estimation', 147), ('t-test', 142), ('data-mining', 137), ('neural-networks', 130), ('categorical-data', 128), ('confidence-interval', 120), ('generalized-linear-model', 118), ('references', 118), ('variance', 117), ('forecasting', 115), ('multivariate-analysis', 113), ('pca', 113), ('sampling', 110), ('data-visualization', 109), ('nonparametric', 102), ('dataset', 101), ('econometrics', 97), ('mixed-model', 97), ('data-transformation', 96), ('modeling', 94), ('repeated-measures', 93), ('factor-analysis', 92), ('cross-validation', 92), ('maximum-likelihood', 89), ('p-value', 88), ('feature-selection', 86), ('predictive-models', 83), ('chi-squared', 81), ('standard-deviation', 80), ('survival', 79), ('random-forest', 79), ('mean', 78), ('sample-size', 77), ('stata', 77), ('matlab', 73), ('interaction', 73), ('ordinal', 71), ('random-variable', 68), ('panel-data', 68), ('linear-model', 66), ('standard-error', 64), ('model-selection', 63), ('poisson', 63), ('outliers', 62), ('likert', 60), ('multiple-comparisons', 58), ('arima', 57), ('terminology', 57), ('survey', 57), ('heteroscedasticity', 56), ('least-squares', 55), ('binomial', 55), ('algorithms', 54), ('optimization', 52), ('kernel', 52), ('cart', 51), ('conditional-probability', 51), ('goodness-of-fit', 51), ('sas', 51), ('experiment-design', 51), ('sample', 50), ('text-mining', 50), ('lasso', 50), ('interpretation', 49), ('residuals', 49), ('missing-data', 48), ('assumptions', 47), ('bootstrap', 47), ('stochastic-processes', 46), ('model', 46), ('books', 46), ('normalization', 46), ('covariance', 45), ('prediction', 45), ('power-analysis', 44), ('meta-analysis', 44), ('normality', 43), ('python', 42), ('autocorrelation', 42), ('error', 41)]\n"
     ]
    }
   ],
   "source": [
    "#print tags1.take(10)\n",
    "toptags = tags.takeOrdered(100, key = lambda x: -x[1])\n",
    "print toptags\n",
    "#print tags2.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('regression', [0.5]), ('r', [0.5]), ('machine-learning', [0.5]), ('time-series', [0.5]), ('hypothesis-testing', [0.5]), ('distributions', [0.5]), ('probability', [0.5]), ('correlation', [0.5]), ('logistic', [0.5]), ('classification', [0.5]), ('statistical-significance', [0.5]), ('self-study', [0.5]), ('anova', [0.5]), ('mathematical-statistics', [0.5]), ('normal-distribution', [0.5]), ('clustering', [0.5]), ('bayesian', [0.5]), ('spss', [0.5]), ('multiple-regression', [0.5]), ('svm', [0.5]), ('estimation', [0.5]), ('t-test', [0.5]), ('data-mining', [0.5]), ('neural-networks', [0.5]), ('categorical-data', [0.5]), ('confidence-interval', [0.5]), ('generalized-linear-model', [0.5]), ('references', [0.5]), ('variance', [0.5]), ('forecasting', [0.5]), ('multivariate-analysis', [0.5]), ('pca', [0.5]), ('sampling', [0.5]), ('data-visualization', [0.5]), ('nonparametric', [0.5]), ('dataset', [0.5]), ('econometrics', [0.5]), ('mixed-model', [0.5]), ('data-transformation', [0.5]), ('modeling', [0.5]), ('repeated-measures', [0.5]), ('factor-analysis', [0.5]), ('cross-validation', [0.5]), ('maximum-likelihood', [0.5]), ('p-value', [0.5]), ('feature-selection', [0.5]), ('predictive-models', [0.5]), ('chi-squared', [0.5]), ('standard-deviation', [0.5]), ('survival', [0.5]), ('random-forest', [0.5]), ('mean', [0.5]), ('sample-size', [0.5]), ('stata', [0.5]), ('matlab', [0.5]), ('interaction', [0.5]), ('ordinal', [0.5]), ('random-variable', [0.5]), ('panel-data', [0.5]), ('linear-model', [0.5]), ('standard-error', [0.5]), ('model-selection', [0.5]), ('poisson', [0.5]), ('outliers', [0.5]), ('likert', [0.5]), ('multiple-comparisons', [0.5]), ('arima', [0.5]), ('terminology', [0.5]), ('survey', [0.5]), ('heteroscedasticity', [0.5]), ('least-squares', [0.5]), ('binomial', [0.5]), ('algorithms', [0.5]), ('optimization', [0.5]), ('kernel', [0.5]), ('cart', [0.5]), ('conditional-probability', [0.5]), ('goodness-of-fit', [0.5]), ('sas', [0.5]), ('experiment-design', [0.5]), ('sample', [0.5]), ('text-mining', [0.5]), ('lasso', [0.5]), ('interpretation', [0.5]), ('residuals', [0.5]), ('missing-data', [0.5]), ('assumptions', [0.5]), ('bootstrap', [0.5]), ('stochastic-processes', [0.5]), ('model', [0.5]), ('books', [0.5]), ('normalization', [0.5]), ('covariance', [0.5]), ('prediction', [0.5]), ('power-analysis', [0.5]), ('meta-analysis', [0.5]), ('normality', [0.5]), ('python', [0.5]), ('autocorrelation', [0.5]), ('error', [0.5])]\n"
     ]
    }
   ],
   "source": [
    "toptags1 = [(x[0], [0.5]) for x in toptags]\n",
    "print toptags1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['regression', 'r', 'machine-learning', 'time-series', 'hypothesis-testing', 'distributions', 'probability', 'correlation', 'logistic', 'classification', 'statistical-significance', 'self-study', 'anova', 'mathematical-statistics', 'normal-distribution', 'clustering', 'bayesian', 'spss', 'multiple-regression', 'svm', 'estimation', 't-test', 'data-mining', 'neural-networks', 'categorical-data', 'confidence-interval', 'generalized-linear-model', 'references', 'variance', 'forecasting', 'multivariate-analysis', 'pca', 'sampling', 'data-visualization', 'nonparametric', 'dataset', 'econometrics', 'mixed-model', 'data-transformation', 'modeling', 'repeated-measures', 'factor-analysis', 'cross-validation', 'maximum-likelihood', 'p-value', 'feature-selection', 'predictive-models', 'chi-squared', 'standard-deviation', 'survival', 'random-forest', 'mean', 'sample-size', 'stata', 'matlab', 'interaction', 'ordinal', 'random-variable', 'panel-data', 'linear-model', 'standard-error', 'model-selection', 'poisson', 'outliers', 'likert', 'multiple-comparisons', 'arima', 'terminology', 'survey', 'heteroscedasticity', 'least-squares', 'binomial', 'algorithms', 'optimization', 'kernel', 'cart', 'conditional-probability', 'goodness-of-fit', 'sas', 'experiment-design', 'sample', 'text-mining', 'lasso', 'interpretation', 'residuals', 'missing-data', 'assumptions', 'bootstrap', 'stochastic-processes', 'model', 'books', 'normalization', 'covariance', 'prediction', 'power-analysis', 'meta-analysis', 'normality', 'python', 'autocorrelation', 'error']\n"
     ]
    }
   ],
   "source": [
    "toptags1 = [x[0] for x in toptags]\n",
    "print toptags1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tags = row_rdd.filter(lambda x: x is not None and x.Tags is not None and x.Body is not None and x.Id is not None)\\\n",
    "                .map(lambda x: (x.Id, x.Body, (re.split('<|>', x.Tags))))\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('22094',\n",
       "  \"I'm looking for a good source of information about Analysis of Molecular Variance (AMOVA), a kind of statistical analysis used in population genetics. Especially how are the $\\\\phi$-statistics calculated and what is the difference compare to F-statistics (fixation index) in measuring the level of differentiation among diffrent population divisions. \",\n",
       "  ['', 'references', '', 'genetics', '', 'biostatistics', '']),\n",
       " ('22098',\n",
       "  'I applied SVM to perform the classification against several data sets. It turns out that the performance metric of recall is pretty bad for one data set. It has recall around 50% while other data sets have recall around 80%. For this kind of scenario, what are the possible approaches that are available to improve the recall? Besides, why some data sets can have a poor performance in terms of recall? How to analyze this kind of problem?',\n",
       "  ['',\n",
       "   'machine-learning',\n",
       "   '',\n",
       "   'classification',\n",
       "   '',\n",
       "   'multivariate-analysis',\n",
       "   '',\n",
       "   'data-mining',\n",
       "   '',\n",
       "   'svm',\n",
       "   '']),\n",
       " ('22135',\n",
       "  'I have a data set from a survey of health problems and annoyance by traffic noise in people living in residential buildings, with \"5-point scale answer\" questions. I want to perform a multilevel model for an ordinal response (health problem or noise annoyance), and I tried with the gllamm command in Stata but it\\'s not allowed the option link(ologit). Does anyone know another way to perform it? If not in Stata, in R or SPSS would be useful anyway.',\n",
       "  ['', 'r', '', 'mixed-model', '', 'spss', '', 'stata', '', 'gllamm', '']),\n",
       " ('22191',\n",
       "  'I am working for two signals. One is a dataset with 10 equally spaced impulses. Another is a dataset with randomly spaced impulses of identical length. The higher order moments, such as kurtosis, skewness or standard deviation are not sensitive to the two signals. Who can help look for an indicator to identify it?',\n",
       "  ['', 'signal-detection', '']),\n",
       " ('22196',\n",
       "  \"\\nIsn't the test statistic for both tests identical? The only difference I see is that the alternative hypothesis is different. Since you don't need to know the function h in the Breusch-Pagan test, what exactly is the difference between the two tests? \\nWhen would you use the White test, and when would you use the Breusch-Pagan test instead? What's the most important criteria to consider here when deciding which test to use?\\n\",\n",
       "  ['', 'heteroscedasticity', '']),\n",
       " ('22212',\n",
       "  'I would like to ask a question about Gower similarity/dissimilarity index.\\nIs it ok to use the Gower dissimilarity measure with Ward linkage clustering?\\nI was reading that the Gower similarity index should not be used with Ward linkage because the index is not metric. \\nI was wondering if this is only the case for the similarity and not for the dissimilarity index that can also handle odrinal variables?!',\n",
       "  ['',\n",
       "   'clustering',\n",
       "   '',\n",
       "   'distance',\n",
       "   '',\n",
       "   'metric',\n",
       "   '',\n",
       "   'ward',\n",
       "   '',\n",
       "   'euclidean',\n",
       "   '']),\n",
       " ('22216',\n",
       "  \"I am looking at attachment and cyber addiction. I'm carrying out an four-way Anova, attachment patterns being my ivs (secure, preoccupied, fearful and dismissing) with cyber addiction as my dependent variable. I am intending to use 100 participants in my study, I wanted to know if that is an appropraite amount to run a four-way anova?\",\n",
       "  ['', 'anova', '']),\n",
       " ('22233',\n",
       "  \"I am working with a data set having N around 200,000. In regressions, I am seeing very small significance values << 0.001 associated with very small effect sizes, e.g. r=0.028.  What I'd like to know is, is there a principled way of deciding an appropriate significance threshold in relation to the sample size? Are there any other important considerations about interpreting effect size with such a large sample?\",\n",
       "  ['',\n",
       "   'regression',\n",
       "   '',\n",
       "   'probability',\n",
       "   '',\n",
       "   'statistical-significance',\n",
       "   '',\n",
       "   'sample-size',\n",
       "   '']),\n",
       " ('22239',\n",
       "  \"I am looking for some medium to long length papers/websites/etc about data mining, specifically where one dataset is explored in depth from data preparation through final model.  I am particularly interested in discussions about application of machine learning algos and also basic data modeling.  An example would be Luis Torgo's book 'Data Mining with R'.  Any suggestions would be appreciated.\",\n",
       "  ['', 'r', '', 'data-mining', '']),\n",
       " ('22241',\n",
       "  'I have a question on the term used for the type of the item that I would like to use and how it is \"usually\" analysed.\\nLet\\'s assume that we have two items and the one is linked to the other, namely, the one is the answer in a multiple choice question and the second on is quantifies an attribute, for example, \"did you find the lesson interesting\" then a LIKERT scale is provided and then another item that asks whether finding the lesson interesting was important for the respondent (something like classification/priotirisation). So an item attaches somehow an additional quantity to the first one. These kinds of \"pair of items\" are used in customer satisfaction research usually...\\nDo you know how they are called?\\nHow are they analysed?\\nWould Kullback-Leibler divergence be needed?',\n",
       "  ['', 'scales', ''])]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s\n"
     ]
    }
   ],
   "source": [
    "print 's'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts = sc.textFile(localpath(\"small_data/allPosts/*\"))\\\n",
    "        .map(postsParse)\\\n",
    "        .filter(lambda x: x is not None)\\\n",
    "        .map(lambda x: (x.Tags.strip(), 0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hypothesis-testing statistical-significance markov-process', 0),\n",
       " ('probability normal-distribution games', 0),\n",
       " ('r cross-validation feature-selection random-forest stepwise-regression', 0),\n",
       " ('estimation markov-process inference', 0),\n",
       " ('regression t-test logistic', 0)]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toptag = posts.flatMap(lambda (line, x): line.split(\" \")).map(lambda x: (x,1)).reduceByKey(lambda a, b: a + b).takeOrdered(100, key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('r', [0.5])\n",
      "('regression', [0.5])\n",
      "('time-series', [0.5])\n",
      "('machine-learning', [0.5])\n",
      "('probability', [0.5])\n",
      "('hypothesis-testing', [0.5])\n",
      "('distributions', [0.5])\n",
      "('self-study', [0.5])\n",
      "('logistic', [0.5])\n",
      "('correlation', [0.5])\n",
      "('classification', [0.5])\n",
      "('statistical-significance', [0.5])\n",
      "('bayesian', [0.5])\n",
      "('anova', [0.5])\n",
      "('normal-distribution', [0.5])\n",
      "('clustering', [0.5])\n",
      "('mathematical-statistics', [0.5])\n",
      "('confidence-interval', [0.5])\n",
      "('data-visualization', [0.5])\n",
      "('multiple-regression', [0.5])\n",
      "('estimation', [0.5])\n",
      "('categorical-data', [0.5])\n",
      "('mixed-model', [0.5])\n",
      "('generalized-linear-model', [0.5])\n",
      "('spss', [0.5])\n",
      "('variance', [0.5])\n",
      "('repeated-measures', [0.5])\n",
      "('sampling', [0.5])\n",
      "('t-test', [0.5])\n",
      "('pca', [0.5])\n",
      "('svm', [0.5])\n",
      "('forecasting', [0.5])\n",
      "('multivariate-analysis', [0.5])\n",
      "('chi-squared', [0.5])\n",
      "('cross-validation', [0.5])\n",
      "('maximum-likelihood', [0.5])\n",
      "('data-mining', [0.5])\n",
      "('modeling', [0.5])\n",
      "('neural-networks', [0.5])\n",
      "('data-transformation', [0.5])\n",
      "('predictive-models', [0.5])\n",
      "('matlab', [0.5])\n",
      "('nonparametric', [0.5])\n",
      "('interaction', [0.5])\n",
      "('survival', [0.5])\n",
      "('model-selection', [0.5])\n",
      "('p-value', [0.5])\n",
      "('linear-model', [0.5])\n",
      "('dataset', [0.5])\n",
      "('binomial', [0.5])\n",
      "('poisson', [0.5])\n",
      "('econometrics', [0.5])\n",
      "('standard-deviation', [0.5])\n",
      "('stata', [0.5])\n",
      "('mean', [0.5])\n",
      "('bootstrap', [0.5])\n",
      "('feature-selection', [0.5])\n",
      "('references', [0.5])\n",
      "('sample-size', [0.5])\n",
      "('interpretation', [0.5])\n",
      "('multiple-comparisons', [0.5])\n",
      "('optimization', [0.5])\n",
      "('least-squares', [0.5])\n",
      "('python', [0.5])\n",
      "('conditional-probability', [0.5])\n",
      "('random-forest', [0.5])\n",
      "('experiment-design', [0.5])\n",
      "('arima', [0.5])\n",
      "('prediction', [0.5])\n",
      "('panel-data', [0.5])\n",
      "('standard-error', [0.5])\n",
      "('survey', [0.5])\n",
      "('factor-analysis', [0.5])\n",
      "('covariance', [0.5])\n",
      "('simulation', [0.5])\n",
      "('missing-data', [0.5])\n",
      "('sas', [0.5])\n",
      "('random-variable', [0.5])\n",
      "('outliers', [0.5])\n",
      "('autocorrelation', [0.5])\n",
      "('pdf', [0.5])\n",
      "('mcmc', [0.5])\n",
      "('ordinal', [0.5])\n",
      "('multilevel-analysis', [0.5])\n",
      "('inference', [0.5])\n",
      "('residuals', [0.5])\n",
      "('random-effects-model', [0.5])\n",
      "('goodness-of-fit', [0.5])\n",
      "('regression-coefficients', [0.5])\n",
      "('expected-value', [0.5])\n",
      "('algorithms', [0.5])\n",
      "('cart', [0.5])\n",
      "('lmer', [0.5])\n",
      "('stochastic-processes', [0.5])\n",
      "('model', [0.5])\n",
      "('proportion', [0.5])\n",
      "('heteroscedasticity', [0.5])\n",
      "('nonlinear-regression', [0.5])\n",
      "('error', [0.5])\n",
      "('multinomial', [0.5])\n"
     ]
    }
   ],
   "source": [
    "toptags2 = [(x[0], [0.5]) for x in toptag]\n",
    "for i in  toptags2:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.Word2Vec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
