{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests as re\n",
    "import pandas as pd \n",
    "pd.set_option('display.max_columns', 30)\n",
    "import sklearn as sk\n",
    "import json as json\n",
    "import urllib2\n",
    "from urllib import urlopen\n",
    "\n",
    "import gzip\n",
    "import ujson\n",
    "import wget\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "\n",
    "from numpy import random\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']\n",
    "\n",
    "from IPython import display\n",
    "from sklearn.externals.six import StringIO\n",
    "import pydot\n",
    "import numpy as np\n",
    "from matplotlib import pylab as plt\n",
    "from sklearn import tree\n",
    "from sklearn import datasets, linear_model, utils, preprocessing\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import nltk\n",
    "from sklearn import cross_validation\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#url = 'http://thedataincubator.s3.amazonaws.com/coursedata/mldata/yelp_train_academic_dataset_review.json.gz'\n",
    "#filename = wget.download(url)\n",
    "\n",
    "#print filename\n",
    "data = []\n",
    "with gzip.open('yelp_train_academic_dataset_review.json.gz', 'rb') as f:\n",
    "    data.extend([ujson.loads(line) for line in f.readlines()])\n",
    "    #file_content = f.read()\n",
    "\n",
    "#data = json.loads(file_content)\n",
    "    \n",
    "#import gzip\n",
    "#with gzip.open('yelp_train_academic_dataset_business.json.gz') as f:\n",
    "#    file_content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'votes': {u'funny': 0, u'useful': 2, u'cool': 1}, u'user_id': u'Xqd0DzHaiyRqVH3WRG7hzg', u'review_id': u'15SdjuK7DmYqUAj6rjGowg', u'text': u\"dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\", u'business_id': u'vcNAWiLM4dR7D2nwwJ7nCA', u'stars': 5, u'date': u'2007-05-17', u'type': u'review'}\n",
      "{u'votes': {u'funny': 0, u'useful': 2, u'cool': 0}, u'user_id': u'H1kH6QZV7Le4zqTRNxoZow', u'review_id': u'RF6UnRTtG7tWMcrO2GEoAg', u'text': u\"Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply never answers the phone.  It usually takes 2 hours of repeated calling to get an answer.  Who has time for that or wants to deal with it?  I have run into this problem with many other doctors and I just don't get it.  You have office workers, you have patients with medical needs, why isn't anyone answering the phone?  It's incomprehensible and not work the aggravation.  It's with regret that I feel that I have to give Dr. Goldberg 2 stars.\", u'business_id': u'vcNAWiLM4dR7D2nwwJ7nCA', u'stars': 2, u'date': u'2010-03-22', u'type': u'review'}\n"
     ]
    }
   ],
   "source": [
    "for i in data[:2]:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datax = [len(i) for i in all_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#print datax\n",
    "\n",
    "print min(s for s in datax)\n",
    "\n",
    "test1 = all_text.sort(key = lambda s: len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'A', u'.', u'.', u'-', u'.', u'.', u'1', u'H', u'C', u'?', u'I', u'A', u'.', u'.', u'.', u'.', u'D', u'.', u'.', u'.', u'c', u'I', u'X', u'.', u'.', u'\"', u'H', u'.', u'.', u'H', u'.', u'.', u'!', u'.', u'.', u'.', u'.', u'.', u'.', u':)', u'ew', u'A+', u'OK', u'OK', u':)', u'ok', u'NC', u'..', u'Nm', u'Ok', u'Eg', u':/', u'Ok', u'Ok', u'NT', u'Eh', u':)', u':)', u'ok', u':)', u'A+', u'..', u'A+', u'~~', u'Lo', u'..', u'Ok', u'Ok', u':)', u'No', u';)', u'..', u'Ok', u'Ok', u':(', u':/', u'Yum', u'eh.', u'...', u'Eh.', u'UPS', u'Yum', u'chi', u'Bad', u'Not', u'Go!', u'Yum', u'fun', u': /', u'OK!', u'...', u'Eh.', u'RIP', u'oks', u': (', u'Yup', u'Meh', u'RIP', u'The', u'Yup', u'eww', u'T\\xeeI', u'Yum', u'n/a', u'...', u'***', u'yum', u'Ugh', u'^^b', u'Yum', u'Fun', u'$$$', u'*-*', u'Yum', u'...', u'Sux', u'fun', u'mmm', u'Yum', u'yum', u'Yum', u'Meh', u': (', u'Fun', u'Yes', u'...', u'meh', u'Meh', u'Nom', u'Wow', u'No.', u'8.2', u'No.', u'Go.', u':-/', u'yum', u'bad', u'Yum', u'yum', u'Meh', u'Yumm', u'Yum!', u'Yum!', u'Yes.', u'Meh.', u'Yum!', u'nice', u'LUV.', u'OMG!', u'Good', u'Meh!', u'Fun!', u'Boop', u'Nice', u'good', u'Nice', u'Epic', u'Tits', u'Poop', u'Yum!', u'BOMB', u'WOW!', u'yum.', u'Yum!', u'2:57', u'nice', u'Like', u'Good', u'Yum!', u'Good', u'Work', u'Good', u'Nice', u'....', u'Wow.', u'Yum.', u'Jeah', u'A-OK', u'YUM!', u'Yum!', u'yum!', u'Meh.', u'Yes!', u'A-ok', u'Good', u'Nice', u'Wow!', u'Yum!', u'Meh.', u'Fair', u'Good', u'Edit', u'Meh.', u'Besy', u'Woo.', u'Okay', u'Good', u'Yay!', u'sigh', u'Fine']\n"
     ]
    }
   ],
   "source": [
    "print sorted(all_text, key=len)[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_stars = []\n",
    "for i in data:\n",
    "    all_stars.append(i['stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = [ [a['text']] for a in data]\n",
    "stars = [[a['stars']] for a in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#stars = [[a['stars']] for a in data]\n",
    "all_text = []\n",
    "for i in data:\n",
    "    all_text.append(i['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u\"dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\", u\"Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply never answers the phone.  It usually takes 2 hours of repeated calling to get an answer.  Who has time for that or wants to deal with it?  I have run into this problem with many other doctors and I just don't get it.  You have office workers, you have patients with medical needs, why isn't anyone answering the phone?  It's incomprehensible and not work the aggravation.  It's with regret that I feel that I have to give Dr. Goldberg 2 stars.\"]\n",
      "[[u\"dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\"], [u\"Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply never answers the phone.  It usually takes 2 hours of repeated calling to get an answer.  Who has time for that or wants to deal with it?  I have run into this problem with many other doctors and I just don't get it.  You have office workers, you have patients with medical needs, why isn't anyone answering the phone?  It's incomprehensible and not work the aggravation.  It's with regret that I feel that I have to give Dr. Goldberg 2 stars.\"]]\n"
     ]
    }
   ],
   "source": [
    "print all_text[:2]\n",
    "print text[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012913\n",
      "1012913\n",
      "1012913\n"
     ]
    }
   ],
   "source": [
    "print len(text)\n",
    "print len(stars)\n",
    "print len(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sent_list = [nltk.tokenize.sent_tokenize(i) for i in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012913\n",
      "[[u'dr. goldberg offers everything i look for in a general practitioner.', u\"he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.\", u'really, what more do you need?', u\"i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\"], [u\"Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.\", u'It seems that his staff simply never answers the phone.', u'It usually takes 2 hours of repeated calling to get an answer.', u'Who has time for that or wants to deal with it?', u\"I have run into this problem with many other doctors and I just don't get it.\", u\"You have office workers, you have patients with medical needs, why isn't anyone answering the phone?\", u\"It's incomprehensible and not work the aggravation.\", u\"It's with regret that I feel that I have to give Dr. Goldberg 2 stars.\"]]\n"
     ]
    }
   ],
   "source": [
    "#print len(sent_list)\n",
    "#print sent_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012913\n"
     ]
    }
   ],
   "source": [
    "print len(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1012913, 62)\n"
     ]
    }
   ],
   "source": [
    "## Rahul\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Using the Count Vectorizer\n",
    "bag_of_words_vectorizer = CountVectorizer(min_df=0.1,max_df=0.9,\n",
    "                                          stop_words=nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "fitVocabulary = bag_of_words_vectorizer.fit(all_text)\n",
    "fitVocabulary_transformed = fitVocabulary.transform(all_text)\n",
    "\n",
    "print fitVocabulary_transformed.shape\n",
    "\n",
    "# Apply fitVocabulary to the reviews\n",
    "\n",
    "#vectorizer = CountVectorizer(min_df=0.1,max_df=0.9, stop_words=nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "\n",
    "\n",
    "#vec = CountVectorizer(max_features=350,\n",
    "#                        stop_words=nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "#q1_fit2 = vec.fit(all_text)\n",
    "#q1_transformed = q1_fit2.transform(all_text)\n",
    "\n",
    "#print vec.get_feature_names()\n",
    "#counts = vec.fit_transform( all_text  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'also', u'always', u'around', u'back', u'best', u'better', u'came', u'come', u'could', u'day', u'definitely', u'delicious', u'didn', u'even', u'experience', u'first', u'food', u'friendly', u'get', u'go', u'going', u'good', u'got', u'great', u'know', u'like', u'little', u'love', u'made', u'make', u'menu', u'much', u'never', u'nice', u'night', u'one', u'order', u'ordered', u'people', u'place', u'pretty', u're', u'really', u'restaurant', u'right', u'say', u'see', u'service', u'staff', u'still', u'think', u'time', u'try', u'two', u'us', u've', u'vegas', u'want', u'way', u'well', u'went', u'would']\n"
     ]
    }
   ],
   "source": [
    "print fitVocabulary.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r =Ridge(alpha=1)#SGDRegressor(penalty=&#39;elasticnet&#39;,n_iter = 100, eta0=.005)\n",
    "# #m = LinearRegression()#SGDRegressor(n_iter = 100, eta0=.005)</span>\n",
    "t=time.clock()\n",
    "r.fit(x,y)#.sparsify()</span>\n",
    "\n",
    "print time.clock()-t\n",
    "t=time.clock()\n",
    "print r2_score(y,r.predict(x)), mean_squared_error(y,r.predict(x))\n",
    "\n",
    "x=open(yelp_train_academic_dataset_review.json,).</span><span class=\"n\">read</span><span class=\"p\">()</span>\n",
    "<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">re</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(r&#39;\\n(?={)&#39;,x)# re.split(r&#39;\\n(?={)&#39;, &quot;{Hello there}\\n{how are}\\n{you}&quot;)</span>\n",
    "<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">simplejson</span><span class=\"o\">.</span><span class=\"n\">loads</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"n\">x</span><span class=\"p\">]</span>\n",
    "<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"p\">[(</span><span class=\"n\">i</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s\">&#39;text&#39;</span><span class=\"p\">),</span> <span class=\"n\">i</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s\">&#39;stars&#39;</span><span class=\"p\">))</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"n\">x</span> <span class=\"k\">if</span> <span class=\"n\">i</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s\">&#39;text&#39;</span><span class=\"p\">,</span> <span class=\"bp\">None</span><span class=\"p\">)]</span> <span class=\"c\"># Some don&#39;t have reviews</span>\n",
    "<span class=\"n\">xraw</span><span class=\"p\">,</span> <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"p\">)</span>\n",
    "<span class=\"k\">print</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n",
    "<span class=\"n\">dill</span><span class=\"o\">.</span><span class=\"n\">dump</span><span class=\"p\">(</span> <span class=\"n\">xraw</span><span class=\"p\">,</span> <span class=\"nb\">open</span><span class=\"p\">(</span> <span class=\"s\">&quot;yelp_largexraw.p&quot;</span><span class=\"p\">,</span> <span class=\"s\">&quot;wb&quot;</span> <span class=\"p\">)</span> <span class=\"p\">)</span>\n",
    "<span class=\"n\">dill</span><span class=\"o\">.</span><span class=\"n\">dump</span><span class=\"p\">(</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"nb\">open</span><span class=\"p\">(</span> <span class=\"s\">&quot;yelp_largey.p&quot;</span><span class=\"p\">,</span> <span class=\"s\">&quot;wb&quot;</span> <span class=\"p\">)</span> <span class=\"p\">)</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33593468444230057"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r =linear_model.Ridge(alpha=1)\n",
    "r.fit(X_train, y_train)\n",
    "r.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21176610489989933"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitReviews = fitVocabulary.transform(reviews)\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\\\n",
    "                    fitVocabulary_transformed, all_stars, test_size=0.4) \n",
    "regr5 = linear_model.LinearRegression()\n",
    "regr5.fit(X_train, y_train)\n",
    "regr5.score(X_test, y_test)\n",
    "#dill.dump(regr, open('bag_Of_Words.dill', 'w'))\n",
    "#dill.dump(fitVocabulary, open('vectorizer.dill', 'w'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "dill_out = open(\"r_rego.dill\",\"wb\")\n",
    "dill.dump(regr5,dill_out)\n",
    "dill_out.close()\n",
    "\n",
    "dill_out = open(\"r_fito.dill\",\"wb\")\n",
    "dill.dump(fitVocabulary,dill_out)\n",
    "dill_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Q2 : Normalized Model\n",
    "# tfidf vectorization from scratch\n",
    "# This can also be done by applying TfidfTransformer to the Vectorized dataset from before\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=300, \n",
    "                                   ngram_range=(1,1), \n",
    "                                   stop_words=nltk.corpus.stopwords.words('english'),\n",
    "                                   max_df = 1.0, min_df= 20)\n",
    "fitVocabulary_tfidf = tfidf_vectorizer.fit(text for text in all_text)\n",
    "fitReviews_tfidf = fitVocabulary_tfidf.transform(all_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39571899977270297"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\\\n",
    "                    fitReviews_tfidf, stars, test_size=0.4, random_state=42) \n",
    "regr3 = linear_model.LinearRegression(fit_intercept=True)\n",
    "regr3.fit(X_train, y_train)\n",
    "regr3.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.6352104340917055"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr3.predict(X_test[0])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dill_out = open(\"r_reg2.dill\",\"wb\")\n",
    "dill.dump(regr3,dill_out)\n",
    "dill_out.close()\n",
    "\n",
    "dill_out = open(\"r_fit2.dill\",\"wb\")\n",
    "dill.dump(fitVocabulary_tfidf,dill_out)\n",
    "dill_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\\\n",
    "                    fitReviews_tfidf, stars, test_size=0.4, random_state=42) \n",
    "k_shell = ShellEstimator(TruncateTransformer(0), linear_model.LinearRegression(fit_intercept=True))\n",
    "k_shell.fit(X_train, y_train)\n",
    "regr_nm = linear_model.LinearRegression()\n",
    "regr_nm.fit(X_train, y_train)\n",
    "#dill.dump(regr_nm, open('norm_model.dill', 'w'))\n",
    "#dill.dump(fitVocabulary_tfidf, open('tfidf_vect.dill', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.601920466164092"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.predict(X_train)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'10', u'15', u'20', u'30', u'able', u'actually', u'almost', u'also', u'although', u'always', u'amazing', u'another', u'anything', u'area', u'around', u'ask', u'asked', u'atmosphere', u'away', u'awesome', u'back', u'bad', u'bar', u'beef', u'beer', u'best', u'better', u'big', u'bit', u'bread', u'breakfast', u'bring', u'brought', u'buffet', u'burger', u'business', u'busy', u'call', u'called', u'came', u'car', u'care', u'casino', u'cheap', u'check', u'cheese', u'chicken', u'chocolate', u'clean', u'close', u'club', u'coffee', u'come', u'comes', u'coming', u'cooked', u'cool', u'could', u'couldn', u'couple', u'course', u'cream', u'customer', u'day', u'deal', u'decent', u'decided', u'definitely', u'delicious', u'dessert', u'didn', u'different', u'dining', u'dinner', u'disappointed', u'dish', u'dishes', u'doesn', u'done', u'door', u'drink', u'drinks', u'eat', u'eating', u'either', u'else', u'end', u'enjoy', u'enjoyed', u'enough', u'especially', u'even', u'ever', u'every', u'everyone', u'everything', u'excellent', u'experience', u'extra', u'family', u'fan', u'fantastic', u'far', u'fast', u'favorite', u'feel', u'felt', u'finally', u'find', u'first', u'fish', u'flavor', u'food', u'found', u'free', u'fresh', u'fried', u'friend', u'friendly', u'friends', u'fries', u'front', u'full', u'fun', u'gave', u'get', u'getting', u'give', u'go', u'going', u'good', u'got', u'great', u'group', u'guy', u'half', u'happy', u'hard', u'high', u'highly', u'home', u'hot', u'hotel', u'hour', u'hours', u'house', u'however', u'huge', u'husband', u'ice', u'inside', u'instead', u'isn', u'items', u'keep', u'kids', u'kind', u'know', u'large', u'las', u'last', u'later', u'least', u'left', u'less', u'let', u'like', u'liked', u'line', u'little', u'll', u'location', u'long', u'look', u'looked', u'looking', u'lot', u'love', u'loved', u'lunch', u'made', u'make', u'makes', u'manager', u'many', u'may', u'maybe', u'meal', u'meat', u'menu', u'might', u'minutes', u'money', u'much', u'music', u'must', u'name', u'need', u'needed', u'never', u'new', u'next', u'nice', u'night', u'nothing', u'oh', u'ok', u'old', u'one', u'open', u'order', u'ordered', u'outside', u'overall', u'parking', u'part', u'party', u'pay', u'people', u'perfect', u'person', u'pizza', u'place', u'places', u'plate', u'plus', u'pool', u'pork', u'pretty', u'price', u'prices', u'probably', u'put', u'quality', u'quick', u'quite', u're', u'real', u'really', u'recommend', u'red', u'restaurant', u'restaurants', u'review', u'reviews', u'rice', u'right', u'room', u'rooms', u'said', u'salad', u'sandwich', u'sauce', u'saw', u'say', u'second', u'see', u'seemed', u'selection', u'served', u'server', u'service', u'several', u'shop', u'show', u'shrimp', u'side', u'since', u'sit', u'small', u'someone', u'something', u'soup', u'special', u'spicy', u'spot', u'staff', u'star', u'stars', u'started', u'stay', u'steak', u'still', u'stop', u'store', u'strip', u'stuff', u'style', u'super', u'sure', u'sushi', u'sweet', u'table', u'tables', u'take', u'taste', u'tasted', u'tasty', u'tell', u'thing', u'things', u'think', u'though', u'thought', u'three', u'time', u'times', u'told', u'took', u'top', u'town', u'tried', u'try', u'trying', u'two', u'us', u'use', u'used', u'usually', u've', u'vegas', u'visit', u'wait', u'waiter', u'waiting', u'waitress', u'walk', u'walked', u'want', u'wanted', u'wasn', u'water', u'way', u'week', u'well', u'went', u'whole', u'wife', u'wine', u'without', u'won', u'wonderful', u'work', u'worth', u'would', u'wrong', u'year', u'years', u'yelp', u'yes']\n",
      "(1012913, 350)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(max_features=350,\n",
    "                        stop_words=nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "q1_fit2 = vec.fit(all_text)\n",
    "q1_transformed = q1_fit2.transform(all_text)\n",
    "\n",
    "print vec.get_feature_names()\n",
    "#counts = vec.fit_transform( all_text  )\n",
    "print q1_transformed.shape\n",
    "#print counts\n",
    "#print vec.get_feature_names()\n",
    "#sent_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print data[0]['text']\n",
    "#print my_list_of_sentences\n",
    "#words = [ nltk.tokenize.word_tokenize(sent) for sent in my_list_of_sentences]\n",
    "#print words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3511656440311004"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\\\n",
    "                    q1_transformed, stars, test_size=0.4, random_state=42) \n",
    "q1_reg2 = linear_model.LinearRegression()\n",
    "q1_reg2.fit(X_train, y_train)\n",
    "q1_reg2.score(X_test, y_test)\n",
    "\n",
    "#300: 0.33756830581688479\n",
    "#350: 0.3511656440311004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.601920466164092"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr_q1.predict(X_train)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "dill_out = open(\"q1_reg.dill\",\"wb\")\n",
    "dill.dump(q1_reg2,dill_out)\n",
    "dill_out.close()\n",
    "\n",
    "dill_out = open(\"q1_fit.dill\",\"wb\")\n",
    "dill.dump(q1_fit2,dill_out)\n",
    "dill_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1012913, 18041)\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#vec = CountVectorizer(max_features=350,\n",
    "#                        stop_words=nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "#q1_fit2 = vec.fit(all_text)\n",
    "#q1_transformed = q1_fit2.transform(all_text)\n",
    "\n",
    "#print vec.get_feature_names()\n",
    "#counts = vec.fit_transform( all_text  )\n",
    "#print q1_transformed.shape\n",
    "#print counts\n",
    "#print vec.get_feature_names()\n",
    "#sent_list[0]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "ng_tfidf=TfidfVectorizer(min_df = 100,  \n",
    "                         stop_words=nltk.corpus.stopwords.words('english'))\n",
    "tf_fit=ng_tfidf.fit(all_text)\n",
    "tf_transformed = tf_fit.transform(all_text)\n",
    "\n",
    "#print ng_tfidf.get_feature_names()\n",
    "print tf_transformed.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39571899977270297"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\\\n",
    "                    tf_transformed, stars, test_size=0.4, random_state=42) \n",
    "tf_reg = linear_model.LinearRegression()\n",
    "tf_reg.fit(X_train, y_train)\n",
    "tf_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "dill_out = open(\"tf_reg2.dill\",\"wb\")\n",
    "dill.dump(tf_reg,dill_out)\n",
    "dill_out.close()\n",
    "\n",
    "dill_out = open(\"tf_fit2.dill\",\"wb\")\n",
    "dill.dump(tf_fit,dill_out)\n",
    "dill_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'10', u'20', u'actual', u'almost', u'also', u'alway', u'amaz', u'anoth', u'anyth', u'area', u'around', u'arriv', u'ask', u'atmospher', u'away', u'awesom', u'back', u'bad', u'bar', u'beef', u'beer', u'best', u'better', u'big', u'bit', u'bread', u'breakfast', u'bring', u'buffet', u'burger', u'busi', u'call', u'came', u'car', u'care', u'chang', u'check', u'chees', u'chicken', u'chocol', u'choic', u'clean', u'close', u'club', u'coffe', u'come', u'cook', u'cool', u'could', u'coupl', u'cours', u'cream', u'custom', u'day', u'deal', u'decent', u'decid', u'decor', u'definit', u'delici', u'dessert', u'didn', u'differ', u'dine', u'dinner', u'disappoint', u'dish', u'done', u'drink', u'eat', u'egg', u'els', u'end', u'enjoy', u'enough', u'even', u'ever', u'everi', u'everyon', u'everyth', u'excel', u'expect', u'experi', u'famili', u'far', u'favorit', u'feel', u'final', u'find', u'first', u'fish', u'flavor', u'food', u'found', u'free', u'fresh', u'fri', u'friend', u'front', u'full', u'fun', u'gave', u'get', u'girl', u'give', u'go', u'good', u'got', u'great', u'guy', u'half', u'hand', u'happi', u'hard', u'help', u'high', u'home', u'hot', u'hotel', u'hour', u'hous', u'howev', u'huge', u'husband', u'ice', u'impress', u'insid', u'item', u'keep', u'kid', u'kind', u'know', u'larg', u'las', u'last', u'least', u'leav', u'left', u'let', u'light', u'like', u'line', u'littl', u'live', u'll', u'locat', u'long', u'look', u'lot', u'love', u'lunch', u'made', u'make', u'manag', u'mani', u'mayb', u'meal', u'meat', u'menu', u'minut', u'money', u'much', u'music', u'must', u'name', u'need', u'never', u'new', u'next', u'nice', u'night', u'noth', u'offer', u'oh', u'ok', u'old', u'one', u'open', u'option', u'order', u'outsid', u'overal', u'owner', u'park', u'part', u'parti', u'pay', u'peopl', u'perfect', u'person', u'pick', u'pizza', u'place', u'plate', u'play', u'point', u'pool', u'pork', u'portion', u'potato', u'pretti', u'price', u'probabl', u'put', u'qualiti', u'quick', u'quit', u're', u'realli', u'reason', u'recommend', u'restaur', u'return', u'review', u'rice', u'right', u'roll', u'room', u'said', u'salad', u'sandwich', u'sauc', u'say', u'seat', u'see', u'seem', u'select', u'serv', u'server', u'servic', u'shop', u'show', u'shrimp', u'side', u'sinc', u'sit', u'size', u'small', u'someth', u'soup', u'special', u'spici', u'spot', u'staff', u'star', u'start', u'stay', u'steak', u'still', u'stop', u'store', u'strip', u'super', u'sure', u'surpris', u'sushi', u'sweet', u'tabl', u'taco', u'take', u'tast', u'tasti', u'tell', u'thank', u'thing', u'think', u'though', u'thought', u'three', u'time', u'told', u'took', u'top', u'town', u'tri', u'two', u'us', u'use', u'usual', u've', u'vega', u'visit', u'wait', u'waitress', u'walk', u'want', u'wasn', u'water', u'way', u'week', u'well', u'went', u'whole', u'wine', u'without', u'wonder', u'work', u'worth', u'would', u'year']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "default_tokenizer = TfidfVectorizer().build_tokenizer()\n",
    "stemmer = nltk.stem.SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "    \n",
    "def tokenize_stem(text):\n",
    "    \"\"\"\n",
    "    We will use the default tokenizer from TfidfVectorizer, combined with the nltk SnowballStemmer.\n",
    "    \"\"\"\n",
    "    tokens = default_tokenizer(text)\n",
    "    stemmed = map(stemmer.stem, tokens)\n",
    "    return stemmed\n",
    "\n",
    "ng_stem_tfidf = TfidfVectorizer(max_features=300, \n",
    "                         #ngram_range=(1,2), \n",
    "                         stop_words=map(stemmer.stem, nltk.corpus.stopwords.words('english')),\n",
    "                         tokenizer=tokenize_stem)\n",
    "#ng_stem_tfidf = ng_stem_tfidf.fit( all_text )\n",
    "tf_fit=ng_stem_tfidf.fit(all_text)\n",
    "tf_transformed = tf_fit.transform(all_text)\n",
    "\n",
    "ng_stem_vocab = ng_stem_tfidf.get_feature_names()\n",
    "print ng_stem_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ng_stem_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40976341434931973"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\\\n",
    "                    tf_transformed, stars, test_size=0.4, random_state=42) \n",
    "tf_reg = linear_model.LinearRegression()\n",
    "tf_reg.fit(X_train, y_train)\n",
    "tf_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "dill_out = open(\"tf_reg.dill\",\"wb\")\n",
    "dill.dump(tf_reg,dill_out)\n",
    "dill_out.close()\n",
    "\n",
    "dill_out = open(\"tf_fit.dill\",\"wb\")\n",
    "dill.dump(tf_fit,dill_out)\n",
    "dill_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1012913, 300)\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "#sklearn.feature_extraction.text.HashingVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "vec = HashingVectorizer(n_features=300,\n",
    "                        stop_words=nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "test_fit = vec.fit(all_text)\n",
    "test_transformed = test_fit.transform(all_text)\n",
    "\n",
    "#print vec.get_feature_names()\n",
    "#counts = vec.fit_transform( all_text  )\n",
    "print test_transformed.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test replicate\n",
    "#sklearn.feature_extraction.text.HashingVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "vec = HashingVectorizer(n_features=100,\n",
    "                        stop_words=nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "test_fit = vec.fit(all_text)\n",
    "test_transformed = test_fit.transform(all_text)\n",
    "\n",
    "#print vec.get_feature_names()\n",
    "#counts = vec.fit_transform( all_text  )\n",
    "print test_transformed.shape\n",
    "#fail = vec.transform(all_text[:2])\n",
    "\n",
    "#print np.where(fail.data == 0)\n",
    "#fail.eliminate_zeros()\n",
    "#test_tf_fit = TfidfTransformer(sublinear_tf=True).fit(fail)\n",
    "#test_tf_transformed = test_tf_fit.transform(fail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29815862986714159"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\\\n",
    "                    test_transformed, stars, test_size=0.4, random_state=42) \n",
    "test_reg = linear_model.LinearRegression()\n",
    "test_reg.fit(X_train, y_train)\n",
    "test_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "dill_out = open(\"hash_reg.dill\",\"wb\")\n",
    "dill.dump(test_reg,dill_out)\n",
    "dill_out.close()\n",
    "\n",
    "dill_out = open(\"hash_fit.dill\",\"wb\")\n",
    "dill.dump(test_fit,dill_out)\n",
    "dill_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "buz_res = dill.load(open(\"buz_res.dill\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'JwUE5GmEO-sH1FuwJgKBlQ', u'uGykseHzyS5xAMWoN6YUqA', u'LRKJF43s9-3jG9Lgx4zODg', u'RgDg-k9S5YD_BaxMckifkg', u'_wZTYYL7cutanzAnJUTGMA']\n"
     ]
    }
   ],
   "source": [
    "print buz_res[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "buz_data = []\n",
    "for i in data:\n",
    "    buz_data.append(i['business_id'])\n",
    "#print data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'vcNAWiLM4dR7D2nwwJ7nCA', u'vcNAWiLM4dR7D2nwwJ7nCA', u'vcNAWiLM4dR7D2nwwJ7nCA', u'vcNAWiLM4dR7D2nwwJ7nCA', u'vcNAWiLM4dR7D2nwwJ7nCA']\n"
     ]
    }
   ],
   "source": [
    "print buz_data[:5]\n",
    "matching = [i for i, x in enumerate(buz_data) if x in buz_res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'votes': {u'funny': 0, u'useful': 2, u'cool': 1}, u'user_id': u'Xqd0DzHaiyRqVH3WRG7hzg', u'review_id': u'15SdjuK7DmYqUAj6rjGowg', u'text': u\"dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\", u'business_id': u'vcNAWiLM4dR7D2nwwJ7nCA', u'stars': 5, u'date': u'2007-05-17', u'type': u'review'}, {u'votes': {u'funny': 0, u'useful': 2, u'cool': 0}, u'user_id': u'H1kH6QZV7Le4zqTRNxoZow', u'review_id': u'RF6UnRTtG7tWMcrO2GEoAg', u'text': u\"Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply never answers the phone.  It usually takes 2 hours of repeated calling to get an answer.  Who has time for that or wants to deal with it?  I have run into this problem with many other doctors and I just don't get it.  You have office workers, you have patients with medical needs, why isn't anyone answering the phone?  It's incomprehensible and not work the aggravation.  It's with regret that I feel that I have to give Dr. Goldberg 2 stars.\", u'business_id': u'vcNAWiLM4dR7D2nwwJ7nCA', u'stars': 2, u'date': u'2010-03-22', u'type': u'review'}]\n"
     ]
    }
   ],
   "source": [
    "print data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012913\n",
      "574278\n",
      "[8, 9, 10, 11, 12]\n",
      "1012913\n"
     ]
    }
   ],
   "source": [
    "print len(data)\n",
    "print len(matching)\n",
    "print matching[:5]\n",
    "print len(all_text)\n",
    "\n",
    "#1012913\n",
    "#574278\n",
    "#[8, 9, 10, 11, 12]\n",
    "#1012913"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574278\n"
     ]
    }
   ],
   "source": [
    "all_text_res = [all_text[i] for i in matching]\n",
    "print len(all_text_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574278\n"
     ]
    }
   ],
   "source": [
    "all_stars_res = [all_stars[i] for i in matching]\n",
    "print len(all_stars_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u\"Good truck stop dining at the right price. We love coming here on the weekends when we don't feel like cooking.\", u\"If you like lot lizards, you'll love the Pine Cone!\", u'Enjoyable experience for the whole family. The wait staff was courteous and friendly; the food was reasonably priced and a good value. A word of advice--LEAVE ROOM FOR DESSERT! the deserters are great but huge! Plan to bring some home', u\"One of my favorite truck stop diners with solid food and friendly, quick service. My god, those desserts are huge! I can't imagine eating that giant cream puff. \\nAll the food we had was delicious and I love how they leave a carafe of coffee on the table. Love this place. Would definitely be back if I was in the area!\", u\"Only went here once about a year and a half ago, but they had great pancakes! My only problem with it at the time was that they allowed smoking, so I left smelling like a cigarette. With the change in law, I'm sure the atmosphere has improved!\"]\n"
     ]
    }
   ],
   "source": [
    "print all_text_res[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import re\n",
    "import dill\n",
    "\n",
    "cachedStopWords = stopwords.words(\"english\") # Caching is supposed to help speed up the process\n",
    "def tokenize(text):\n",
    "# break up lines into words\n",
    "    return re.findall('[a-zA-Z0-9]+', text)\n",
    "all_rest_reviews = []\n",
    "for sent in all_text_res:\n",
    "    sentences = []\n",
    "    for w in tokenize(sent):\n",
    "        if w not in cachedStopWords:\n",
    "            sentences.append(w.lower())  \n",
    "    all_rest_reviews.append(sentences)\n",
    "unigram_freq = Counter()\n",
    "for sent in all_rest_reviews:\n",
    "    for w in sent:\n",
    "        unigram_freq[w] += 1.\n",
    "        \n",
    "bigram_freq = Counter()\n",
    "for sent in all_rest_reviews:\n",
    "    for w in itertools.izip(sent, itertools.islice(sent, 1, None)):\n",
    "        bigram_freq[w] += 1.\n",
    "dill.dump(unigram_freq, open(\"unigram_freq.p\", \"wb\")) \n",
    "dill.dump(bigram_freq, open(\"bigram_freq.p\", \"wb\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = unigram_freq + bigram_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(unigram_freq)\n",
    "print len(bigram_freq)\n",
    "print len(a)\n",
    "print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42943725.0 42369485.0\n"
     ]
    }
   ],
   "source": [
    "unigram_freq = dill.load(open(\"unigram_freq.p\", \"rb\"))\n",
    "bigram_freq  = dill.load(open(\"bigram_freq.p\", \"rb\"))\n",
    "unigram_count = sum(unigram_freq.values())\n",
    "bigram_count = sum(bigram_freq.values())\n",
    "print unigram_count, bigram_count\n",
    "bigrams = []\n",
    "for b in bigram_freq.keys():\n",
    "    p_w1_w2 = bigram_freq[b]/bigram_count\n",
    "    p_w1 = (90+unigram_freq[b[0]])/unigram_count\n",
    "    p_w2 = (90+unigram_freq[b[1]])/unigram_count\n",
    "    weight = p_w1_w2 / (p_w1 * p_w2)\n",
    "    bigrams.append((weight, b))\n",
    "sorted_list = sorted(bigrams, key=lambda x: (-x[0])) #-- to sort by all three values add x[1], x[2]....\n",
    "top100 = []\n",
    "for i in range(100):\n",
    "    top100.append(sorted_list[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ezzyujdouig4p gyb3pv', 'hodge podge', 'himal chuli', 'hoity toity', 'roka akor', 'unx wrafcxuakbzrdw', 'knick knacks', 'reina pepiada', 'cien agaves', 'baskin robbins', 'itty bitty', 'khai hoan', 'riff raff', 'grana padano', 'tutti santi', 'ropa vieja', 'gulab jamun', 'pel meni', 'ore ida', 'laan xang', 'dac biet', 'rula bula', 'rcapajgkojc2vo l3xa42a', 'hu tieu', 'bandeja paisa', 'tammie coe', 'ghillie dhu', 'chicha morada', 'innis gunn', 'alain ducasse', 'feng shui', '2weqs rnoobhb1kshkyosq', 'dol sot', 'itsy bitsy', 'mille feuille', 'leaps bounds', 'marche bacchus', 'uuu uuu', 'nooks crannies', 'celine dion', 'hofbr uhaus', 'resducs7fiiihp38 d6', 'd6 6g', 'nanay gloria', 'doon varna', 'krispy kreme', 'luc lac', 'woonam jung', 'perrier jouet', 'deja vu', 'molecular gastronomy', 'puerto rican', 'vice versa', 'patatas bravas', 'sais quoi', 'cullen skink', 'lloyd wright', 'pura vida', 'holyrood 9a', 'lomo saltado', 'wal mart', 'dueling pianos', 'nuoc mam', 'valle luna', 'nha trang', 'bradley ogden', 'barnes noble', 'rustler rooste', 'avant garde', 'honky tonk', 'haricot vert', 'kao tod', 'irn bru', 'ak yelpcdn', 'jayde fuzion', 'khao soi', 'malai kofta', 'aguas frescas', 'porta alba', 'lis doon', 'rx boiler', 'yadda yadda', 'humpty dumpty', 'mccormick schmick', 'ref 19485', 'lq9vmiwvm6v ronpkyx8aq', 'yada yada', 'shiner bock', 'artery clogging', 'buen provecho', 'ritz carlton', 'womp womp', 'rinky dink', 'sri lankan', 'chino bandido', 'sous vide', 'hunnie bunnie', 'preconceived notions', 'harry potter', 'hon machi']\n"
     ]
    }
   ],
   "source": [
    "q22 = [(a.encode('ascii','ignore')+' '+ b.encode('ascii','ignore')) for a,b in top100]\n",
    "print q22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5], [2], [4], [4], [4]]\n"
     ]
    }
   ],
   "source": [
    "print stars[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012913\n"
     ]
    }
   ],
   "source": [
    "print len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012913\n"
     ]
    }
   ],
   "source": [
    "from pandas.io.json import json_normalize\n",
    "data = []\n",
    "data_file = 'yelp_train_academic_dataset_review.json.gz'\n",
    "with gzip.open(data_file, 'rb') as data_file:\n",
    "    for line in data_file:\n",
    "        data.append(ujson.loads(line))\n",
    "print len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No JSON object could be decoded",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-aac7cc3d5c1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mob\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mob\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'yelp_train_academic_dataset_review.json.gz'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-93-aac7cc3d5c1c>\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      5\u001b[0m     ''' Convert a json string to a flat python dictionary\n\u001b[0;32m      6\u001b[0m     which can be passed into Pandas. '''\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python2.7/json/__init__.pyc\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    336\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 338\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    339\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python2.7/json/decoder.pyc\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m         \"\"\"\n\u001b[1;32m--> 366\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python2.7/json/decoder.pyc\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    382\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No JSON object could be decoded\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No JSON object could be decoded"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "def convert(x):\n",
    "    ''' Convert a json string to a flat python dictionary\n",
    "    which can be passed into Pandas. '''\n",
    "    ob = json.loads(x)\n",
    "    for k, v in ob.items():\n",
    "        if isinstance(v, list):\n",
    "            ob[k] = ','.join(v)\n",
    "        elif isinstance(v, dict):\n",
    "            for kk, vv in v.items():\n",
    "                ob['%s_%s' % (k, kk)] = vv\n",
    "            del ob[k]\n",
    "    return ob\n",
    "df = pd.DataFrame([convert(line) for line in file('yelp_train_academic_dataset_review.json.gz')])\n",
    "\n",
    "\"\"\"        \n",
    "data_bus = []\n",
    "data_file = 'yelp_train_academic_dataset_business.json.gz'\n",
    "with gzip.open(data_file, 'rb') as data_file:\n",
    "    for line in data_file:\n",
    "        data_bus.append(ujson.loads(line))\"\"\"\n",
    "#https://gist.github.com/paulgb/5265767\n",
    "\n",
    "#restaurants = df[df['categories'] == 'Restaurants' ]\n",
    "pattern = r'(Restaurants)'\n",
    "df_rest = df[df['categories'].str.contains(pattern)]['business_id'].to_frame().drop_duplicates()\n",
    "df_yelp = pd.DataFrame(data)\n",
    "df_rest = pd.merge(df_rest, df_yelp, how='inner', left_on='business_id', right_on='business_id')\n",
    "#restaurants = pd.concat(restaurants, df_yelp, join='inner', axis='business_id')\n",
    "subset = df_yelp[['stars', 'text']]\n",
    "df_List = [tuple(x) for x in subset.values]\n",
    "#Sampling from the Reviews\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "all_text_res_c = [re.sub(r\"\\d+\", '', y) for y in all_text_res]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574278\n"
     ]
    }
   ],
   "source": [
    "df_List = zip(all_stars_res, all_text_res_c)\n",
    "print len(df_List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574278\n",
      "[u'Decent food, slightly more expensive than their competitors, and more legit varieties on their menu.\\n\\nI\\'ll give credit to Imperial Garden\\'s decor as it is significantly more eye-pleasing than other Asian restaurants and the fact that they have a full bar. \\n\\nI give kudos to this place because they\\'re the only Chinese restaurant in the area that offers peking duck, a true Chinese specialty. Their seafood selection is also pretty upstanding and are cooked well. I\\'m also a fan of their \"banquet\" specials, which is for when you go in groups and they offer you a selection of X many dishes to share for a set price per person (this is available for lunch and dinner I believe, though I\\'ve never been here for lunch). \\n\\nAside from the aforementioned pros of Imperial Garden, the other dishes on their menu (if you order without doing the \"banquet\") is average-tasting at best and overpriced.', u'Im rating this more on being American Chinese then true authentic Chinese. That being said it was very nice inside. Not too formal but definitely not a take-out joint.\\n\\nWe got a cup of the Hot and Sour soup. Which is always my favorite. To try something different we got the Seafood Hot and Sour soup also. Almost taste just like the regular Hot and Sour put of course more of a seafood flavor then pork. It only comes in a serving that serves 2-3. Which is a great value because the 3 of us easily got 2 cups each out of it.\\n\\nThe Fragrant Pork had chestnuts, green peppers, onions, mushrooms and carrots in a garlic tomato sauce, Said that it was spicy hot put I only got a tiny bit of kick. Put still good being sweet and garlicky. \\n\\nThe Three Delicacies Sizzling Rice dish caught me by surprise in a good way. Expecting it would come with that same bland and over salty white sauce. But I couldn\\'t stop tasting the sauce. It was like no other white sauce I have tasted before. The waitress poured the sauce for the dish right in front of you so it was \"sizzling\". The rice was on the bottom of the skillet plate so it ended up getting crunchy. Adding a great texture to the dish. The shrimp, scallops and chicken in the dish were tender. Also in it were pea pods, water chestnuts, carrots and mushrooms.\\n\\nEven the fortune cookie at the end was a better grade cookie then take-out places.\\n\\nService was helpful, attentive and friendly.', u\"For Wisconsin, this is about the best you are going to get for Chinese Food.\\n\\nSorry, you live in Wisconsin.\\n\\nBut the food is very good.  Huge servings, enormous shrimps, chicken that is tender, fresh rice.\\n\\nDon't complain Cheeseheads, this is good as it gets for Chinese in Wisconsin.\", u'Yum. Check out back of menu soups. Very good!', u\"How do you get voted best of for 31 years when what is served is awful? This is the worst Chinese food my entire family has had in years. We all ordered different things and none of us could eat our meals. That was $50 wasted. There was no flavor, or the flavor there was did not match the dish. How do you mess up beef fried rice? I can make it with more flavor. The Mongolian beef had a flavor but it was all wrong. If you want to waste money and still be hungry, this is the place for you. How do places keep getting good reviews when they are so bad. I use to trust Yelp reviews but lately, I've been fooled by the reviews.\"]\n",
      "[u'Decent food, slightly more expensive than their competitors, and more legit varieties on their menu.\\n\\nI\\'ll give credit to Imperial Garden\\'s decor as it is significantly more eye-pleasing than other Asian restaurants and the fact that they have a full bar. \\n\\nI give kudos to this place because they\\'re the only Chinese restaurant in the area that offers peking duck, a true Chinese specialty. Their seafood selection is also pretty upstanding and are cooked well. I\\'m also a fan of their \"banquet\" specials, which is for when you go in groups and they offer you a selection of X many dishes to share for a set price per person (this is available for lunch and dinner I believe, though I\\'ve never been here for lunch). \\n\\nAside from the aforementioned pros of Imperial Garden, the other dishes on their menu (if you order without doing the \"banquet\") is average-tasting at best and overpriced.', u'Im rating this more on being American Chinese then true authentic Chinese. That being said it was very nice inside. Not too formal but definitely not a take-out joint.\\n\\nWe got a cup of the Hot and Sour soup. Which is always my favorite. To try something different we got the Seafood Hot and Sour soup also. Almost taste just like the regular Hot and Sour put of course more of a seafood flavor then pork. It only comes in a serving that serves -. Which is a great value because the  of us easily got  cups each out of it.\\n\\nThe Fragrant Pork had chestnuts, green peppers, onions, mushrooms and carrots in a garlic tomato sauce, Said that it was spicy hot put I only got a tiny bit of kick. Put still good being sweet and garlicky. \\n\\nThe Three Delicacies Sizzling Rice dish caught me by surprise in a good way. Expecting it would come with that same bland and over salty white sauce. But I couldn\\'t stop tasting the sauce. It was like no other white sauce I have tasted before. The waitress poured the sauce for the dish right in front of you so it was \"sizzling\". The rice was on the bottom of the skillet plate so it ended up getting crunchy. Adding a great texture to the dish. The shrimp, scallops and chicken in the dish were tender. Also in it were pea pods, water chestnuts, carrots and mushrooms.\\n\\nEven the fortune cookie at the end was a better grade cookie then take-out places.\\n\\nService was helpful, attentive and friendly.', u\"For Wisconsin, this is about the best you are going to get for Chinese Food.\\n\\nSorry, you live in Wisconsin.\\n\\nBut the food is very good.  Huge servings, enormous shrimps, chicken that is tender, fresh rice.\\n\\nDon't complain Cheeseheads, this is good as it gets for Chinese in Wisconsin.\", u'Yum. Check out back of menu soups. Very good!', u\"How do you get voted best of for  years when what is served is awful? This is the worst Chinese food my entire family has had in years. We all ordered different things and none of us could eat our meals. That was $ wasted. There was no flavor, or the flavor there was did not match the dish. How do you mess up beef fried rice? I can make it with more flavor. The Mongolian beef had a flavor but it was all wrong. If you want to waste money and still be hungry, this is the place for you. How do places keep getting good reviews when they are so bad. I use to trust Yelp reviews but lately, I've been fooled by the reviews.\"]\n"
     ]
    }
   ],
   "source": [
    "print len(all_text_res_c)\n",
    "print all_text_res[100:105]\n",
    "print all_text_res_c[100:105]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxxx adsf;asf  dkjafs '"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "re.sub(r\"\\d+\", '', 'xxxx adsf;a231sf234 0 dkjafs 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "sample_list = random.sample(df_List, 50000)\n",
    "filtered_reviews = filter(lambda (stars, text): text.strip()!='' , sample_list)\n",
    "stars, reviews = zip(*filtered_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_text = random.sample(all_text_res_c,50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Rahul second try\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Using the Count Vectorizer\n",
    "bag_of_words_vectorizer = CountVectorizer(max_features=300,\n",
    "                                          ngram_range=(1,1),\n",
    "                                          stop_words=nltk.corpus.stopwords.words('english'), \n",
    "                                          max_df = 1.0, min_df= 1)\n",
    "fitVocabulary2 = bag_of_words_vectorizer.fit(text for text in reviews)\n",
    "# Apply fitVocabulary to the reviews\n",
    "fitReviews2 = fitVocabulary2.transform(reviews)\n",
    "#Q1 : Bag of Words model\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\\\n",
    "                    fitReviews2, stars, test_size=0.4, random_state=42)\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "#Q2 : Normalized Model\n",
    "# tfidf vectorization from scratch\n",
    "# This can also be done by applying TfidfTransformer to the Vectorized dataset from before\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dill.dump(regr, open('bag_Of_Words.dill', 'w'))\n",
    "dill.dump(fitVocabulary2, open('vectorizer.dill', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34189670708772668"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=300, \n",
    "                                   ngram_range=(1,1), \n",
    "                                   stop_words=nltk.corpus.stopwords.words('english'),\n",
    "                                   max_df = 1.0, min_df= 20)\n",
    "fitVocabulary_tfidf = tfidf_vectorizer.fit(text for text in reviews)\n",
    "fitReviews_tfidf = fitVocabulary_tfidf.transform(reviews)\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\\\n",
    "                    fitReviews_tfidf, stars, test_size=0.4, random_state=42) \n",
    "k_shell = linear_model.LinearRegression(fit_intercept=True)\n",
    "k_shell.fit(X_train, y_train)\n",
    "regr_nm = linear_model.LinearRegression()\n",
    "regr_nm.fit(X_train, y_train)\n",
    "dill.dump(regr_nm, open('norm_model.dill', 'w'))\n",
    "dill.dump(fitVocabulary_tfidf, open('tfidf_vect.dill', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40459855365375685"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr_nm.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7446115107367426"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr_nm.predict(X_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'actually', u'almost', u'also', u'always', u'amazing', u'another', u'anything', u'area', u'around', u'ask', u'asked', u'ate', u'atmosphere', u'away', u'awesome', u'back', u'bacon', u'bad', u'bar', u'bbq', u'beef', u'beer', u'best', u'better', u'big', u'bit', u'bread', u'breakfast', u'brought', u'buffet', u'burger', u'burgers', u'busy', u'came', u'cheap', u'check', u'cheese', u'chicken', u'chinese', u'chips', u'chocolate', u'clean', u'coffee', u'come', u'comes', u'coming', u'cooked', u'cool', u'could', u'couple', u'course', u'crab', u'cream', u'day', u'decent', u'decided', u'definitely', u'delicious', u'dessert', u'didn', u'different', u'dining', u'dinner', u'disappointed', u'dish', u'dishes', u'drink', u'drinks', u'eat', u'eating', u'egg', u'either', u'else', u'end', u'enjoy', u'enjoyed', u'enough', u'especially', u'even', u'ever', u'every', u'everyone', u'everything', u'excellent', u'experience', u'extra', u'family', u'fan', u'fantastic', u'far', u'fast', u'favorite', u'feel', u'finally', u'find', u'first', u'fish', u'flavor', u'food', u'found', u'free', u'fresh', u'fried', u'friend', u'friendly', u'friends', u'fries', u'full', u'fun', u'gave', u'get', u'getting', u'give', u'go', u'going', u'good', u'got', u'great', u'green', u'half', u'happy', u'hard', u'high', u'home', u'hot', u'hour', u'house', u'however', u'huge', u'husband', u'ice', u'inside', u'isn', u'items', u'kind', u'know', u'large', u'last', u'least', u'left', u'let', u'like', u'liked', u'line', u'little', u'll', u'location', u'long', u'look', u'looked', u'looking', u'lot', u'love', u'loved', u'lunch', u'made', u'make', u'many', u'maybe', u'meal', u'meat', u'menu', u'mexican', u'minutes', u'much', u'must', u'need', u'never', u'new', u'next', u'nice', u'night', u'nothing', u'oh', u'ok', u'okay', u'old', u'one', u'open', u'order', u'ordered', u'outside', u'overall', u'part', u'people', u'perfect', u'pizza', u'place', u'places', u'plate', u'pm', u'pork', u'portions', u'potatoes', u'pretty', u'price', u'prices', u'probably', u'put', u'quality', u'quick', u'quite', u're', u'really', u'recommend', u'red', u'restaurant', u'restaurants', u'review', u'reviews', u'rice', u'right', u'roll', u'rolls', u'room', u'said', u'salad', u'salsa', u'sandwich', u'sat', u'sauce', u'say', u'seated', u'see', u'seemed', u'selection', u'served', u'server', u'service', u'shrimp', u'side', u'since', u'sit', u'small', u'something', u'soup', u'special', u'spicy', u'spot', u'staff', u'star', u'stars', u'steak', u'still', u'strip', u'style', u'super', u'sure', u'sushi', u'sweet', u'table', u'tables', u'tacos', u'take', u'taste', u'tasted', u'tasty', u'thai', u'thing', u'things', u'think', u'though', u'thought', u'three', u'time', u'times', u'told', u'took', u'top', u'town', u'tried', u'try', u'two', u'us', u'used', u'usually', u've', u'vegas', u'visit', u'wait', u'waiter', u'waitress', u'want', u'wanted', u'wasn', u'water', u'way', u'well', u'went', u'whole', u'wife', u'wine', u'without', u'work', u'worth', u'would', u'years', u'yelp', u'yes', u'yummy']\n"
     ]
    }
   ],
   "source": [
    "print fitVocabulary_tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'actually', u'almost', u'also', u'although', u'always', u'amazing', u'another', u'anything', u'area', u'around', u'ask', u'asked', u'ate', u'atmosphere', u'away', u'awesome', u'back', u'bacon', u'bad', u'bar', u'bbq', u'beef', u'beer', u'best', u'better', u'big', u'bit', u'bread', u'breakfast', u'brought', u'buffet', u'burger', u'burgers', u'busy', u'came', u'check', u'cheese', u'chicken', u'chips', u'chocolate', u'clean', u'coffee', u'cold', u'come', u'comes', u'coming', u'cooked', u'cool', u'could', u'couple', u'course', u'crab', u'cream', u'day', u'decent', u'decided', u'decor', u'definitely', u'delicious', u'dessert', u'didn', u'different', u'dining', u'dinner', u'disappointed', u'dish', u'dishes', u'drink', u'drinks', u'eat', u'eating', u'else', u'end', u'enjoy', u'enjoyed', u'enough', u'especially', u'even', u'ever', u'every', u'everyone', u'everything', u'excellent', u'experience', u'family', u'fan', u'fantastic', u'far', u'fast', u'favorite', u'feel', u'finally', u'find', u'first', u'fish', u'flavor', u'food', u'found', u'free', u'fresh', u'fried', u'friend', u'friendly', u'friends', u'fries', u'full', u'fun', u'gave', u'get', u'getting', u'give', u'go', u'going', u'good', u'got', u'great', u'green', u'half', u'happy', u'hard', u'high', u'home', u'hot', u'hour', u'house', u'however', u'huge', u'husband', u'ice', u'inside', u'items', u'kind', u'know', u'large', u'las', u'last', u'least', u'left', u'let', u'like', u'liked', u'line', u'little', u'll', u'location', u'long', u'look', u'looked', u'looking', u'lot', u'love', u'loved', u'lunch', u'made', u'make', u'many', u'maybe', u'meal', u'meat', u'menu', u'mexican', u'minutes', u'much', u'must', u'need', u'never', u'new', u'next', u'nice', u'night', u'nothing', u'oh', u'ok', u'okay', u'old', u'one', u'open', u'order', u'ordered', u'outside', u'overall', u'part', u'people', u'perfect', u'pizza', u'place', u'places', u'plate', u'pm', u'pork', u'portions', u'pretty', u'price', u'prices', u'probably', u'put', u'quality', u'quick', u'quite', u're', u'really', u'recommend', u'red', u'restaurant', u'restaurants', u'review', u'reviews', u'rice', u'right', u'roll', u'rolls', u'room', u'said', u'salad', u'salsa', u'sandwich', u'sat', u'sauce', u'say', u'seated', u'see', u'seemed', u'selection', u'served', u'server', u'service', u'shrimp', u'side', u'since', u'sit', u'small', u'something', u'soup', u'special', u'spicy', u'spot', u'staff', u'star', u'stars', u'steak', u'still', u'strip', u'style', u'super', u'sure', u'sushi', u'sweet', u'table', u'tables', u'tacos', u'take', u'taste', u'tasted', u'tasty', u'thai', u'thing', u'things', u'think', u'though', u'thought', u'three', u'time', u'times', u'told', u'took', u'top', u'town', u'tried', u'try', u'trying', u'two', u'us', u'used', u'usually', u've', u'vegas', u'visit', u'wait', u'waiter', u'waitress', u'want', u'wanted', u'wasn', u'water', u'way', u'well', u'went', u'whole', u'wife', u'wine', u'without', u'won', u'work', u'worth', u'would', u'wrong', u'years', u'yelp', u'yes', u'yummy']\n"
     ]
    }
   ],
   "source": [
    "print fitVocabulary.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'actual', u'almost', u'also', u'alway', u'amaz', u'anoth', u'anyth', u'appet', u'area', u'around', u'arriv', u'ask', u'ate', u'atmospher', u'attent', u'away', u'awesom', u'back', u'bacon', u'bad', u'bar', u'bbq', u'bean', u'beef', u'beer', u'best', u'better', u'big', u'bit', u'bite', u'bread', u'breakfast', u'bring', u'buffet', u'burger', u'busi', u'cake', u'call', u'came', u'check', u'chees', u'chef', u'chicken', u'chip', u'chocol', u'choic', u'clean', u'close', u'coffe', u'come', u'cook', u'could', u'coupl', u'cours', u'crab', u'cream', u'custom', u'day', u'decent', u'decid', u'decor', u'definit', u'delici', u'dessert', u'didn', u'differ', u'dine', u'dinner', u'disappoint', u'dish', u'drink', u'eat', u'egg', u'els', u'end', u'enjoy', u'enough', u'especi', u'even', u'ever', u'everi', u'everyon', u'everyth', u'excel', u'expect', u'experi', u'famili', u'fan', u'fantast', u'far', u'fast', u'favorit', u'feel', u'fill', u'final', u'find', u'first', u'fish', u'flavor', u'food', u'found', u'free', u'fresh', u'fri', u'friend', u'full', u'fun', u'get', u'give', u'go', u'good', u'got', u'great', u'green', u'grill', u'group', u'guy', u'half', u'happi', u'hard', u'help', u'high', u'home', u'hot', u'hour', u'hous', u'howev', u'huge', u'husband', u'ice', u'impress', u'insid', u'item', u'kind', u'know', u'larg', u'last', u'least', u'left', u'let', u'light', u'like', u'line', u'littl', u'live', u'll', u'locat', u'long', u'look', u'lot', u'love', u'lunch', u'made', u'make', u'manag', u'mani', u'mayb', u'meal', u'meat', u'menu', u'mexican', u'minut', u'much', u'must', u'name', u'need', u'never', u'new', u'next', u'nice', u'night', u'noodl', u'noth', u'offer', u'oh', u'ok', u'old', u'one', u'onion', u'open', u'option', u'order', u'outsid', u'overal', u'owner', u'parti', u'pay', u'peopl', u'perfect', u'person', u'pizza', u'place', u'plate', u'pork', u'portion', u'potato', u'pretti', u'price', u'probabl', u'put', u'qualiti', u'quick', u'quit', u're', u'realli', u'reason', u'recommend', u'red', u'restaur', u'return', u'review', u'rib', u'rice', u'right', u'roll', u'room', u'said', u'salad', u'salsa', u'sandwich', u'sauc', u'say', u'seat', u'see', u'seem', u'select', u'serv', u'server', u'servic', u'share', u'shrimp', u'side', u'sinc', u'sit', u'size', u'slice', u'small', u'someth', u'soup', u'special', u'spici', u'spot', u'staff', u'star', u'start', u'steak', u'still', u'stop', u'strip', u'style', u'super', u'sure', u'surpris', u'sushi', u'sweet', u'tabl', u'taco', u'take', u'tast', u'tasti', u'tell', u'thai', u'thing', u'think', u'though', u'thought', u'three', u'time', u'told', u'took', u'top', u'town', u'tri', u'two', u'us', u'use', u'usual', u've', u'vega', u'visit', u'wait', u'waiter', u'waitress', u'walk', u'want', u'wasn', u'water', u'way', u'week', u'well', u'went', u'wife', u'wine', u'without', u'wonder', u'work', u'worth', u'would', u'year', u'yelp']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "default_tokenizer = TfidfVectorizer().build_tokenizer()\n",
    "stemmer = nltk.stem.SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "    \n",
    "def tokenize_stem(text):\n",
    "    \"\"\"\n",
    "    We will use the default tokenizer from TfidfVectorizer, combined with the nltk SnowballStemmer.\n",
    "    \"\"\"\n",
    "    tokens = default_tokenizer(text)\n",
    "    stemmed = map(stemmer.stem, tokens)\n",
    "    return stemmed\n",
    "\n",
    "ng_stem_tfidf = TfidfVectorizer(max_features=300, \n",
    "                         ngram_range=(1,1), \n",
    "                         stop_words=map(stemmer.stem, nltk.corpus.stopwords.words('english')),\n",
    "                                max_df = 1.0, min_df= 20,\n",
    "                         tokenizer=tokenize_stem)\n",
    "\n",
    "tf_fit=ng_stem_tfidf.fit(text for text in reviews)\n",
    "tf_transformed = tf_fit.transform(reviews)\n",
    "\n",
    "ng_stem_vocab = ng_stem_tfidf.get_feature_names()\n",
    "print ng_stem_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\\\n",
    "                    tf_transformed, stars, test_size=0.4, random_state=42) \n",
    "tf_reg = linear_model.LinearRegression()\n",
    "tf_reg.fit(X_train, y_train)\n",
    "\n",
    "#dill.dump(regr_nm, open('norm_model.dill', 'w'))\n",
    "#dill.dump(fitVocabulary_tfidf, open('tfidf_vect.dill', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43642653139010523"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_reg.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dill.dump(tf_reg, open('tf_regnew.dill', 'w'))\n",
    "dill.dump(tf_fit, open('tf_fitnew.dill', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7408997045519921"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_reg.predict(X_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'Three words: Lemon Bar Brulee\\n\\nA few more words: Red Velvet Cupcake with Bourbon Chocolate Sauce\\n\\nFor the most part, if you\\'ve been to one CJ, you\\'ve been to them all.  The only exception I know is the ones in the Seattle area have a \"Produce Bar\" for about $-$ which is the mother of all salad bars as far as regular non-buffet restaurants go.\\n\\nThe portions are gargantuan and I often find solace in the half portions where available and the appetizer salad and soup combo which is still a big portion but is a little easier on the purse.  The food is always good, home-style fare and I\\'ve never had a problem with service.  The bar area is nice and has many TVs strategically placed for the avid sports fan.  You really have to stake it out when it\\'s busy and be prepared to steal tables from people.  They don\\'t seat you in the bar, so every spot is up for the taking.\\n\\nThe desserts are really where it is at for me.  They smartened up and now have two dessert sections: individual desserts and desserts to share.  This is a plus b/c so many of their menu items could feed a small family, so now you can order appropriately.  My soon-to-be mother-in-law ordered the eclair once and luckily the waitress told her it was big enough for everyone at the table.  That would have been a very big eclair for a very little lady.  Anyway...  The red velvet cupcake with chocolate bourbon sauce is a fantastic individual dessert.  My all-time favorite is the lemon bar brulee which consists of a shortbread crust, layer of cheesecake, layer of lemon bar, bruleed on the top and served with whipped cream and raspberry sauce.  It\\'s on the desserts to share list, but I get it for myself and enjoy it for a day or two.  I encourage you to do the same.',\n",
       " u\"This food court sucks hard-core. They charge steep prices. My friend got fish and chips (fried fish and french fries). The fries tasted like they were frozen and the batter was definitely allll the same color for both the fish and fries supporting my believe that the fries, and perhaps also the fish, were frozen. Please don't give us a frozen meal for $.\\n\\nI ordered some pollo tacos from the mexican joint. The rice and beans were surprisingly well-seasoned but the tacos themselves were hard to eat due to the slimy texture of the soft corn taco. It was submerged in bean sauuce.\\n\\nOh well. Lesson learned. Las Vegas isn't really like one's adult disneyland since the food at disneyland is at least yummy for the price.\\n\\nOn the plus side the service was AMAZING. I loved all the wonderful servers. They treated us like family. :)\",\n",
       " u\"It's in the name, Fat. Don't expect something healthy. It'll satisfy your hunger.\\n\\nGood enough for me to come back if convenient.\",\n",
       " u'So I\\'m on my way back from the DMV (that\\'s another story for another day) and am hungry.  For anyone who doesn\\'t know, they actually have a food counter at the DMV.  It reminds me of the food counter that used to be in the ice skating rink when I was a kid.  Well, the DMV food counter sends the smell of popcorn out into the crowd - perhaps to cover the stench of those who haven\\'t showered for the day, week - whatever.  So this made me hungry enough to pull out my Blackberry and start searching for Chinese takeout on Yelp.\\n\\nThanks to location and the few reviews, I found the Pearl Wok.  \\nI ordered the $. daily lunch which included egg roll, soup and steamed/fried rice.  Chose the Beef & Broccoli.  Also ordered an appetizer/side of () Chicken dumplings and had them steamed ($.).  The beef/broccoli was yummy - broccoli was cooked well and not fatty beef.  The fried rice was fried rice - not extraordinary but good nonetheless.  The dumplings (or potstickers, as I call them) were great but the best part was the dipping sauce.  That awesome homemade brown potsticker sauce that has fresh ginger and green onions ... oh so good.  And now here\\'s my favorite part - when I asked for the hot oil - they had it!!  Some chinese places look at me like I\\'m speaking ... well, English .. when I ask for the hot oil.  I\\'m sure it has a name but I\\'m going with hot oil - the one with the chili pepper seeds.  Oh my, put a little of this on your potsticker and dip into the sauce ... oh my.  Obviously this is written by a girl who likes a little \"heat\" ....\\n\\nOf note:  They offer free delivery with purchase of $ within -mile radius.  I\\'d definitely order again.',\n",
       " u'Hands down the best pizza in the valley!  \" morire di affamato \" This heavenly place was near my employment on the weekend, I decided to try them out before work. Now its my weekend ritual, trust me give them a chance and you will be pleased!')"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1012913, 1048576)\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "#sklearn.feature_extraction.text.HashingVectorizer\n",
    "\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "vec = HashingVectorizer(\n",
    "                        stop_words=nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "hash_fit = vec.fit(all_text)\n",
    "hash_transformed = hash_fit.transform(all_text)\n",
    "\n",
    "#print vec.get_feature_names()\n",
    "#counts = vec.fit_transform( all_text  )\n",
    "print hash_transformed.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6249767251686642"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\\\n",
    "                    hash_transformed, all_stars, test_size=0.4, random_state=42) \n",
    "\n",
    "\n",
    "r =linear_model.Ridge(alpha=0.5815)\n",
    "r.fit(X_train, y_train)\n",
    "r.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "dill_out = open(\"hash_reg4.dill\",\"wb\")\n",
    "dill.dump(r,dill_out)\n",
    "dill_out.close()\n",
    "\n",
    "dill_out = open(\"hash_fit4.dill\",\"wb\")\n",
    "dill.dump(hash_fit,dill_out)\n",
    "dill_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nearest_neighbors_cv.fit(X,y)\n",
    "cv_accuracy = pd.DataFrame.from_records(\n",
    "    [(score.parameters['n_neighbors'],\n",
    "      score.mean_validation_score)\n",
    "     for score in nearest_neighbors_cv.grid_scores_],\n",
    "columns=['n_neighbors', 'accuracy'])\n",
    "\n",
    "plt.plot(cv_accuracy.n_neighbors, cv_accuracy.accuracy)\n",
    "plt.xlabel('n_neighbors')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "cv = cross_validation.ShuffleSplit(len(y), n_iter=20, test_size=0.4, random_state=42)\n",
    "alphas = np.arange(0, 20, 1)\n",
    "grid = GridSearchCV(linear_model.Ridge(), \n",
    "                                                param_grid=dict(alpha=alphas), cv=cv, \n",
    "                                                scoring='mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-5166be6a9c6f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/opt/conda/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m         \"\"\"\n\u001b[1;32m--> 732\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    734\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[0;32m    549\u001b[0m                 **best.parameters)\n\u001b[0;32m    550\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m                 \u001b[0mbest_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m                 \u001b[0mbest_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python2.7/site-packages/sklearn/linear_model/ridge.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    508\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0man\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mof\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m         \"\"\"\n\u001b[1;32m--> 510\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRidge\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python2.7/site-packages/sklearn/linear_model/ridge.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    397\u001b[0m                                       \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m                                       \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m                                       solver=solver)\n\u001b[0m\u001b[0;32m    400\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_intercept\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_std\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python2.7/site-packages/sklearn/linear_model/ridge.pyc\u001b[0m in \u001b[0;36mridge_regression\u001b[1;34m(X, y, alpha, sample_weight, solver, max_iter, tol, verbose)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msolver\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'sparse_cg'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 331\u001b[1;33m         \u001b[0mcoef\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_solve_sparse_cg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0msolver\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"lsqr\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python2.7/site-packages/sklearn/linear_model/ridge.pyc\u001b[0m in \u001b[0;36m_solve_sparse_cg\u001b[1;34m(X, y, alpha, max_iter, tol, verbose)\u001b[0m\n\u001b[0;32m     55\u001b[0m             C = sp_linalg.LinearOperator(\n\u001b[0;32m     56\u001b[0m                 (n_samples, n_samples), matvec=mv, dtype=X.dtype)\n\u001b[1;32m---> 57\u001b[1;33m             \u001b[0mcoef\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msp_linalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_column\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m             \u001b[0mcoefs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmatvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoef\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python2.7/site-packages/scipy/sparse/linalg/isolve/iterative.pyc\u001b[0m in \u001b[0;36mcg\u001b[1;34m(A, b, x0, tol, maxiter, xtype, M, callback)\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python2.7/site-packages/scipy/sparse/linalg/isolve/iterative.pyc\u001b[0m in \u001b[0;36mnon_reentrant\u001b[1;34m(func, *a, **kw)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'__entered'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'__entered'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python2.7/site-packages/scipy/sparse/linalg/isolve/iterative.pyc\u001b[0m in \u001b[0;36mcg\u001b[1;34m(A, b, x0, tol, maxiter, xtype, M, callback)\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mijob\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m             \u001b[0mwork\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mslice2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0msclr2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m             \u001b[0mwork\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mslice2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msclr1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmatvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwork\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mslice1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mijob\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m             \u001b[0mwork\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mslice1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpsolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwork\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mslice2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python2.7/site-packages/scipy/sparse/linalg/interface.pyc\u001b[0m in \u001b[0;36mmatvec\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    218\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dimension mismatch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_matvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python2.7/site-packages/scipy/sparse/linalg/interface.pyc\u001b[0m in \u001b[0;36m_matvec\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_matvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 460\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__matvec_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_rmatvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python2.7/site-packages/sklearn/linear_model/ridge.pyc\u001b[0m in \u001b[0;36m_mv\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcreate_mv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurr_alpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0m_mv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mX1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmatvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcurr_alpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python2.7/site-packages/scipy/sparse/linalg/interface.pyc\u001b[0m in \u001b[0;36mmatvec\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    218\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dimension mismatch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_matvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python2.7/site-packages/scipy/sparse/linalg/interface.pyc\u001b[0m in \u001b[0;36m_matvec\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[0mwill\u001b[0m \u001b[0mdefine\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mvector\u001b[0m \u001b[0mmultiplication\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mwell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \"\"\"\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmatvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python2.7/site-packages/scipy/sparse/linalg/interface.pyc\u001b[0m in \u001b[0;36mmatmat\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    325\u001b[0m                              % (self.shape, X.shape))\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m         \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_matmat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python2.7/site-packages/scipy/sparse/linalg/interface.pyc\u001b[0m in \u001b[0;36m_matmat\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_matmat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 597\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    598\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_adjoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python2.7/site-packages/scipy/sparse/base.pyc\u001b[0m in \u001b[0;36mdot\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \"\"\"\n\u001b[1;32m--> 252\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python2.7/site-packages/scipy/sparse/base.pyc\u001b[0m in \u001b[0;36m__mul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mul_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mul_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mul_multivector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python2.7/site-packages/scipy/sparse/compressed.pyc\u001b[0m in \u001b[0;36m_mul_vector\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    463\u001b[0m         \u001b[1;31m# csr_matvec or csc_matvec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m         \u001b[0mfn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_sparsetools\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_matvec'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 465\u001b[1;33m         \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "grid.fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.26571430169\n",
      "0.5815\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=ShuffleSplit(734, n_iter=20, test_size=0.4, random_state=42),\n",
      "       error_score='raise',\n",
      "       estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, solver='auto', tol=0.001),\n",
      "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
      "       param_grid={'alpha': array([ 0.5 ,  0.51,  0.52,  0.53,  0.54,  0.55,  0.56,  0.57,  0.58,\n",
      "        0.59,  0.6 ,  0.61,  0.62,  0.63,  0.64,  0.65,  0.66,  0.67,\n",
      "        0.68,  0.69])},\n",
      "       pre_dispatch='2*n_jobs', refit=True, score_func=None,\n",
      "       scoring='mean_squared_error', verbose=0)\n"
     ]
    }
   ],
   "source": [
    "print grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\n"
     ]
    }
   ],
   "source": [
    "print 'b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_json = [\n",
    "    {\"votes\": {\"funny\": 0, \"useful\": 0, \"cool\": 0}, \"user_id\": \"WsGQfLLy3YlP_S9jBE3j1w\", \"review_id\": \"kzFlI35hkmYA_vPSsMcNoQ\", \"stars\": 5, \"date\": \"2012-11-03\", \"text\": \"Love it!!!!! Love it!!!!!! love it!!!!!!!   Who doesn't love Culver's!\", \"type\": \"review\", \"business_id\": \"LRKJF43s9-3jG9Lgx4zODg\"},\n",
    "    {\"votes\": {\"funny\": 0, \"useful\": 0, \"cool\": 0}, \"user_id\": \"Veue6umxTpA3o1eEydowZg\", \"review_id\": \"Tfn4EfjyWInS-4ZtGAFNNw\", \"stars\": 3, \"date\": \"2013-12-30\", \"text\": \"Everything was great except for the burgers they are greasy and very charred compared to other stores.\", \"type\": \"review\", \"business_id\": \"LRKJF43s9-3jG9Lgx4zODg\"},\n",
    "    {\"votes\": {\"funny\": 0, \"useful\": 0, \"cool\": 0}, \"user_id\": \"u5xcw6LCnnMhddoxkRIgUA\", \"review_id\": \"ZYaS2P5EmK9DANxGTV48Tw\", \"stars\": 5, \"date\": \"2010-12-04\", \"text\": \"I really like both Chinese restaurants in town.  This one has outstanding crab rangoon.  Love the chicken with snow peas and mushrooms and General Tso Chicken.  Food is always ready in 10 minutes which is accurate.  Good place and they give you free pop.\", \"type\": \"review\", \"business_id\": \"RgDg-k9S5YD_BaxMckifkg\"},\n",
    "    {\"votes\": {\"funny\": 0, \"useful\": 0, \"cool\": 0}, \"user_id\": \"kj18hvJRPLepZPNL7ySKpg\", \"review_id\": \"uOLM0vvnFdp468ofLnszTA\", \"stars\": 3, \"date\": \"2011-06-02\", \"text\": \"Above average takeout with friendly staff. The sauce on the pan fried noodle is tasty. Dumplings are quite good.\", \"type\": \"review\", \"business_id\": \"RgDg-k9S5YD_BaxMckifkg\"},\n",
    "    {\"votes\": {\"funny\": 0, \"useful\": 0, \"cool\": 0}, \"user_id\": \"L5kqM35IZggaPTpQJqcgwg\", \"review_id\": \"b3u1RHmZTNRc0thlFmj2oQ\", \"stars\": 4, \"date\": \"2012-05-28\", \"text\": \"We order from Chang Jiang often and have never been disappointed.  The menu is huge, and can accomodate anyone's taste buds.  The service is quick, usually ready in 10 minutes.\", \"type\": \"review\", \"business_id\": \"RgDg-k9S5YD_BaxMckifkg\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love it!!!!! Love it!!!!!! love it!!!!!!!   Who doesn't love Culver's!\n",
      "Everything was great except for the burgers they are greasy and very charred compared to other stores.\n",
      "I really like both Chinese restaurants in town.  This one has outstanding crab rangoon.  Love the chicken with snow peas and mushrooms and General Tso Chicken.  Food is always ready in 10 minutes which is accurate.  Good place and they give you free pop.\n",
      "Above average takeout with friendly staff. The sauce on the pan fried noodle is tasty. Dumplings are quite good.\n",
      "We order from Chang Jiang often and have never been disappointed.  The menu is huge, and can accomodate anyone's taste buds.  The service is quick, usually ready in 10 minutes.\n"
     ]
    }
   ],
   "source": [
    "for record in test_json:\n",
    "    print record['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\n",
      "Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply never answers the phone.  It usually takes 2 hours of repeated calling to get an answer.  Who has time for that or wants to deal with it?  I have run into this problem with many other doctors and I just don't get it.  You have office workers, you have patients with medical needs, why isn't anyone answering the phone?  It's incomprehensible and not work the aggravation.  It's with regret that I feel that I have to give Dr. Goldberg 2 stars.\n",
      "Dr. Goldberg has been my doctor for years and I like him.  I've found his office to be fairly efficient.  Today I actually got to see the doctor a few minutes early!  \n",
      "\n",
      "He seems very engaged with his patients and his demeanor is friendly, yet authoritative.    \n",
      "\n",
      "I'm glad to have Dr. Goldberg as my doctor.\n",
      "Been going to Dr. Goldberg for over 10 years. I think I was one of his 1st patients when he started at MHMG. He's been great over the years and is really all about the big picture. It is because of him, not my now former gyn Dr. Markoff, that I found out I have fibroids. He explores all options with you and is very patient and understanding. He doesn't judge and asks all the right questions. Very thorough and wants to be kept in the loop on every aspect of your medical health and your life.\n",
      "Got a letter in the mail last week that said Dr. Goldberg is moving to Arizona to take a new position there in June.  He will be missed very much.  \n",
      "\n",
      "I think finding a new doctor in NYC that you actually like might almost be as awful as trying to find a date!\n"
     ]
    }
   ],
   "source": [
    "for i in all_text[:5]:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ii"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
